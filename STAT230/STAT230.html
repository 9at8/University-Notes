<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>STAT230 | Anthony Zhang</title>
  <style type="text/css">
  body {
    font-family: "Segoe UI", Verdana, Arial, Helvetica, sans-serif;
    background: #fffefe;
    padding: 5em;
  }
  
  pre {
    margin-left: 2em;
  }
  
  code {
    border: solid 1px black;
    background: #665555;
    color: white;
    padding: 0.1em;
    border-radius: 0.3em;
    display: inline-block;
  }
  
  pre code {
    padding: 1em;
    border-radius: 0.5em;
  }
  
  h1 {
    font-size: 4em;
  }
  
  table {
    margin: 0 auto;
  }
  
  td, th {
    padding: 0.5em;
    border: 1px solid grey;
  }
  
  tr {
    padding:: 0;
  }
  
  a.button {
    display: inline-block;
    padding: 1em;
    font-family: monospace;
    color: black;
    text-decoration: none;
    border: 0.2em solid black;
    border-radius: 0.5em;
    background: white;
  }
  
  a.button:hover, a.button:focus, a.button:active {
    background: black;
    color: white;
  }
  </style>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>
<body>
<a class="button" href="..">&#8666; Return to University Notes index</a>
<h1 id="stat230">STAT230</h1>
<p>Probability.</p>
<pre><code>Instructor: Dina Dawoud
Office Hours: Monday, Wednesday 2:30PM-3:20PM in M3 3126</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l&#39;H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
\]</span></p>
<h1 id="section">5/5/14</h1>
<p>There are two tests and five online quizzes on MapleTA</p>
<p>;wip: there is a test date conflict, email the riley guy mentioned on the course outline</p>
<p>;wip: look at the schedule and put it on the calendar</p>
<p>Probability is a tool used to model uncertainty and variability, like size, weight, and height. It allows us to work with uncertainty, though it cannot eliminate it.</p>
<h2 id="definition">Definition</h2>
<p>We first specify the <strong>outcome/event</strong> that we are interested in that might occur in a particular setting. This setting is the <strong>experiment/process</strong>.</p>
<p>For example, rolling a 2 on a 6-sided die has an outcoem of 2 and rolling the die is the experiment.</p>
<p>The <strong>sample space</strong> is the set of all possible outcomes. For example, the die has a sample set of <span class="math">\(S = \set{1, 2, 3, 4, 5, 6}\)</span>. We usually represent the sample space as <span class="math">\(S\)</span>.</p>
<p>The <strong>classical definition</strong> of the probability of an event is the ratio of the number of ways the event can occur to the total number of outcomes in <span class="math">\(S\)</span>.</p>
<p>However, this only works if all outcomes are equally likely to occur. In practice, this is not always the case.</p>
<p>The <strong>relative frequency definition</strong> of the probability of an event is the fraction of times when the event occurs when we repeat the experiment a large number of times.</p>
<p>For example, if we toss a coin a large number of times, the fraction approaches 50%. However, we cannot always repeat the experiment due to time and other constraints. Additionally, we have to keep the setting exactly the same.</p>
<p>We generally mean that we repeat the experiment an infinite number of times. In practice, we often settle for a lot less.</p>
<p>The <strong>subjective definition</strong> of the probability of an event is the confidence of the person making the statement that the event will occur. Often, this is based on the person's experiences.</p>
<h3 id="probability-model">Probability model</h3>
<ol style="list-style-type: decimal">
<li>Identify the experiment and the events of interest.</li>
<li>Define the sample space <span class="math">\(S\)</span> of the random experiment.</li>
<li>Define the subset of the sample space to which we can assign probabilities.</li>
<li>Assign the probabilities to events somehow.</li>
</ol>
<p>A <strong>discrete sample space</strong> is one where there is a finite number of simple events.</p>
<p>A <strong>simple event</strong> is an event that is made up of only one outcome, synonymous with outcome, like rolling a die and getting 2.</p>
<p>A <strong>compound event</strong> is an event made up of more than one outcome, like rolling a die and getting an even number (outcomes 2, 4, and 6 trigger the event).</p>
<p>Let <span class="math">\(S = \set{a_1, \ldots, a_n}\)</span>. Let <span class="math">\(P(a_i)\)</span> be the probability of the outcome <span class="math">\(a_i\)</span> taking place. Then <span class="math">\(0 \le P(a_i) \le 1\)</span> and <span class="math">\(\sum_{i = 1}^n P(a_i) = 1\)</span>.</p>
<p>The function <span class="math">\(P(x)\)</span> is the <strong>probability distribution</strong> on <span class="math">\(S\)</span>.</p>
<p>For a die, <span class="math">\(P(a_i) = \frac 1 6\)</span>.</p>
<p>When we write <span class="math">\(P(a, b, c)\)</span>, we mean the probability of either <span class="math">\(a\)</span>, <span class="math">\(b\)</span>, or <span class="math">\(c\)</span> occurring.</p>
<p>The <strong>odds</strong> of an event <span class="math">\(x\)</span> occurring is <span class="math">\(\frac{P(x)}{1 - P(x)}\)</span>. So odds of 3:1 is a 25% probability. To convert odds like <span class="math">\(a\)</span>:<span class="math">\(b\)</span> to a probability, we simply do <span class="math">\(P(x) = \frac b {a + b}\)</span>.</p>
<h1 id="section-1">7/5/14</h1>
<p>;wip: do the mapleta stuff on LEARN</p>
<p>If we toss a coin twice, what is the probability of getting exactly one heads?</p>
<blockquote>
<p>Clearly, <span class="math">\(S = \set{(Tails, Tails), (Tails, Heads), (Heads, Tails}, (Heads, Heads)\)</span>. Let <span class="math">\(A\)</span> be the event of getting one heads.<br />Clearly, each outcome is equally likely, so occurs 25% of the time.<br />We care about two of the outcomes, <span class="math">\((Tails, Heads)\)</span> and <span class="math">\((Heads, Tails)\)</span>.<br />So the probability of exactly one heads is <span class="math">\(P(A) = \frac{Event Outcomes}{Total Outcomes} = \frac{2}{4} = \frac{1}{2}\)</span>.<br />Alternatively, if we were to use <span class="math">\(S = \set{0 Heads, 1 Heads, 2 Heads}\)</span>, each outcome would no longer be equally likely - it would be a mistake to say the probability is <span class="math">\(\frac{1}{3}\)</span>.<br />Essentially, we found the probability <span class="math">\(P(A) = P((Tails, Heads)) + P(Heads, Tails)\)</span>.</p>
</blockquote>
<p>What is the probability that the sum of two dice rolls is 5?</p>
<blockquote>
<p>Clearly, there are <span class="math">\(6 \times 6 = 36\)</span> possible outcomes, all equally likely. Basically, <span class="math">\(S = \set{(1, 1), \ldots, (6, 6)}\)</span>. Let <span class="math">\(A\)</span> represent the event of the sum being 5.<br />Clearly, the combinations resulting in a total of 5 are <span class="math">\((1, 4), (2, 3), (3, 2), (4, 1)\)</span>.<br />Since there are 4 outcomes, <span class="math">\(P(A) = \frac{4}{36} = \frac{1}{9}\)</span>.<br />If two indistinguishable dice were used, then we could no longer tell which die was which. Since the pairs in <span class="math">\(S\)</span> are no longer ordered, we remove the duplicates to obtain 21 possibilities. However, they now do not all have the same probability of occurring - <span class="math">\((2, 2)\)</span> is only half as likely as <span class="math">\((1, 2)\)</span> since <span class="math">\((1, 2) = (2, 1)\)</span> when the dice are indistinguishable.</p>
</blockquote>
<h2 id="counting-techniques">Counting Techniques</h2>
<p>When there are <span class="math">\(n\)</span> outcomes that are all equally likely, then the probability of each outcome is <span class="math">\(\frac{1}{n}\)</span>.</p>
<p>If job A can be done in P ways and job B can be done in Q ways, then we can do either job A or job B in <span class="math">\(P + Q\)</span> ways. Here, &quot;or&quot; becomes addition. For example, the probabiity of getting heads or tails is <span class="math">\(P(Heads) + P(Heads) = \frac 1 2 + \frac 1 2 = 1\)</span></p>
<p>If job A can be done in P ways, and for each of those P ways, job B can be done in Q distinct ways, then we can do both job A and B in <span class="math">\(p \times q\)</span> ways. For example, <span class="math">\(P((Heads, Heads)) = P(Heads) AND P(Heads) = \frac 1 2 \times \frac 1 2 = \frac{1}{4}\)</span>.</p>
<p>Sampling <strong>with replacement</strong> means that every time we select an object for sampling, we simply put it back into the population, so we could potentially sample it again.</p>
<p>This means that what we get on each selection will not affect subsequent selections. For example, a coin flip has both sides of the coin replaced after each sample, so there is always the same probability of getting heads or tails.</p>
<p>Sampling <strong>without replacement</strong> means that after selecting an object, we do not put it back. Therefore, an object can only be selected at most once.</p>
<p>This means that what we get on each selection will influence sebsequent selections and is influenced by previous selections. For example, sampling genders of students from a classroom without replacement results in a different probabilities of choosing a certain gender.</p>
<p>Sampling without replacement has a big effect on small samples, but for larger samples its effect becomes negligible.</p>
<p>Many problems have sample spaces that are a set of arrangements - permutations. <span class="math">\(n^{(r)} = \frac{n!}{(n - r)!}\)</span> means &quot;<span class="math">\(n\)</span> to <span class="math">\(r\)</span> factors&quot; and is the number of <span class="math">\(r\)</span>-permutations - arrangements of length <span class="math">\(r\)</span> of the <span class="math">\(n\)</span> elements without duplicates. ;wip: nPr notation</p>
<p>For example, consider a set of 20 people's birthdays:</p>
<blockquote>
<p>Clearly, <span class="math">\(S\)</span> is a set containing 20 dates (each one of 365 days), so there are <span class="math">\(365^{20}\)</span> outcomes.<br />What is the probability of everyone having a different birthdays? Clearly, everyone has a different birthday if and only if the ordered birthdays are a 20-permutation of <span class="math">\([365]\)</span>. This is because the 20-permutations account for all possible sequence of <span class="math">\([365]\)</span> where all the dates are unique.<br />Therefore, there are <span class="math">\(365^{(20)}\)</span> possible outcomes, and since they are all equally likely, the probability is <span class="math">\(\frac{365^{(20)}}{365^{20}} \approxeq 59%\)</span>.</p>
</blockquote>
<h1 id="section-2">9/5/14</h1>
<p>;wip: do the mapleta quiz available on Wednesday, due on Wednesday</p>
<p>The factorial function grows extremely quickly and can be difficult to calculate for large numbers. Therefore, we have various approximations that help us do this more easily.</p>
<p><strong>Stirling's approximation</strong> is <span class="math">\(n! \approxeq n^n e^{-n} \sqrt{2 \pi n}\)</span>. This approximation is asymptotically equivalent to the factorial function - as <span class="math">\(n \to \infty\)</span>, the percentage error gradually decreases. ;wip: memorize this</p>
<p>The <strong>complement</strong> of an event <span class="math">\(A\)</span> is the opposite event - the event of <span class="math">\(A\)</span> not <strong>occurring</strong>. It is represented using <span class="math">\(\overline A\)</span> or <span class="math">\(A^C\)</span>. It is always true that an event occurs, or it does not occur, so <span class="math">\(P(A) + P(\overline A) = 1\)</span>.</p>
<p>A 4-digit PIN code is selected with replacement. What is the probability that the number is even? What is the probability that it contains at least one &quot;1&quot;?</p>
<blockquote>
<p>Clearly, the sample space is <span class="math">\(S = \set{0000, \ldots, 9999}\)</span>, with 10000 possible outcomes.<br />Clearly, all possible outcomes are equally likely because the PIN codes are selected randomly.<br />Clearly, if the number is even, then the last digit must be in <span class="math">\(\set{0, 2, 4, 6, 8}\)</span>.<br />Clearly, there are 5 possible last digits, and the other three digits can be any of the ten digits.<br />So there are <span class="math">\(10 \times 10 \times 10 \times 5\)</span> possible outcomes, or <span class="math">\(5000\)</span>.<br />So the probability of an even number is <span class="math">\(\frac{5000}{10000}\)</span>, or 50%.<br />To find the probability of containing at least one &quot;1&quot; digit, we consider the cases where there is one &quot;1&quot; digit, two, three, or four.<br />Alternatively, we can consider all the numbers that contain no &quot;1&quot; digits - the complement of the number containing at least one &quot;1&quot;. This is a more efficient way of finding the answer.<br />Clearly, if there are no &quot;1&quot; digits, then every digit can be anything but &quot;1&quot;, so there are 9 possibilities. There are therefore <span class="math">\(9 \times 9 \times 9 \times 9 = 6561\)</span> possible PIN codes without any &quot;1&quot; digits.<br />Clearly, the event of having no &quot;1&quot; digits is the complement of the event of interest, so <span class="math">\(P(1 occurs) = 1 - P(1 does not occur)\)</span>. Since <span class="math">\(P(1 does not occur) = \frac{6561}{10000}\)</span>, the probability of a &quot;1&quot; occurring is 34.39%.</p>
</blockquote>
<p>Now we derived the formula for <span class="math">\(n \choose r\)</span>, read the MATH239 notes from yesterday to get the same material.</p>
<h1 id="section-3">12/5/14</h1>
<p><span class="math">\({n \choose r} = \frac{n!}{r!(n - r)!} = \frac{n^{(r)}}{r!}\)</span>.</p>
<p>Given an event <span class="math">\(B\)</span>, <span class="math">\(P(B) = 1 - P(\overline B)\)</span>.</p>
<p>Also, <span class="math">\(n^{(k)} = n(n - 1)^{(k - 1)}\)</span> for <span class="math">\(k \ge 1\)</span> and <span class="math">\({n \choose k} = {n \choose n - k}\)</span>.</p>
<p>The Binomial theorem states that <span class="math">\((1 + x)^n = \sum_{k = 0}^n {n \choose k}x^k\)</span></p>
<p>What is the probability that a random arrangement of the letters in &quot;STATISTICS&quot; begins and ends with S?</p>
<blockquote>
<p>Clearly, the total number of arrangements is <span class="math">\(10!\)</span>, since there are 10 letters.<br />Let <span class="math">\(W\)</span> be a permutation. Assume <span class="math">\(W\)</span> begins and ends with an S.<br />Then the remaining letters are &quot;TATISTIC&quot;, and there are <span class="math">\(8!\)</span> possible permutations of these letters.<br />Therefore, there are <span class="math">\(8!\)</span> event outcomes and <span class="math">\(10!\)</span> total outcomes, so the probability is <span class="math">\(\frac{8!}{10!} = \frac 1 {90}\)</span>.</p>
</blockquote>
<p>What are the unique anagrams of the letters of &quot;STATISTICS&quot;?</p>
<blockquote>
<p>Clearly, there are 3 S's, 3 T's, 1 A, 2 I's, and 1 C, with 10 letters total. Then there are <span class="math">\(10!\)</span> possible permutations.<br />Clearly, for each permutation, the S, T, and I letters can be swapped around without changing the anagram - it doesn't matter which S is first or second.<br />So there are <span class="math">\(3!\)</span> permutations representing the same anagram due to S, and for each of these a factor of <span class="math">\(3!\)</span> more due to T, and for each of these a factor of <span class="math">\(2!\)</span> more due to I.<br />So there are <span class="math">\(3! 3! 2!\)</span> duplicate permutations for each anagram, and therefore <span class="math">\(\frac{10!}{3! 3! 2!}\)</span> unique anagrams.</p>
</blockquote>
<p>If we have <span class="math">\(n_i\)</span> symbols of type <span class="math">\(i\)</span>, with <span class="math">\(n = n_1 + \ldots + n_k\)</span>, then the number of arrangements using <span class="math">\(n\)</span> symbols is <span class="math">\({n \choose n_1} \times {n - n_1 \choose n_2} \times {n - n_1 - n_2 \choose n_3} \times \ldots \times {n_k \choose n_k} = \frac{n!}{n_1! n_2! \cdots n_k!} = \frac{(n_1 + \ldots + n_k)!}{n_1! n_2! \cdots n_k!}\)</span>.</p>
<h1 id="section-4">14/5/14</h1>
<p>How many ways can the 4 aces in a deck of 52 cards all be adjacent?</p>
<blockquote>
<p>Assume the aces are all adjacent. Then we can consider the four aces as a single large unit.<br />Clearly, there are <span class="math">\(4!\)</span> ways to arrange these 4 aces within the unit.<br />Clearly, for each of these ways there are <span class="math">\(48!\)</span> ways to arrange the other 48 cards.<br />Clearly, there are 49 different places to insert the other cards.<br />So there are <span class="math">\(49 \times 4! \times 48!\)</span> ways the 4 aces can be adjacent.</p>
</blockquote>
<p>How many ways can one choose 13 cards from a deck and have two of them be aces?</p>
<blockquote>
<p>Clearly, there are <span class="math">\(52 \choose 2\)</span> ways to choose the two aces.<br />For each of these ways, there are <span class="math">\(50 \choose 11\)</span> ways to choose the other cards.<br />So there are <span class="math">\({52 \choose 2} {50 \choose 11}\)</span> ways.</p>
</blockquote>
<p>What is the probability of choosing a 6-4-2-1 split between the suits from a deck of cards?</p>
<blockquote>
<p>Assume we have chosen 13 cards.<br />Clearly, there are <span class="math">\(4!\)</span> permutations of suits we can split between.<br />Clearly, there are <span class="math">\(13 \choose 6\)</span> ;wip: is this right? check the lecture slides Clearly, the probability is <span class="math">\(\frac{4! {13 \choose 6} {13 \choose 4} {13 \choose 2} {13 \choose 1}}{52 \choose 13}\)</span>.</p>
</blockquote>
<p>The Multinomial Theorem says that <span class="math">\((a_1 + \ldots + a_k)^n = \sum_{x_1, \ldots, x_k} \frac{n!}{x_1! \cdots x_k!} a^{x_1} \cdots a^{x_k}\)</span>.</p>
<p>The Hypergeometric identity says that <span class="math">\(\sum_{x = 0}^\infty {a \choose x} {b \choose n - x} = {a + b \choose n}\)</span>.</p>
<h2 id="probability-rules">Probability Rules</h2>
<p>Events are simply sets of outcomes. We can use things like set notation and similar when working with events.</p>
<p>If an event <span class="math">\(A\)</span> consists of the outcomes <span class="math">\(\set{a_1, \ldots, a_n}\)</span>, then <span class="math">\(P(A) = P(a_1) + \ldots + P(a_n)\)</span>.</p>
<p>Also, <span class="math">\(0 \le P(A) \le 1\)</span>. This can be proven by proving <span class="math">\(P(S) = 1\)</span> (probability of one of any of the possible outcomes occurring is 1), and that <span class="math">\(P(A) \le P(S)\)</span>.</p>
<p>If <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are events, and <span class="math">\(A \subseteq B\)</span>, then <span class="math">\(P(A) \le P(B)\)</span>. This can be proven by the comparing the sum of the probabilities of the outcomes.</p>
<h3 id="venn-diagrams">Venn Diagrams</h3>
<p>Venn diagrams can be used to visually represent the events resulting from set operations on events.</p>
<p>These diagrams have a rectangle representing <span class="math">\(S\)</span>, the sample space, and circles within the rectangle representing events. We can label the circles with names anywhere inside them. Sometimes, we also write some of the possible outcomes in the circles or rectangle.</p>
<p>We can have circles that overlap each other to represent them sharing outcomes. To represent the resulting event, we can shade in the area representing the set of the event's outcomes.</p>
<p>For example, a union of two events <span class="math">\(A\)</span> and <span class="math">\(B\)</span> (represented as <span class="math">\(A \cup B\)</span>) means we shade in both events (even if they intersect), and a conjunction of two events <span class="math">\(A\)</span> and <span class="math">\(B\)</span> (represented <span class="math">\(A \cap B\)</span> or <span class="math">\(AB\)</span>) would mean we only shade in the intersection of the two events. The complement of an event <span class="math">\(A\)</span> (represented <span class="math">\(\overline A\)</span>) simply shades in everything in the rectangle that is not in the event.</p>
<h1 id="section-5">16/5/14</h1>
<p>;wip: buy the stats textbook</p>
<p>De Morgan's Laws:</p>
<ol style="list-style-type: decimal">
<li><span class="math">\(\overline{A \cup B} = \overline A \cap \overline B\)</span></li>
<li><span class="math">\(\overline{A \cap B} = \overline A \cup \overline B\)</span></li>
</ol>
<p>This can be proved by proving the left set is a subset of the right set, and the right set is a subset of the left set.</p>
<p>Basically, we can &quot;break the bar&quot; and &quot;flip the operator&quot; when we have a negation of a disjunction or conjunction.</p>
<p>The probability of a union is the sum of the probabilities of either event, minus the probability that they both occur: <span class="math">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span>. The last term is because when we add <span class="math">\(P(A)\)</span> and <span class="math">\(P(B)\)</span>, we added <span class="math">\(P(A \cap B)\)</span> twice, so we subtract it once to get the correct amount. This becomes clear if we draw a Venn diagram of the equation.</p>
<p>The probability of a union of more events is <span class="math">\(P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)\)</span>. The reason for the negative terms is because the conjunctions are being added twice, so we subtract them once to get the correct amount.</p>
<p><span class="math">\(P(A_1 \cup \ldots \cup A_n) = \sum_{i = 1}^n P(A_i) - \sum_{i &lt; j} P(A_i A_j) + \sum_{i &lt; j &lt; k} P(A_i A_j A_k) -\)</span> ;wip: get it from the slides or online</p>
<p>Events <span class="math">\(A_1, \ldots, A_n\)</span> are <strong>mutually exclusive</strong> if <span class="math">\(\forall 1 \le i \le n, 1 \le j \le n, i \ne j \implies A_i \cap A_j = \emptyset\)</span>, so <span class="math">\(P(A_i \cap A_j) = \emptyset\)</span>. They are basically events that can never happen together. Two events <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are mutually exclusive if <span class="math">\(P(A \cap B) = 0\)</span> or <span class="math">\(A \cap B = \emptyset\)</span>.</p>
<p>The equation <span class="math">\(P(A) = 1 - P(\overline A)\)</span> comes from the idea of mutual exclusivity:</p>
<blockquote>
<p>Clearly, <span class="math">\(S = A \cup \overline A\)</span>, and <span class="math">\(P(S) = 1\)</span>.<br />So <span class="math">\(1 = P(S) = P(A \cup \overline A) = P(A) + P(\overline A) - P(A \cap \overline A) = P(A) + P(\overline A)\)</span>.</p>
</blockquote>
<h1 id="section-6">21/5/14</h1>
<p>When our event consists of the majority of the outcomes in the sample space, it is often easier to calculate using the complement - it is generally easier to consider fewer outcomes.</p>
<p>For example, if we wanted to find the probability that at least one of two dice rolls is a 6, there are multiple outcomes - the first is 6, the second is 6, or both. If we use the complement, we can simply find the probability that none of the rolls are 1, which is a simpler calculation.</p>
<p>Two events are <strong>independent</strong> if and only if <span class="math">\(P(A \cap B) = P(A) P(B)\)</span>, and dependent otherwise. If two events are dependent, then they have some sort of relationship or association.</p>
<p><span class="math">\(n\)</span> events are <strong>pairwise independent</strong> if and only if for any <span class="math">\(1 \le i \le n, 1 \le j \le n, i \ne j\)</span>, <span class="math">\(P(A_i \cap A_j) = P(A_i) P(A_j)\)</span>. This means that the events in every possible pair of events is independent to each other.</p>
<p><span class="math">\(n\)</span> events are <strong>mutually independent</strong> if and only if <span class="math">\(P(A_1 \cap \ldots \cap A_n) = P(A_1) \cdots P(A_n)\)</span>. This means that every event is independent of every possible intersection of all other events. Mutual independence implies pairwise independence, but not the other way around.</p>
<p>For example, separate dice rolls are independent, because they are unrelated to one another. Independence means that whether <span class="math">\(A\)</span> happens or not has no effect on whether <span class="math">\(B\)</span> happens, and vice versa.</p>
<p>If <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are independent, then <span class="math">\(A\)</span> and <span class="math">\(\overline B\)</span>, <span class="math">\(\overline A\)</span> and <span class="math">\(B\)</span>, and <span class="math">\(\overline A\)</span> and <span class="math">\(\overline B\)</span> are all independent.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, <span class="math">\(P(B) = P(AB) + P(\overline A B)\)</span>. Since <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are independent, <span class="math">\(P(AB) = P(A)P(B)\)</span> and <span class="math">\(P(B) = P(A)P(B) + P(\overline A B)\)</span>.<br />So <span class="math">\(P(B) - P(A)P(B) = P(\overline A B) = P(B)(1 - P(A)) = P(B)P(\overline A)\)</span>, and <span class="math">\(P(\overline A B) = P(\overline A) P(B)\)</span>, so <span class="math">\(\overline A\)</span> and <span class="math">\(B\)</span> are independent.</p>
</blockquote>
<p>Given a large set of elements <span class="math">\(S\)</span> with properties <span class="math">\(W\)</span> and <span class="math">\(F\)</span> such that <span class="math">\(P(F) = 0.15, P(W) = 0.45\)</span>, and that if <span class="math">\(W\)</span>, then <span class="math">\(P(F) = 0.2\)</span>, what is the probability of, in 10 randomly selected elements, at least 1 being <span class="math">\(W\)</span> and 1 being <span class="math">\(F\)</span>?</p>
<blockquote>
<p>Let <span class="math">\(T\)</span> be a set of 10 randomly selected elements.<br />Since <span class="math">\(S\)</span> is large and <span class="math">\(T\)</span> is small, we can pretend we are selecting with replacement even though it is without replacement. This is because the probabilities would not hold if we did not do replacement.<br />Let <span class="math">\(W_i\)</span> or <span class="math">\(F_i represent the \)</span>i<span class="math">\(th element of \)</span>T$ being <span class="math">\(W\)</span> or <span class="math">\(F\)</span>, respectively.<br />Let <span class="math">\(W_a = W_1 \cup \ldots \cup W_{10}\)</span>, <span class="math">\(F_a = F_1 \cup \ldots \cup F_{10}\)</span>.<br />Clearly, the probability is <span class="math">\(P(W_a \cap F_a) = 1 - P(\overline{W_a \cap F_a}) = 1 - P(\overline{W_a} \cup \overline{F_a}) = 1 - P(\overline{W_a}) - P(\overline{F_a}) + P(\overline{W_a} \cap \overline{F_a})\)</span>.<br />Clearly, <span class="math">\(P(\overline{W_a}) = P(\overline{W_1}) \cdots P(\overline{W_{10}}) = (1 - 0.45)^{10} = 0.55^{10}\)</span>.<br />Clearly, <span class="math">\(P(\overline{F_a}) = P(\overline{F_1}) \cdots P(\overline{F_{10}}) = (1 - 0.15)^{10} = 0.85^{10}\)</span>.<br />We want to find <span class="math">\(P(\overline{F_a})\)</span> given that <span class="math">\(\overline{W_a}\)</span>. This will allow us to find <span class="math">\(P(\overline{W_a} \cap \overline{F_a})\)</span>.<br />Clearly, <span class="math">\(P(W_i) = 0.45 = P(W_i \cap \overline{F_i}}) + P(W_i \cap F_i) = P(W_i \cap \overline{F_i}) + 0.09\)</span>, since <span class="math">\(P(F_i \cap W_i)\)</span> means we assume that <span class="math">\(W_i\)</span> and <span class="math">\(P(F_i)\)</span> then becomes 0.2.<br />So <span class="math">\(P(W_i \cap \overline{F_i}) = 0.36\)</span>.<br />Clearly, <span class="math">\(P(\overline{F_i}) = 1 - 0.15 = P(\overline{F_i} \cap W_i) + P(\overline{F_i} \cap \overline{W_i}) = 0.36 + P(\overline{F_i} \cap \overline{W_i})\)</span>.<br />So <span class="math">\(P(\overline{F_i} \cap \overline{W_i}) = 0.49\)</span> and <span class="math">\(P(\overline{W_a} \cap \overline{F_a}) = 0.49^{10}\)</span>.<br />So <span class="math">\(P(W_a \cap F_a) = 1 - 0.55^{10} - 0.85^{10} + 0.49^{10} \approxeq 0.801390566701062\)</span>.</p>
</blockquote>
<p>This demonstrated the very useful identity <span class="math">\(P(A) = P(A \cap B) + P(A \cap \overline B)\)</span>, where <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are events.</p>
<h1 id="section-7">23/5/14</h1>
<p>The conditional probability of an event <span class="math">\(A\)</span> given event <span class="math">\(B\)</span> is denoted <span class="math">\(P(A \mid B) = \frac{P(A \cap B)}{P(B)}\)</span>, assuming that <span class="math">\(P(B) \ne 0\)</span>.</p>
<p>This is the probability that <span class="math">\(A\)</span> will take place, given that <span class="math">\(B\)</span> is already known to take place.</p>
<p>If <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are independent, then <span class="math">\(P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A) P(B)}{P(B)} = P(A)\)</span>. In other words, for independent events it does not matter whether we know <span class="math">\(B\)</span> occurred or not; the probability is still the same.</p>
<p>If 5% of males are color blind and 0.25% of females are as well, what is the probability of a random color blind person selected from an equal number of males and females being male?</p>
<blockquote>
<p>Let <span class="math">\(M\)</span> represent being male and <span class="math">\(F\)</span> represent being female in the population. Clearly, <span class="math">\(P(M) = P(F) = 0.5\)</span>. Let <span class="math">\(C\)</span> represent being color blind in the population.<br />Clearly, <span class="math">\(P(C \mid M) = 0.05\)</span>, <span class="math">\(P(C \mid F) = 0.0025\)</span>, and <span class="math">\(F = \overline M\)</span>. We want to find <span class="math">\(P(M \mid C)\)</span>, the probability that the person is male given color blindness.<br />Clearly, <span class="math">\(P(M \mid C) = \frac{P(M \cap C)}{P(C)}\)</span>.<br />Clearly, <span class="math">\(P(C \mid M) = \frac{P(M \cap C)}{P(M)}\)</span>, so <span class="math">\(P(C \mid M) P(M) = P(M \cap C) = 0.05 \times 0.5 = 0.025\)</span>.<br />Clearly, <span class="math">\(P(C) = P(C \cap \overline M) + P(C \cap M) = P(C \mid \overline M) P(\overline M) + P(C \mid M) P(M) = P(C \mid \overline M) P(\overline M) + P(C \mid M) P(M) = P(C \mid F) P(F) + P(C \mid M) P(M) = 0.0025 \times 0.5 + 0.05 \times 0.5 = 0.02625\)</span>.<br />So <span class="math">\(P(M \mid C) = \frac{0.025}{0.02625} \approxeq 0.952380952380952\)</span>.<br />Note that <span class="math">\(P(C)\)</span> is the weighted average of all the possible conditional probabilities.</p>
</blockquote>
<h3 id="multiplication-rule">Multiplication Rule</h3>
<p>The <strong>multiplication rule</strong> states that <span class="math">\(P(A \cap B) = P(A) P(B \mid A)\)</span>. Extending this, <span class="math">\(P(ABC) = P(A)P(B \mid A)P(C \mid (A \cap B))\)</span> and <span class="math">\(P(ABCD) = P(A) P(B \mid A) P(C \mid (A \cap B)) P(D \mid (A \cap B \cap C))\)</span>, and so on.</p>
<h3 id="partition-rule">Partition Rule</h3>
<p>The <strong>partition rule</strong> states that if <span class="math">\(A_1 \cup \ldots \cup A_k = S\)</span> where <span class="math">\(A_1, \ldots, A_k\)</span> are disjoint sets (mutually exclusive), and <span class="math">\(B\)</span> is an event in <span class="math">\(S\)</span>, then <span class="math">\(P(B) = P(B \cap A_1) + \ldots + P(B \cap A_k) = \sum_{i = 1}^k P(B \mid A_i)P(A_i)\)</span>.</p>
<p>This is because <span class="math">\(B = B \cap A_1 \cup \ldots \cup B \cap A_k\)</span>, and <span class="math">\(B \cap A_i\)</span> is mutually exclusive with any <span class="math">\(B \cap A_j\)</span> when <span class="math">\(i \ne j\)</span>. So <span class="math">\(P(B) = P(B \cap A_1 \cup \ldots \cup B \cap A_k) = P(B \cap A_1) + \ldots + P(B \cap A_k)\)</span>.</p>
<p>A <strong>tree diagram</strong> is a diagram that helps represent conditional probabilities by showing the possibilities of several runs of an experiement as a tree. For example, we might draw a tree for flips of a coin, with each level being a subsequent flip of the coin.</p>
<p>When we move downwards in a tree, we multiply the probabilities together. When we include other nodes, we do a union and add the probabilities.</p>
<p>We label the edges of the tree with the probability of the child occurring given that all the parent events have occurred. So in the above example, the root node has labels <span class="math">\(P(H)\)</span> and <span class="math">\(P(T)\)</span>, while the second level nodes have <span class="math">\(P(H \mid H), P(T \mid H), P(H \mid T), P(T \mid T)\)</span>.</p>
<p>If we want the probability of a particular sequence of outcomes, then we would travel down the tree multplying by edges when we encounter them. So in the above example, the probability of getting a heads, and then another heads is <span class="math">\(P(H \cap H) = P(H) P(H \mid H) = 0.25\)</span>.</p>
<p>We are eventually going to develop <strong>Bayes Theorem</strong>, which is <span class="math">\(P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B \mid \overline A) P(A) + B(B \mid A) P(A)}\)</span>.</p>
<h1 id="section-8">26/5/14</h1>
<p><span class="math">\(P(A \mid B) = 1 - P(\overline A \mid B) \ne 1 - P(A \mid \overline B)\)</span>.</p>
<p>If <span class="math">\(P(H \cap M) = 0.1\)</span> and <span class="math">\(P(H \cap \overline M) = 0.15\)</span>, find <span class="math">\(P(M \mid H)\)</span>:</p>
<blockquote>
<p>Clearly, <span class="math">\(P(H) = P(H \cap M) + P(H \cap \overline M) = 0.25\)</span>.<br />Clearly, <span class="math">\(P(M \mid H) = \frac{P(M \cap H)}{P(H)} = \frac{0.1}{0.25} = 0.4\)</span>.</p>
</blockquote>
<h2 id="discrete-random-variablesprobability-models">Discrete Random Variables/Probability Models</h2>
<p>Probability models are used to describe outcomes associated with random processes.</p>
<p>Until this point we have been using sets in sample spaces to describe these outcomes.</p>
<p>Now we introduce <strong>random variables</strong>. A random variable is a function that assigns a real number in a set <span class="math">\(A\)</span> to each outcome in a sample space <span class="math">\(S\)</span>. They are denoted with capital letters <span class="math">\(X, Y, \ldots\)</span> and their values are denoted with lowercase letters <span class="math">\(x, y, \ldots\)</span>.</p>
<p>Random variables are defined for every outcome of an experiment - <span class="math">\(X: S \to A\)</span>.</p>
<p>For example, if <span class="math">\(S\)</span> is the set of possible results of 3 coin tosses, <span class="math">\(\set{TTT, TTH, THT, THH, HTT, HTH, HHT, HHH}\)</span>, we might have a random variable <span class="math">\(X\)</span> map each outcome in <span class="math">\(S\)</span> to the number of heads in that outcome.</p>
<p>We can also invert <span class="math">\(X\)</span> to get a set of all outcomes in <span class="math">\(S\)</span> for a given value <span class="math">\(x\)</span>.</p>
<p>We are interested in finding <span class="math">\(P(X = x)\)</span>. This is the probability that <span class="math">\(x\)</span> occurs, and random variables give us new tools to work with them. In the above example, <span class="math">\(P(X = 3) = \frac 1 8\)</span> because there is 1 outcome with 3 heads and 8 outcomes total.</p>
<p>There are two types of random variables - <strong>discrete</strong> and <strong>continuous</strong>. Discrete variables can only take on finite or countably infinite (like natural numbers) values. Continuous variables can take on values in some interval of real numbers.</p>
<p>The <strong>probability function/probability mass function</strong> (PMF/PF) of a random variable <span class="math">\(X\)</span> is <span class="math">\(f(x) = P(X = x)\)</span>, defined for all <span class="math">\(x \in A\)</span>. <span class="math">\(f(x)\)</span> is a probability function if and only if <span class="math">\(f(x) \ge 0\)</span> and <span class="math">\(\sum_{x \in A} f(x) = 1\)</span>. So <span class="math">\(0 \le f(x) \le 1\)</span>.</p>
<p>In the above example, <span class="math">\(f(x) = \begin{cases} P(\set{TTT}) &amp;\text{if } x = 0 \\ P(\set{HTT, THT, TTH}) &amp;\text{if } x = 1 \\ P(\set{HHT, HTH, THH}) &amp;\text{if } x = 2 \\ P(\set{HHH}) &amp;\text{if } x = 3 \end{cases} = \begin{cases} \fraac 1 8 &amp;\text{if } x = 0 \\ \frac 3 8 &amp;\text{if } x = 1 \\ \frac 3 8 &amp;\text{if } x = 2 \\ \frac 1 8 &amp;\text{if } x = 3 \end{cases}\)</span>.</p>
<p>Let <span class="math">\(f(x) = \frac{kx}{2 + x}\)</span> be a probability function that gives <span class="math">\(x \in \set{1, 2, 3, 4, 5}\)</span>. Clearly, <span class="math">\(k\)</span> must satisfy <span class="math">\(f(1) + f(2) + f(3) + f(4) + f(5) = 1 = \frac k 3 + \frac {2k} 4 + \frac {3k} 5 + \frac {4k} 6 + \frac {5k} 7 = \frac {140k + 105k + 84k + 70k + 60k} {420} = \frac{459}{420}k\)</span>, so <span class="math">\(k = \frac{420}{459}\)</span>.</p>
<p>The Cumulative Distribution Function is <span class="math">\(F(x) = P(X \le x) = \sum_{u \le x} f(u)\)</span>. It is always true that <span class="math">\(\lim_{x \to -\infty} F(x) = 0\)</span> and <span class="math">\(\lim_{x \to \infty} F(x) = 1\)</span>. Therefore, <span class="math">\(f(x) = F(x) - F(x - 1)\)</span> and <span class="math">\(P(X = x) = P(X \le x) - P(X \le x - 1)\)</span>.</p>
<hr>
<p>Copyright 2013 Anthony Zhang</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>.
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>