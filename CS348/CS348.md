CS348
=====

Introduction to Database Management.

    Edward Chan
    epfchan@uwaterloo.ca
    Office Hours: Monday, Wednesday 12:30 PM-1:15 PM, DC2338

$$
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
$$

# 14/9/15

Databases, data modelling, and relational algebra, in theory and in practice.

Assignments are due before 1 PM on their respective due dates. No late assignments. Midterm on November 6, 4:30 PM-6 PM.

# 16/9/15

Elements of Databases
---------------------

Databases are needed to manage data because we want to store and retrieve it in increasingly elaborate ways. Data is stored information.

The goal of data management is to model part of the real world in a way that is practically useful. We want to do this efficiently and accurately, and hiding irrelevant details so we can concentrate on the common properties of the set (abstraction).

**Intension** of data is the definition of data, like the definition of employee records. **Extension** of data is the actual instances of the data, such as the set of employee records - it changes with the state of the world. In databases, the intension of data is the **database schema**. and the extension of data are the actual records in the database.

A database management system (DBMS) dictates how data is organized and accessed - the structure of the data. A DBMS also exposes interfaces to store, retrieve, and update data efficiently. A modern DBMS must handle concurrent operations, ensure integrity (database constraints, such as a column being unique or a superset of another), enforce security (access control), and support versioning and recovery (ensuring data can be restored after failures such as hard drive crashes and data corruption).

# 18/9/15

Consider a banking app. The following code might be used for a transfer:

    read source account
    subtract amount to transfer
    write source account
    read target account
    add amount to transfer
    write target account

This is a very poor way of going about data management. For example, if a system failure occurs right after `write source account`, the amount to transfer is simply lost, and the system is left in an inconsistent state. Additionally, if the source account has an amount transferred into it between `read source amount` and `write source amount`, that transfer is lost. In this case, we want the entire thing to happen as a unit, so it either goes through successfully, or if there is a system failure, it doesn't happen at all - it should be **atomic**.

Logical files are those seen by application programmers, such as database tables. Physical files are those seen by system programmers, such as the actual files organized by the database.

The goal of the modern DBMS is to make the data and the applications that use it more independent. This allows us to do things like swapping out the physical storage without changing any applications, or adding and removing indicese.

DBMSs can require people like operators (data entry, machine operators), system developers, and database administrators. DBMSs also have associated procedures such as what to do in case of failures and how to use the interfaces

DBMSs have a 3-level architecture - the external level (the application programs and views), the conceptual level (the conceptual schema/data model), and the internal level (data storage, OS, hardware).

DBMSs also have various language interfaces, such as **data definition language** (DDL), which is used for defining schemas. The DDL is compiled into a system catalog, which is essentially just schema metadata. There is also the **data manipulation language**, which is the instructions from the application that allows data to be manipulated, and the **query language**, which allows data to be manipulated.

A **data model** is the logical organization of data. On top of the data model, we define **operations** to manipulate it, and constraints to ensure data integrity and enforce logical restrictions. The **entity-relational model** is the most common data model.

Objects in DBMSs can be stored by value or by reference. Storing by reference supports everything storing by value does, but can also have multiple entries reference the same object.

# 21/9/15

An **entity** is an object that has properties and can be uniquely identified. An **entity set** is a set of entities with similar properties - an extension. An **entity type** is an abstraction over an entity set - an intension. An attribute is a property of an entity, and an attribute value is an instance of an attribute. Attributes can be single-valued or multi-valued (such as arrays of other values), and simple (primitive) or composite (contains other simple/composite values). The **domain** is the set of all possible attribute values.

A **relationship** is an association between multiple entities. An $n$-ary relationship associates $n$ entities with each other. A **relationship set** is a set of similar relationships, such as those containing common entities - an extension. A **relationship type** is an abstraction over a relationship set - the intension, like WORKS_IN_DEPARTMENT in an employee databse.

Relationship and entity types can have attributes too.

When designing databases, we need to determine the functional requirements and the database requirements. Using these, we figure out the database interface (the transaction specification) and schema, and then implement the application program and set up the DBMS.

Contraints and Relationships
----------------------------

A **candidate key** is a minimal set of attributes (least possible number of attributes) of an entity type that uniquely identifies each entity in its entity set. For example, a collection of students might have student number or social insurance number as candidate key.

A **primary key** is a candidate key that is chosen (as part of the design) as the main way to identify entities in an entity set. Primary keys can be added to any entity type, but are always optional. These **key constraints** apply to entity types.

An **entity-relation diagram** represents entity types and relationship types using a flowchart-like diagram:

* Entity types are represented with labelled rectangles.
* Entity attributes are represented with ellipses, connected to entity types with lines.
    * Attributes that are part of the primary key, if there is one, have underlined labels.
* Relationships are labelled diamonds, connected to the entity types they act over with lines.
    * The cardinality is specified by labelling each of the two lines from the diamond with `(MIN_ENTITIES, MAX_ENTITIES)`. For example, a relationship between department and employee might have labels `(0, *)` for the department side, and `(1, 1)` for the employee side - departments can have any number of employees, while an employe always has just one department.
* Relationships and attributes should not be redundant - a relationship or attribute is reduntant if its removal doesn't change the meaning of the information.
    * For example, an attribute of an employee type that represents the department is redundant if there is also a relationship that associates employee entities with departments.
    * For example, a relationship between department and classes is redundant if there is already a department-student relationship and a student-classes relationship - we could get the same information as a department-class relationship by going through the department-student and student-classes relationship.

Entity-relationship diagrams often get cluttered by attribute ellipses. We can avoid this by not drawing attribute ellipses, and then also including a list with entries of the form `ENTITY_TYPE (ATTRIBUTE1, ATTRIBUTE2, ...)` to list out the entities. As before, primary key attributes are underlined in this list.

# 23/9/15

For example, for a school application we might have entity types STUDENT, PROGRAM, and CLASS. Relationships might include a STUDENT instance being ENROLLED_IN multiple CLASS entities, or a STUDENT instance MAJORED_IN a PROGRAM entity.

The **cardinality** of a relationship type restricts the number of each entity type that can be in that relationhip type. For example, for binary relationships (relationships between two entity types), common cardinalities are 1:1 (one to one, 1 of each entity type per relationship), 1:N (one to many, one or more of one entity type per each of the other), and N:M (many to many, any number of either entity type). These **cardinality constraints** apply to relationship types. The cardinality, essentially, retricts the number of relationships an entity can can participate in.

The **existance constraint** constrains whether an entity can exist independently of something else. For example, a department should not exist if there are no employees in it. Since a department is always associated with an employee. An entity type is **totally dependent** on another if every entity of the first is always associated with at least one of the second - the first cannot exist without the second.

Relationship Types
------------------

An **is-a** relationship represents subclass entity types inheriting from a superclass entity type, and is drawn in entity-relationship diagrams as a down-pointing triangle labelled "ISA". The superclass entity type is connected by a line, above the triangle, and the subclass entity types are connected by lines below it. Also, attributes of the superclass are inherited by subclasses.

A **generalization** is when multiple related entity types are combined into a single, more general entity type. In ERDs, we make the new generalized entity type, and then add an is-a relationship between the specialized entity types and the new general entity type.

# 25/9/15

A **specialization** is when a single general entity type is broken down into multiple, more specific entity types. In ERDs, we make the new specialized entity types, and then add an is-a relationship between these new entity types and the general entity type.

An **aggregation** is when a relationship between multiple entities is itself treated like an entity type, like having relationships between that relationship and other entity types. In ERDs, we draw these as a box around the entire relationship diamond and any entity types it relates together.

For example, suppose a school offers courses, and students can enquire into offered courses, we can model this using a school entity type, related with a course entity type with a "OFFERS" relationship, and a student entity type. Here, we could have a "ENQUIRES_ABOUT" relationship between students, schools, and courses, but this rather cumberome. Instead, we can use aggregation - we treat the OFFERS relationship like an entity, and relate students to "OFFERS" using an "ENQUIRES_ABOUT" relationship.

# 28/9/15

Databases are often too large to fit in main memory. Therefore, they are generally stored on disk drives or SSDs.

Disk drives have disk packs (disks on a spindle), which have multiple platters (disks), each with many concentric tracks (circles). Tracks are divided into equally sized pages/blocks/sectors (chunks), each of which stores around 512 bytes to 4 kilobytes. The tracks with corresponding radii on each platter form a cylinder.

The disk pack is rotated at a constant speed (typically thousands of RPM), and read/write heads move back and forth over the disks to read/write the data. The blocks on each track have the track number (the cylinder in which the block resides), surface number (which platter the block is on), and block number within the track. Blocks are the unit of transfer and are uniquely identified by these 3 values.

To perform a read/write a block on a disk drive, the read/write head has to seek to the right track, wait for the disk to spin around to the right track, and then perform the read/write until done. As this is a mechanical process, this can take multiple milliseconds - two orders of magnitude greater than CPU operations.

As a result of the mechanical aspects of disk drives, it's a good idea to put related data close together on the same track, in order to avoid seek times and rotational delay when reading related information - this is known as **clustering**. We also try to cache recently used values (and marking modified cache entires as dirty to write them back later), in order to avoid disk operations entirely when possible.

Solid state drives are a lot better, with access times around 150 nanoseconds, but are still very slow relative to the CPU.

A **file** is a sequence of records (fixed or variable length). Each record is stored on at most one block. The blocks storing records can be contiguous (side by side in memory), linked (blocks point to their neighbors), or indexed (a separate mapping is maintained for where certain data is located).

An **ordered/sequential** file has its records sorted by a particular attribute, the **ordering field**. This allows us to find records very fast using binary search, but it also slows down insertions - one common pattern is to accumulate records to insert in an overflow file, and merge it in periodically.

An **index** is extra information added to a file to provide faster accesses at the cost of slower writes. The index is generally a separate file, so there is a data file and an index file.

Indices are built on a particular set of attributes, called the **search key**. A **primary index** is one built on the primary key alone, and any other is a **secondary index**. An **ordered** index keeps the search key in sorted order, while an **unordered** index does not.

Indices are generally implemented using B+ trees. These are very good for range queries, and can easily be inserted into or removed from.

# 30/9/15

B+ trees are similar to the B trees introduced in CS240, but with a few important differences:

* The interior nodes do not contain values, only ranges for its children.
* The leaf nodes are linked to their adjacent siblings, forming a doubly linked list of leaf nodes that makes in-order traversal very easy.

The index is a useful concept because it is independent of the data itself - we can manage indices without touching the data at all.

Although indices make data access faster, they also take up additional space, and slow down data insertion/deletion, as we also have to manage structures like th B+ tree.

The B+ tree represents data as the leaf nodes of a tree in **data blocks**, each of which can contain multiple entries (these are generally pages on the disk). Entries within a data block contain the key and a reference/pointer to a record in the database, and are sorted by their key.

The interior nodes of the tree, the **index blocks**, contain references to other blocks for each range they contain.

Each index block will have a lot of ranges, generally around 70 or more. Each block is limited only by the size of disk pages, and we try to fill the pages as much as possible to minimise the number of page accesses - the most expensive operation. Also, index blocks are cached as much as possible, especially the upper levels, in order to avoid the first few page accesses.

A B+ tree of order $m$ is a search tree in which all leaves are on the same level. Every interior node has between $\floor{\frac m 2}$ and $m - 1$ keys sorted in asecending order (except the root, which can have between 1 and $m - 1$), and has children for each range between its keys - for keys $k_1, \ldots, k_n$, we have ranges $(-\infty, k_1), (k_1, k_2), \ldots, (k_{n - 1}, k_n), (k_n, \infty)$. Leaf nodes have between $\floor{\frac d 2}$ and $d$ records, where $d$ is a positive integer.

The constraint on the number of keys in each node ensures that the tree's leaf level is always at least half full, to minimise the height of the tree.

# 2/10/15

Suppose we want to insert a record into a B+ tree. First, we search for the leaf node we want to insert into, and then insert it.

If a leaf node overflows (has more than $d$ keys), we split the records into two new, roughly equally sized data nodes (the left size is 0 or 1 more than the right size), then add a key to the parent index node for the largest value of the left side. This can cause the parent node to overflow as well.

If an index node overflows (has more than $m - 1$ keys), we split the keys into two new, roughly equally sized index nodes (the left side is 0 or 1 more than the right size), where the middle key of the original index node (or the key right after the middle of the key list) is moved into the parent index node. This can cause the parent node to overflow as well.

Suppose we want to delete a record from a B+ tree. First, we search for the leaf node we want to delete from, ad then delete it.

If a leaf node underflows (has less than $\floor{\frac d 2}$ keys), we check its neighbor - if the neighbor has $\floor{\frac d 2}$ keys, we merge the leaf node with its neighbor. Otherwise, we redistribute records from the neighbor into this leaf node so they have roughly the same amount (at this point, neither is underflowing). This can cause the parent node to underflow as well if we end up merging.

If an index node underflows (has less than $\floor{m 2}$ keys), we also check its neighbor - if the neighbor has $\floor{\frac m 2}$ keys, we merge the index node with its neighbor. Otherwise, we redistribute keys from the neighbor into this index node, like with leaf nodes. This can cause the parent node to underflow as well if we end up merging.