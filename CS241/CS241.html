<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS241 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>
function highlight() { // highlight all code blocks using HighlightJS
  var code_blocks = document.getElementsByTagName("code");
  for (var i = 0; i < code_blocks.length; i++)
    hljs.highlightBlock(code_blocks[i]);
}
</script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/Résumé.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.mesecons.net/">mesecons</a></li>
    <span class="divider"></span>
    <li><a href="http://www.autohotkey.net/~Uberi/">autohotkey.net</a></li>
  </ul>
<h1 id="cs241">CS241</h1>
<p>Foundations of Sequential programs.</p>
<pre><code>Instructor: Bradley Lushman
ISA: Sean Harrap
Email: CS241@uwaterloo.ca
Web: http://www.student.cs.uwaterloo.ca/~cs241</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l&#39;H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
\]</span></p>
<h1 id="section">6/1/15</h1>
<p>Weekly assignments, 11 total.</p>
<p>A sequential program is an &quot;ordinary&quot; program - one that is not concurrent, parallel, or so on. Sequential programs only do one thing at a time - they are single threaded.</p>
<p>As this course focuses on the foundation, we will be starting from the hardware, and figure out how sequential program works from there.</p>
<p>This course uses a simulated MIPS machine for the assignment. At the end we will be able to run a relatively complex C-like language that can run on the MIPS machine.</p>
<h2 id="binary-and-hexadecimal-numbers">Binary and Hexadecimal Numbers</h2>
<p>Refer to earlier notes for background.</p>
<p>A bit is a binary digit - a 0 or a 1. A nibble is a collection of 4 bits. A byte is a collection of 8 bits, although historically it could be any number needed to hold a signle character - sometimes 7. A word is the a machine-specific grouping of bits - the number of bits in an address on the computer.</p>
<p>In this course we will be using a 32-bit architecture.</p>
<p>Data in a computer's memory could mean anything - it is simply data.</p>
<p>There are a number of conventions for representing negative numbers in binary. The <strong>sign magnitude</strong> convention has the leftmost bit of a binary number represent whether it is negative. Although this system is simple, it's difficult to do math with since we have to treat the first bit separately for a lot of operations. Plus, there's two representations for the number 0 - <code>00...00</code> and <code>10...00</code>.</p>
<p>The <strong>one's complement</strong> convention has positive numbers written normally, and negative numbers are their magnitude with their bits flipped. This works better for certain operations, but there is still two 0 representations, which complicates things unnecessarily.</p>
<p>The <strong>two's complement</strong> convention has positive numbers written normally, and the negative numbers are simply their magnitude with their bits flipped and 1 added. The advantage of this system is that the first bit still denotes the sign. Plus, there is only one representation for 0, and operations like addition work as usual, modulo <span class="math">\(2^n\)</span>.</p>
<p>To find the numerical value of a number represented using <span class="math">\(n\)</span>-bit two's complement, we first check the sign bit. If 0, then we simply convert binary to a non-negative number as usual. Otherwise, we convert the binary to a non-negative number as usual, and subtract <span class="math">\(2^n\)</span> from that result to get a negative number as a result.</p>
<p>To quickly negate a binary number in two's complement quickly, flip all the bits and add 1, convert that to a positive number, then negate that number. This allows us to quickly convert negative numbers to binary and hex.</p>
<p>For 3-bit numbers, each binary string, in ascneding order, represents 0, 1, 2, 3, -4, -3, -2, -1.</p>
<p>A byte could also potentially be a character. Characters have multiple possibble encodings, such as unicode and ASCII. ASCII is a 7-bit code (for historical reasons, in order to avoid sending expensive bits) that has representations for most America-centric characters, although there are 8-bit extensions like those by IBM that add multilingual characters. In an 8-bit byte world, the eighth bit is 0 in standard ASCII. We will be using ASCII in this course to represent characters.</p>
<p>A byte could also be part of a data structure, or even part of an instruction. In this course, we have 32-bit instructions. A byte could also just be garbage - memory that isn't currently being used for anything and has no meaning.</p>
<p>The idea is that the meaning of a byte is whatever meaning we assign to it, and we have to remember what each little chunk of memory we're using actually means.</p>
<h1 id="section-1">8/1/15</h1>
<h2 id="the-machine">The Machine</h2>
<p>Computer programs operate on data, and are theselves data. Historically, the program code was stored in special memory such as punch cards or switches, and it operated on separate data memory. This was known as the Harvard architecture.</p>
<p>John Von Neumann proposed that we could just put the program code in the same memory as the rest of the data. This is known as the Von Neumann architecture. The result of program code being treated as just a program was that we can now write programs that operate on programs, even themselves. For example, operating systems, compilers, and viruses that modify existing code to do what it wants.</p>
<p>Machine language is specific to a particular processor type, like MIPS, ARM, and x86. In this course we will be using a simplified version of MIPS.</p>
<h3 id="architecture">Architecture</h3>
<p>The MIPS machine consists of a CPU and main memory (RAM), connected via a bus. The CPU contains the ALU, the registers, and the control unit.</p>
<p>The ALU (arithmetic logic unit) is responsible for doing arithmetic and logic, for doing things like addition and subtraction.</p>
<p>Registers are simply small pieces of memory that are really fast to work with since they are so close to the CPU - they are the fastest pieces of memory we have, and we have relatively little of it. Most instructions can actually only work directly on registers, so we usually need to bring data from the main memory into registers before we work on them.</p>
<p>There are the 32-bit <strong>general purpose registers</strong> R0 to R31, which can be used for general purpose data storage. In our assembly code, we refer to them with $0, $1, $2, and so on, until $31. However, R0 is always 0, and R30 (stack pointer/frame pointer) and R31 (return pointer) have other purposes in control flow by convention.</p>
<p>The <strong>program counter register</strong> is responsible for keeping track of where we are in the main memory for executing code. It holds an address to the next instruction to run while the current instruction is running. Some commands manipulate this register, so we can actually do control flow by modifying this in a certain way.</p>
<p>By convention, we guarantee that a certain address in memory like 0 always starts off with valid code, and then we initialize PC to that location. So when we start the machine, it will start executing instructions from that location and go onwards.</p>
<p>The <strong>instruction register</strong> IR is closely related to PC. Instead of holding the address of the next instruction, it holds the current instruction itself (not its address).</p>
<p>The HI/LO registers are a pair of registers that are useful for storing the output of operations like division, which results in the quotient and the remainder, stored in this pair of registers. They cannot be accessed directly, but are implicitly used by certain instructions.</p>
<p>The <strong>memory address register/memory data register</strong> MAR and MDR are used for working with memory. MAR is used to store the desired address for us to load, and MDR is used to store the loaded value.</p>
<p>MAR/MDR, PC, and IR are all hidden registers - they are not visible to our programs.</p>
<p>The control unit decodes instructions, and dispatches them to the rest of the computer to actually execute them.</p>
<p>There are many types of memory, such as registers, CPU cache, main memory, disk memory, and network memory, in order of increasing distance from the CPU. The closer the memory to the CPU, the faster it is to work with.</p>
<p>The main memory (RAM) is a large amount of memory stored away from the CPU, and communicates with the CPU over the bus (physically, a bunch of wires). It can be thought of as a huge array of <span class="math">\(n\)</span> bytes. Each byte in the RAM has an address, one of <span class="math">\(0, \ldots, n - 1\)</span>.</p>
<p>Since a word in MIPS is 32 bits (4 bytes), and we can only access memory on word boundaries, we can only get or set addresses that are multiples of 4.</p>
<p>RAM access is much slower than register access - as much as three orders of magnitude. Usually, data in RAM must be transferred to registers before it can be used.</p>
<h3 id="instructions">Instructions</h3>
<p>It takes 5 bits to encode the 32 general purpose registers (we can't directly use the other registers as memory). Therefore, we need 15 bits to encode two source registers and a destination register. That means we have 17 bits left to encode the rest of our instruction.</p>
<p>There are actually only two instructions for working with RAM - load word and store word.</p>
<p>If we do something invalid like dereferencing an invalid address, our machine will simply crash. Real-world machines have machine exceptions to deal with this.</p>
<p>The CPU essentially works as follows:</p>
<pre><code>PC &lt;- 0
loop:
    IR &lt;- MEM[PC]
    PC &lt;- PC + 4
    decode and execute the instruction in IR</code></pre>
<p>A program gets executed by getting called by a <strong>loader</strong>, which puts the program into memory (from somewhere like the hard drive) and sets PC to the first instruction in the program. This is the role of the operating system when opening a program.</p>
<p>When our program wants to end, we want to return control flow back to the loader. This is done by setting PC to the next instruction of the loader after the one that started our program.</p>
<p>Conventionally, R31 is set to the address of the next instruction in the loader, by the loader - the address we need to jump to in order to return to the loader. Therefore, in our program code, we simply set PC to R31 in order to jump back to the loader. This looks something like <code>jr $31</code> in MIPS assembly.</p>
<p>Example code to add value in register 3 to register 5, store the result in register 7, then return:</p>
<pre><code>RAM LOCATION  BINARY                                   HEX         MEANING
------------  ---------------------------------------  ----------  --------------
00000000      0000 0000 1010 0111 0001 1000 0010 0000  0x00A71820  add $3, $5, $7
00000004      0000 0011 1110 0000 0000 0000 0000 1000  0x03e00008  jr $31</code></pre>
<p>The resulting code is therefore the binary representation of the hex value <code>00A7182003e00008</code>.</p>
<p>To store a literal value into a register, we can use the <strong>load immediate and skip</strong> (<code>lis</code>) instruction. Essentially, it reads the next word in memory, adjacent to the <code>lis</code> instruction, and stores it in the specified register.</p>
<p>Note that ASCII is a mapping between characters and binary, and ASCII values are not binary. For example, <code>0010000100110101</code> is simply an ASCII string of 1 and 0, while <code>!5</code> would be the ASCII encoding the previous string would result in if interpreted as binary.</p>
<p>To read the ASCII representation of a given binary file, use <code>xxd &lt;filename&gt;</code> or <code>hexdump -Cv &lt;filename&gt;</code>, where <code>&lt;filename&gt;</code> is the path to the file to represent. <code>xxd -bits</code> shows binary, and <code>xxd -cols n</code> shows <code>n</code> bytes per line.</p>
<h1 id="section-2">13/1/15</h1>
<h2 id="assembly-language">Assembly Language</h2>
<p>Instead of writing in hex or binary, which is very tedious to convert, we can simply use the mnemonics like <code>add</code> and <code>lis</code>, and have a program (the <strong>assembler</strong>) translate it to binary for us. This reduces our chances of error and makes programming much simpler.</p>
<p>Essentially, one line of assembly is one word, generally one machine instruction like <code>jr</code>. There are also lines that are <strong>directives</strong>, which are instructions to the assembler to do something, rather than the machine. For example, the <code>.word X</code> directive tells the assembler that at the current point, we want the resulting binary output to have the specified literal value <code>X</code>.</p>
<p>The above code example could therefore be written as:</p>
<pre><code>add $3, $5, $7
jr $31</code></pre>
<p>Some instructions unconditionally modify PC, like <code>jr</code> - these are known as jumps. Some instructions conditionally modify PC, like <code>bne</code> - these are known as branches. Generally, the modification is given in terms of the number of words to move by. Note that it is possible to give negative values.</p>
<p>A quick way to zero a register is <code>add X, $0, $0</code>, where <code>X</code> is a register. <code>lw A, i(B)</code> loads the value in memory at address specified by register <code>B</code> with offset specified by literal number <code>i</code>, storing the result in register <code>A</code>. <code>sw A, i(B)</code> does the opposite - writing the value of <code>A</code> in memory instead of reading from it.</p>
<p>The <code>mult A, B</code> instruction has a somewhat unusual form - it only has the multiplicands, and no output parameter. Instead, since multiplication can result in a 64-bit number, the answer is is stored in the HI and LO registers, the LO bits having the lower 32 bits and the rest in HI. To get values out of HI and LO, we have <code>mflo A</code>, which gets the value of LO and stores it in the register <code>A</code>, and <code>mfhi A</code>, which does the same thing, but for HI.</p>
<p>Division is similar. After dividing, the quotient is in LO, and the remainder is in HI.</p>
<p>Counting the number of instructions to jump by is tedious and error-prone. We can mitigate this using <strong>labels</strong>. For example:</p>
<pre><code>some_label: add $1, $2, $3
add $4, $5, $6
some_other_label: .word 0x12345678</code></pre>
<p>Labels are assembler features that allow us to specify a specific location and use it later on. In any jump or branch instruction, we can simply write the label name rather than the offset, like <code>bne, $2, $0, some_label</code>, which will jump to the location specified by <code>some_label</code> if and only if <code>$2</code> is non-zero.</p>
<h1 id="section-3">15/1/15</h1>
<p>We want to be able to write functions in MIPS assembly - blocks of reusable code that we can call with parameters and have control returned to us afterward.</p>
<p>Our procedures need to guarantee that the registers will remain unchanged - after calling the function, the values of registers will be guaranteed to be as they were before calling. Otherwise, subfunctions will clobber their caller's register values.</p>
<p>The MIPS loader initializes R30 to the address of the word just past the last word in memory - this is the <strong>stack pointer</strong>, and we are using the end of the memory as a stack that grows downward. This convention allows us to use RAM as a stack for storing our calling contexts - by decreasing the stack pointer, we reveal more memory we can use, and by increasing the stack pointer, we can deallocate that memory.</p>
<p>We will allocate blocks of memory on the stack that we use as stack frames. Stack frames store register values and the return address.</p>
<p>When we call a function, we allocate a frame and store all our register values in it for safekeeping, after which we can use the registers as we want in the function. When the function returns, we restore the values of the registers, and then jump back to the return address.</p>
<p>When we want to return from a function, we need to set the value of PC to after the jump that called the function - the very next instruction after the function call. Since functions can be called from multiple places, the caller must give the return address.</p>
<p>There is a &quot;jump and link register&quot; instruction <code>jalr $s</code> that sets R31 to the next instruction (PC), and jumps to the specified address (setting PC to <code>$s</code>). This allows us to store the return address in R31, and jump to the function, all in one operation.</p>
<p>However, we also need to save R31 since <code>jalr</code> will overwrite it. In other words, we will need to store R31 on the stack.</p>
<p>To pass parameters to functions, we can just use registers (documenting what each parameter register is for is very important). If there are more parameters than available registers, it is also possible to store parameters on the stack.</p>
<p>We can also pass back the return value via an agreed-upon register. We might specify that the return value is in R2, for example.</p>
<p>A template for functions can now be defined:</p>
<pre><code>some_func:
; store the registers we will be using on the next free spaces on the stack
sw $2, -4($30)
sw $3, -8($30)

; decrement the stack pointer by the number of bytes used by our stack frame
lis $2 ; $2 is free to use now that we have saved its former value
.word 8
sub $30, $30, $2

; body of the function

; increment the stack pointer again to restore it to its previous value, assuming we didn&#39;t change $3
add $30, $30, $3

; make sure to pop the registers off in reverse order
lw $3, -8($30)
lw $2, -4($30)

jr $31 ; return to the instruction after the one we called the function from</code></pre>
<p>And we can call it as follows:</p>
<pre><code>main:
; body before the function call

; save the value of $31 on the stack to keep our return address
sw $31, -4($30)
lis $31 ; $31 is free to use now that we have saved its former value
.word 4
sub $30, $30, $31

; jump to the function with $31 as the return address
lis $31
.word some_func
jalr $31 ; store the value of PC into $31 as the next instruction, and set PC to $31 - we are storing the return address in $31

; restore the value of $31 back to its original value
lis $31
.word 4
add $30, $30, $31
lw $31, -4($30) ; we do this after we&#39;re done using $31 as a temporary variable

; body after the function call

jr $31 ; return back to the loader</code></pre>
<p>Example procedure:</p>
<pre><code>; sum_1_to_N: adds natural numbers from 1 to N
; Registers:
;     $1 - Value of N
;     $2 - Output sum
sum_1_to_N:
; save the register values on the stack
  sw $1, -4($30)
  sw $3, -8($30)
  lis $1
  .word 8
  sub $30, $30, $1
  
  ; function body
  add $2, $0, $0 ; zero out the output register
  lis $3 ; set R3 to 1
  .word 1
  sum_1_to_N_loop:
    add $2, $2, $1 ; add the current value to the output
    sub $2, $2, $3 ; decrement the value
    bne $2, $0, sum_1_to_N_loop ; break out of the loop if the value is 0
  
  ; restore the values of the registers
  lis $1
  .word 8
  add $30, $30, $1
  lw $3, -8($30)
  lw $1, -4($30)
  jr $31</code></pre>
<h2 id="inputoutput">Input/Output</h2>
<p>To do output, write a word to 0xFFFF000C. The least significant byte will be printed as an ASCII character:</p>
<pre><code>; prints a newline
lis $1
.word 0xFFFF000C
lis $2
.word 65
sw $2, 0($1)</code></pre>
<h1 id="section-4">20/1/15</h1>
<h2 id="the-assembler">The Assembler</h2>
<p>The assembler is a translater, from assembly code to machine code. Any translation process will involve two phases, analysis (understanding the input) and synthesis (producing the output from the understanding of the input).</p>
<p>Our approach is to first group the characters into meaningful tokens, like a label, a hex number, a register, a directive, or so on. Tokenization is done by something known as a <strong>lexer</strong>, which accepts source code and outputs a stream of tokens.</p>
<p>We already have a tokenizer available to us in <code>asm.ss</code> and <code>asm.cc</code>. Our goal is to group tokens into actual instructions if possible - the analysis phase. If there is an error, we must output the string &quot;ERROR&quot; to stderr. The assembled MIPS code is printed to stdout.</p>
<p>To parse labels, occasionally we will have the label definition occur after their usages. To properly assemble this, we need to do things like make an extra pass beforehand to create a symbol table, or put in a placeholder value and fix it after parsing all the labels.</p>
<p>Also note that it is also possible to label the address just past the end of the program, and a line may have more than one label associated with it.</p>
<p>Instructions in MIPS start with a 6 bit opcode, followed by their parameters that specify things like registers and offsets.</p>
<h1 id="section-5">22/1/15</h1>
<p>An OS can be as simple as the following:</p>
<pre><code>repeat:
    p &lt;= the next program to run
    $2 &lt;= loader(p)
    jalr $2
beq $0, $0, repeat</code></pre>
<p>Note that the OS itself is just a program, which means it needs to be in memory as well. Since execution starts at 0x00000000, we need to put the OS at the very beginning of the memory. That means our program cannot occupy the first location in memory.</p>
<p>That also means that the addresses of the labels will not be correct if we load them directly, since the assembler created the machine code assuming addresses start from the beginning of the program (a label at the very beginning of the assembly should result in a value of 0x00000000). If we load our program at offset <span class="math">\(d\)</span>, then we must add <span class="math">\(d\)</span> to the value of each label or reference to a label, like <code>.word some_label</code>. This process is called <strong>relocation</strong> and allows us to load the code anywhere in memory and have it run properly.</p>
<p>Since the machine code is just a stream of bytes, we don't know which words are labels and which are not - the assembler has to give us additional information about what type of thing each word is.</p>
<p>As a result, what most assemblers output is not actually just machine code, but actually <strong>object code</strong> - machine code with additional metadata that lets us load and run the program in a clean way. These assemblers are called <strong>relocatable assemblers</strong>.</p>
<p>Basically, the metadata needed in object files includes the addresses of words that represent label addresses in the code so we can relocate it, plus things like exports and imports.</p>
<p>The object file format we will be using is called <strong>MIPS executable relocatable linkable</strong> (MERL), and often has a filename of the form <code>*.merl</code>. It looks like this:</p>
<ol style="list-style-type: decimal">
<li>The literal word 0x10000002, which is the MIPS instruction for <code>beq $0, $0, 2</code>, and skips two words ahead, past the rest of the header.
<ol style="list-style-type: decimal">
<li>this means that the object file can also be run as a normal MIPS program, since this value causes it to jump past the header and straight to the code if loaded at address 0x00000000.</li>
<li>It also acts as a sanity check to make sure the file is really a MERL file.</li>
</ol></li>
<li>The length of the entire module in bytes as a word - this is the length of the header (12), the machine code, and the relocation table.</li>
<li>The address of the end of the code section in bytes as a word, which is the length of the header (12) and the machine code.</li>
<li>The actual MIPS machine code, which includes the 12 byte header offset - a label at the beginning of the machine code is at address 12.</li>
<li>The <strong>relocation table</strong>, which appears after the code returns and has a sequence of entries for different types of relocatables. An entry in the relocation table consists of two words:
<ol style="list-style-type: decimal">
<li>The format code - a word representing the type of entry this is. For example, a value of 1 represents that this entry is a relocation entry.</li>
<li>The actual address, which, if this entry is a relocation entry, is the address of a word holding the address of a label. This is relative to the beginning of the header.</li>
</ol></li>
</ol>
<h1 id="section-6">27/1/15</h1>
<p>The MERL format could also be written as follows:</p>
<pre><code>; MERL header
beq $0, $0, 2
.word end_of_module
.word end_of_code

;code goes here
add $1, $2, $0

; MERL footer - relocation table
end_of_code:
.word 0x1 ; relocatable address
.word SOME_ADDRESS
; ...
.word 0x1
.word SOME_OTHER_ADDRESS
end_of_module:</code></pre>
<p>The <code>cs241.merl</code> tool accepts a MERL file and a relocation address and outputs MIPS machine code relocated at the desired address, ready to be loaded starting at that offset. The <code>mips.twoints</code> and <code>mips.array</code> tools also support an optional second argument that specifies the address to load the code at.</p>
<p>Relocation is typically done by the loader, automatically. The loader usually has the following structure:</p>
<pre><code>verify(read_word()) # check cookie
end_of_module = read_word()
code_length = read_word() - 12

# load code
p = allocate_RAM(code_length + stack_space)
for i in range(0, code_length, 4):
    RAM[p + i] = read_word()

relocation_table = code_length + 12 # start of relocation table
for i in range(relocation_table, end_of_module, 8):
    format = read_word()
    if format == 1: # we only support format 0x1 for now
        offset = read_word() - 12
        RAM[p + offset] += p - 12 # we subtract 12 since in MERL addresses are relative to the start of the header, and the header wasn&#39;t loaded</code></pre>
<p>When we have multiple files, we might have a function that calls a function in another file. However, the assembler can't assemble this directly since the label of the function isn't defined in the file it's being called from.</p>
<p>We could just append all the code together and then assemble them, which works but is not very good, since that requires the assembly code to be available (in practice, people just want to distribute MERL), and we would need to assemble it possibly multiple times in larger projects.</p>
<p>A better solution is a program that accepts multiple MERL files and properly puts them together into one - the <strong>linker</strong>. We want to put MERL files together without ever forcing it to know about ASM.</p>
<p>First, what we could do is have the assembler ignore unknown labels, leaving them to be resolved to actual values later. This is done by simply using a placeholder value like 0, and then storing a list of these addresses in the relocation table along with the labels we want to assign to them.</p>
<p>To make this easier for the assembler, we add the <code>.import IDENTIFIER</code> directive, which does not result in any output, but tells the assembler that <code>IDENTIFIER</code> is an identifier that is external to the current file and will be linked in later. This allows us to catch typos in our label names more easily - we can give an error if a label isn't defined and isn't imported.</p>
<p>In the MERL files, we define a new format code for the relocation table - 0x11, the <strong>external symbol reference</strong> (ESR) format. The table entry then contains the address of the label reference, and the name of the label:</p>
<pre><code>; ESR relocation table entry
.word 0x11
.word ADDRESS # location where the label was used
.word LENGTH # number of words/chars that make up the label name
.word C[0] # first character of the label as ASCII value
; ...
.word C[LENGTH - 1] # last character of the label as ASCII value</code></pre>
<h1 id="section-7">29/1/15</h1>
<p>However, we also need a way for a module to say what labels it has, so other modules can actually import it. We don't want to just export all the labels in a MERL module - labels are often duplicated, expecially when we start having compilers generating them automatically.</p>
<p>Instead, we have a <code>.export LABEL</code> directive, which makes a new entry in the relocation table, in the <strong>external symbol definition</strong> (ESD) format. This contains the address where the label is defined, and the name of the label:</p>
<pre><code>; ESD relocation table entry
.word 0x5
.word ADDRESS # location where the label was defined
.word LENGTH # number of words/chars that make up the label name
.word C[0] # first character of the label as ASCII value
; ...
.word C[LENGTH - 1] # last character of the label as ASCII value</code></pre>
<p>In programming languages like C, each function gets its own <code>.export</code> entry, which is how the linker can link C object files together. A <code>static</code> function doesn't get an ESD entry, which is why it can't be imported from other files.</p>
<p>Now we can write a linker:</p>
<pre><code># Given MERl files m1 and m2 as binary blobs, and we want to make a single MERL file with m2 linked in after m1
# `get_imports` and `get_exports` get the ESRs and ESDs, respectively, as a dictionary mapping label names to addresses

a &lt;- len(m1) - 12
relocate m2 higher by `a`
add `a` to every address in m2&#39;s symbol table
assert set_conjunction(get_exports(m1).keys(), get_exports(m2).keys()) == empty_set

# link imports and exports together
for address1, label in get_imports(m1):
    if label in get_exports(m2):
        m1[address1] = get_exports(m2)[label]
        delete the ESR entry `label` from m1&#39;s relocation table
        add relocation entry `address1` to m1&#39;s relocation table
for address2, label in get_imports(m2):
    if label in get_exports(m1):
        m1[address1] = get_exports(m1)[label]
        delete the ESR entry `label` from m2&#39;s relocation table
        add relocation entry `address1` to m2&#39;s relocation table
    # we don&#39;t have an else to check for any references still unresolved because this linker only links two MERL files at a time, so it&#39;s possible that we run this multiple times for three or more files

# make the final relocation table
relocations = set_disjunction(get_relocations(m1), get_relocations(m2))
imports = set_disjunction(get_imports(m1), get_imports(m2))
exports = set_disjunction(get_exports(m1), get_exports(m2))

# output MERL file
output MERL cookie
output total code length plus relocation table size plus 12 (header size)
output total code length plus 12 (header size)
output m1
output m2
output relocations, exports, and imports</code></pre>
<h2 id="high-level-languages">High Level Languages</h2>
<p>The compiler is responsible for converting a high level language to assembly. Unlike an assembler, the compiled result is not unique - there are multiple possible ways to convert high level languages to assembly.</p>
<p>We want a formal theory of string recognition - general principles that work in the context of any programming language.</p>
<p>An <strong>alphabet</strong> is a finite set of symbols, generally denoted <span class="math">\(\Sigma\)</span> (<span class="math">\(\Sigma = \set{a, b, c}\)</span>, for example). Symbols can be anything we want, and might not even be strings.</p>
<p>A <strong>string/word</strong> is a finite sequence of symbols, each symbol belonging to <span class="math">\(\Sigma\)</span> (<span class="math">\(abaabbcca\)</span>, for example).</p>
<p>The length of a string <span class="math">\(s\)</span> is <span class="math">\(\abs{s}\)</span>. The empty string is usually denoted <span class="math">\(\epsilon\)</span>. This should not be confused with any symbols in the alphabet - <span class="math">\(\abs{\epsilon} = 0\)</span> and <span class="math">\(\epsilon\)</span> is not in any alphabets.</p>
<p>Given a string <span class="math">\(s\)</span>, <span class="math">\(s^n\)</span> is equivalent to <span class="math">\(s\)</span> repeated <span class="math">\(n\)</span> times - <span class="math">\(s^n = s \ldots s\)</span>.</p>
<p>A <strong>language</strong> is a set of strings (<span class="math">\(\set{a^{2n}b \mid n \ge 0}\)</span>, for example, which is an even number of <span class="math">\(a\)</span> followed by a <span class="math">\(b\)</span>). The empty language is just an empty set <span class="math">\(\emptyset\)</span> - a language with no words. This is different from <span class="math">\(\set{\epsilon}\)</span>, which is a language with one word, the empty word.</p>
<p>We want to determine if a given string belongs to a given language. How difficult this is depends on the language. For infinite sets of strings, like the set of all valid MIPS or C programs, things get a little trickier. In fact, there exist languages where this is impossible.</p>
<p>Therefore, we want to characterize languages by how hard they are to recognize. Noam Chomsky came up the Chomsky heirarchy to deal with this. The following is based on that, and lists language classes in ascending order of difficulty recognizing:</p>
<ul>
<li>The <strong>finite languages</strong> are the easiest to recognize - there are a finite number of strings, so we can just check each one.</li>
<li>The <strong>regular languages</strong>.</li>
<li>The <strong>context-free languages</strong>.</li>
<li>The <strong>context-sensitive languages</strong>.</li>
<li>The <strong>recursive languages</strong>.</li>
<li>There are other types that are more difficult to recognize, and some that are even impossible. For example, a language that contains one word, the solution for an unsolvable problem in mathematics.</li>
</ul>
<p>Our goal is to use as easy a level as possible on the heirarchy, since then the compiler will be simpler. Higher levels are supersets of lower levels.</p>
<h1 id="section-8">3/2/15</h1>
<h3 id="finite-languages">Finite Languages</h3>
<p>We can see if a word is in a finite language by comparing it is in each element of the finite set. However, we can do this more efficiently by going char by char and filtering out the ones that are not matches.</p>
<p>For example, if we have a language with words &quot;cat&quot;, &quot;car&quot;, &quot;cow&quot;, we can follow the following procedure:</p>
<ol style="list-style-type: decimal">
<li>Scan the input left to right.</li>
<li>If the first character is &quot;c&quot;, then move on, and otherwise produce false.</li>
<li>If the next character is &quot;a&quot;, and the next character is &quot;t&quot; or &quot;r&quot;, and there is no next character, then produce true, and otherwise produce false if &quot;t&quot;, &quot;r&quot;, or end failed to match.</li>
<li>If the next character is &quot;o&quot;, and the next character is &quot;w&quot;, and there is no next character, then produce true, and otherwise produce false if &quot;w&quot; or end didn't match.</li>
<li>Otherwise, produce false.</li>
</ol>
<p>This only scans the word once - we read only in one pass. Although the above procedure uses no memory, there is still state - the value of PC is different whenever we read a character.</p>
<p>We can abstract this sort of decision-making program into something like a graph. Every state in the program is represented as a circle, and transitions between states can be represented by drawing unidirectional arrows between them, labelled with what the transition is. In the above example, states might be values of the PC register, and state transitions might be labelled by the character that is read next.</p>
<p>A circle within a circle (or a circle with a double border) accepts - it represents a state that, if the input ends there, we can simply accept it as valid, and we can use it as an end state. A circle with an arrow pointing into it from the left represents the start state.</p>
<p>This sort of program is known as a <strong>state machine</strong>. The circles we draw are <strong>states</strong> - configurations of the program based on the inputs seen. The diagrams we draw are called <strong>deterministic finite automata</strong> (DFAs).</p>
<p>All state machines that check the validity of an arbitrary string in a finite language can be written as a tree. In other words, for any finite language there exists a DFA without an cycles.</p>
<p>We match DFAs as follows:</p>
<ol style="list-style-type: decimal">
<li>Start at the start state.</li>
<li>For each char in the input, follow the correponding arrow to the next state, and produce false if there are no possible transitions from any of the states.</li>
<li>We are now at the end of the input. Produce true if the current state is an ending state, and false otherwise.</li>
</ol>
<p>To make the math more elegant, we define an implicit <strong>error state</strong>, and every single state goes to this state for unlabeled transitions. So if we get an unexpected input for a node (so we don't have a defined transition for the input), we implicitly define a transition to the error state. The error state is not special, but it is always a state that cannot transition to any other states, and it never matches.</p>
<p>A DFA represents a finite language if and only if it does not contain any cycles.</p>
<h3 id="regular-languages">Regular Languages</h3>
<p>The regular languages are built from finite languages, plus set operations like union (set union of the words of two languages - <span class="math">\(L_1 \cup L_2 = \set{x \mid x \in L_1 \vee x \in L_2}\)</span>), concatenation (every possible combination of elements from one set concatenated to another - <span class="math">\(L_1 L_2 = \set{xy \mid x \in L_1 \wedge y \in L_2}\)</span>), and repetition (a language repeated an arbitrary number of times - <span class="math">\(L^* = \set{\epsilon} \cup \set{xy \mid x \in L^* \wedge y \in L} = \set{\epsilon} \cup L \cup LL \cup LLL \cup \ldots\)</span>).</p>
<p>Note that repetition generally results in infinite sets - this means that the regular languages contain infinite languages.</p>
<p>We show that a language is regular by building it up from finite languages and the set uperations (union, concatenation, and repetition). For example, <span class="math">\(\set{a^{2n}b \mid n \ge 0}\)</span> is regular because <span class="math">\(\set{a^{2n}b \mid n \ge 0} = \set{aa}*\set{b}\)</span>.</p>
<p>A <strong>regular expression</strong> is a shorthand for all the cumbersome set notation we are using for describing regular languages, and is itself a language we will denote <span class="math">\(E\)</span>. It works as follows:</p>
<ul>
<li><span class="math">\(E = \emptyset\)</span> represents <span class="math">\(L = \emptyset\)</span>.</li>
<li><span class="math">\(E = \epsilon\)</span> represents <span class="math">\(L = \set{\epsilon}\)</span>.</li>
<li><span class="math">\(E = aaa\)</span> and other sequences of non-special characters represent <span class="math">\(L = \set{aaa}\)</span> and themselves in a set.</li>
<li><span class="math">\(E_1|E_2\)</span> represents <span class="math">\(L_1 \cup L_2\)</span>.</li>
<li><span class="math">\(E_1 E_2\)</span> represents <span class="math">\(L_1 L_2\)</span>.</li>
<li><span class="math">\(E_1 = E_2^*\)</span> represents <span class="math">\(L_1 = L_2^*\)</span>.</li>
</ul>
<p>For example, <span class="math">\(\set{a^{2n}b \mid n \ge 0}\)</span> can be represented with <span class="math">\((aa)*b\)</span>. Regular languages are, in practice, relatively simple. For example, C is not a regular language, but C programs are made up of tokens, each of which is part of a regular language - tokens, string literals, number literals, types, and so on.</p>
<p>For regular languages, DFAs can have cycles - this occurs as a result of repetition.</p>
<p>Consider the language defined as the set containing all strings that are composed of an even number of &quot;a&quot; and an odd number of &quot;b&quot;. Clearly, we can write a DFA for this using four states (even &quot;a&quot; and even &quot;b&quot; - start, odd &quot;a&quot; and even &quot;b&quot;, odd &quot;a&quot; and odd &quot;b&quot;, and even &quot;a&quot; and odd &quot;b&quot; - end/accept). The regular expression for this is rather difficult to write. ;wip: how?</p>
<h1 id="section-9">5/2/15</h1>
<p>Formally, a DFA is a 5-tuple <span class="math">\((Q, \Sigma, q_0, A, \delta)\)</span>, where the variables represent the following:</p>
<ul>
<li><span class="math">\(Q\)</span> is the set of states of the DFA - all possible states. This is a finite, non-empty set.</li>
<li><span class="math">\(\Sigma\)</span> is the alphabet of the DFA - all possible input values. This is a finite, non-empty set.</li>
<li><span class="math">\(q_0\)</span> is the start state - the state at which the DFA is initialized. This is an element of <span class="math">\(Q\)</span>.</li>
<li><span class="math">\(A\)</span> is the set of accepting states of the DFA - the states that the input can terminate at. This is a subset of <span class="math">\(Q\)</span></li>
<li><span class="math">\(\delta\)</span> is the transition function - a mapping from a state and input character to a new state. This is a function <span class="math">\(q, x \to r, q \in Q, x \in x \in \Sigma, r \in Q\)</span>.</li>
</ul>
<p>DFAs don't always have to just accept characters. It is also possible to consume entire words, using an extension of the original <span class="math">\(\delta\)</span> function, which only consumes one character at a time.</p>
<p>This <strong>word consuming function</strong> can be defined as <span class="math">\(\delta^*(q, \epsilon) = q, \delta^*(q, cw) = \delta^*(\delta(q, c), w), q \in Q, c \in \Sigma, w \in \Sigma^*\)</span>. A DFA accepts a word <span class="math">\(w\)</span> if <span class="math">\(\delta^*(q_0, w) \in A\)</span> - the DFA accepts the input if read from its start state. <span class="math">\(\delta^*\)</span> represents a function that gives the end state of the DFA given a start state and input.</p>
<p>Given a DFA <span class="math">\(M\)</span>, <span class="math">\(L(M)\)</span> represents the set of all strings accepted by <span class="math">\(M\)</span>. This is the set of all elements in of <span class="math">\(\Sigma^*\)</span> such that <span class="math">\(M\)</span> accepts them.</p>
<p><strong>Kleene's theorem</strong> states that <span class="math">\(L\)</span> is regular if and only if <span class="math">\(L = L(M)\)</span> for some DFA <span class="math">\(M\)</span>. In other words, the regular languages are exactly the languages for which there exists a DFA that accepts them.</p>
<p>A DFA is still just a representation of a computer program. In practive, these are often implemented using switch statements:</p>
<pre><code>int state = $q_0$
char current;
while (read_char(current)) {
    switch (state) {
        case $q_0$:
            switch (current) {
                case &#39;a&#39;: state = // the state to transition to
                // cases should exist for all possible transitions from this state
                case &#39;z&#39;: state = // the state to transition to
                default: error(&quot;No transitions available&quot;);
            }
        case $q_1$:
            // another switch statement like the one for $q_0$, for $q_1$&#39;s transitions
    }
}</code></pre>
<p>This works but is also long and error-prone. Instead, we can use a state table, where rows are chars and columns are states, and each cell points to the next state. See the assignment 3 starter code for an example.</p>
<p>It is also possible to attach actions to the transitions in the DFA. Potentially, we could use this to do things like get the numerical value of a number literal while we are parsing it.</p>
<p>Every regular expression can be converted into a DFA, and every DFA can be converted into a regular expression. However, the translation is not always straightforward. For example, consider <code>(a|b)*abb</code>, the set of all words in <span class="math">\(\set{a, b}^*\)</span> that end with &quot;abb&quot;. When we generate the DFA, we need to add extra transitions for the states that make up &quot;abb&quot; in order to backtrack, in order to handle inputs like &quot;aba&quot;:</p>
<pre><code>1: START
    &quot;a&quot;: 2
    &quot;b&quot;: 1
2:
    &quot;a&quot;: 2
    &quot;b&quot;: 3
3:
    &quot;a&quot;: 2
    &quot;b&quot;: 4
4: ACCEPT
    &quot;a&quot;: 2
    &quot;b&quot;: 1</code></pre>
<h3 id="nfas">NFAs</h3>
<p>What if we had a DFA that allowed more than one transition from the same state that have the same label? For example, a state with transitions to two other states, each with the same label.</p>
<p>This would mean that when we have an input with multiple possible transitions, the machine would get to choose one of the transitions - the state machine is non-deterministic. This is a <strong>non-deterministic finite automata</strong> (NFA). The difference from a DFA is that the machine gets choices in which transition to use.</p>
<p>We also assume that the machine always makes the correct choice - the state machine accepts if there exists a sequence of choices such that the input is accepted. In practice, we do this with something like backtracking - when we get to the end and we haven't accepted the input, we undo and choose a different set of choices. By using a stack storing when we made choices, we can easily use backtracking search over all possible choices.</p>
<p>With NFAs, the above example becomes a lot simpler, which is the main benefit of this type of machine:</p>
<pre><code>1: START
    &quot;a&quot;: 1, 2
    &quot;b&quot;: 1
2:
    &quot;b&quot;: 3
3:
    &quot;b&quot;: 4
4: ACCEPT</code></pre>
<p>Formally, an NFA is a 5-tuple like a DFA - <span class="math">\((Q, \Sigma, q_0, A, \delta)\)</span>, where the variables represent the same things as with a DFA, except the transition function. With an NFA, <span class="math">\(\delta\)</span> is a mapping from a state and input character to a set of new states. This is a function <span class="math">\(q, x \to R, q \in Q, x \in x \in \Sigma, R \subseteq Q\)</span>.</p>
<p>An NFA accepts the input if a path exists such that the NFA results in acceptance, and rejects if none do.</p>
<p>We now want to define a transition function for NFAs that accepts words. What we will do is define a function that accepts a set of possible current states and a word, resulting in a new set of possible states. When we are moving through the NFA, we are basically stepping through all the possible transitions at once when we have a choice. In other words, we step through all the possible choice paths in parallel.</p>
<p>We define this as <span class="math">\(\delta^*(S, \epsilon) = S, \delta^*(S, cw) = \delta^*(\set{\delta(q, c) \mid q \in S}, w), S \subseteq Q, c \in \Sigma, w \in \Sigma^*\)</span>. This is a function that results in the set of all possible end states of the machine. The NFA accepts a word <span class="math">\(w\)</span> if and only if <span class="math">\(\delta^*(\set{q_0}, w) \cap A \ne \emptyset\)</span> - if any of the possible end states are accepting.</p>
<p>To simulate a DFA:</p>
<pre><code>state = $q_0$
while read_char(current):
    state = $\delta$(state, current)
return state in $A$</code></pre>
<p>To simulate an NFA:</p>
<pre><code>states = set($q_0$)
while read_char(current):
    states = set($\delta$(q, current) for q in states)
return states &amp; $A$ != set()</code></pre>
<p>If we have an NFA and give each possible set of possible states (which we get from stepping through every possible option) a name, and use these sets as the states instead, we get a DFA. In this way it is possible to convert any NFA into a DFA. This is called <strong>subset construction</strong>.</p>
<p>Note that this means that an NFA with <span class="math">\(n\)</span> states can be converted into a DFA with at most <span class="math">\(2^n\)</span> states - one for every subset of states. We can convert an NFA to a DFA by stepping through all the possibilities for input. The accepting states of our DFA are the <strong>sets containing accepting states in the original NFA</strong>.</p>
<p>Whenever we step an NFA, we are actually simulating a DFA in a simpler way - we are constructing a DFA on the fly, with only the states that we actually need for a certain input.</p>
<h1 id="section-10">10/2/15</h1>
<p>Every NFA must have a corresponding DFA, and every DFA is also an NFA - the set of NFAs is a superset of the set of DFAs, and every NFA has a corresponding DFA. Therefore, NFAs accept the same class of languages that DFAs do, the regular languages.</p>
<p>Consider the language <span class="math">\(L = \set{cab} \cup \set{w \in \set{a, b, c}^* \mid w \text{ has an even number of } a}\)</span>.</p>
<pre><code>1, 2, 3, 4, 5, 6
1 &gt; c &gt; 2
2 &gt; a &gt; 3
3 &gt; b &gt; 4
1 &gt; a &gt; 5
1 &gt; b,c &gt; 6
5 &gt; b,c &gt; 5
5 &gt; a &gt; 6
6 &gt; a &gt; 5
6 &gt; b,c &gt; 6
1 &gt; accept
6 &gt; accept</code></pre>
<p>To convert this to a DFA, we start by stepping the NFA with <span class="math">\(\set{1}\)</span> (a set containing only the start state), while considering every possible next input for the currently stepped states to get the next candidate states. Repeating until no new subset states are found, we get the equivalent DFA:</p>
<pre><code>{1} &gt; a &gt; {5}
{1} &gt; b &gt; {6}
{1} &gt; c &gt; {2, 6}
{5} &gt; b,c &gt; {5}
{5} &gt; a &gt; {6}
{6} &gt; a &gt; {5}
{6} &gt; b,c &gt; {6}
{2, 6} &gt; b,c &gt; {6}
{2, 6} &gt; a &gt; {3, 5}
{3, 5} &gt; a &gt; {6}
{3, 5} &gt; b &gt; {4, 5}
{3, 5} &gt; c &gt; {5}
{4, 5} &gt; a &gt; {6}
{4, 5} &gt; b, c &gt; {5}
{1} &gt; accept
{6} &gt; accept
{2, 6} &gt; accept</code></pre>
<p>We will now consider an <span class="math">\(\epsilon\)</span>-NFA. This is like a normal NFA, but we can change states without reading any input at all. This is represented with arrows labelled with <span class="math">\(\epsilon\)</span>. This makes many alternation cases easier, because it gives a much easier way of gluing multiple small NFAs together into a bigger NFA. Basically, the <span class="math">\(\epsilon\)</span> transition is an optional transition we can take from one state to another.</p>
<p>Using the same argument as before with states being sets of states, the <span class="math">\(\epsilon\)</span>-NFAs also accept regular languages.</p>
<p>Also, <span class="math">\(\epsilon\)</span>-NFAs can be easily converted into a regular expression using a simple algorithm. Additionally, if we can prove that all regular expressions have an <span class="math">\(\epsilon\)</span>-NFA, then we have one direction in the proof for Kleene's theorem - that every regular expression has a DFA.</p>
<p>The <span class="math">\(\epsilon\)</span>-NFA for any regular expression can be found recursively:</p>
<ul>
<li>Empty language is a non-accepting start state.</li>
<li>The regular expression <span class="math">\(\epsilon\)</span> is an accepting start state.</li>
<li>A regular expression with only one letter is a start state that transitions to an accepting state via that letter.</li>
<li>A regular expression of the form <span class="math">\(E_1|E_2\)</span> is a start state, with two <span class="math">\(\epsilon\)</span> transitions, one to the <span class="math">\(\epsilon\)</span>-NFAs for <span class="math">\(E_1\)</span> and one to the <span class="math">\(\epsilon\)</span>-NFA for <span class="math">\(E_2\)</span>.</li>
<li>A regular expression of the form <span class="math">\(E_1 E_2\)</span> is the <span class="math">\(\epsilon\)</span>-NFA for <span class="math">\(E_1\)</span>, where the accepting states are not accepting and instead have an <span class="math">\(\epsilon\)</span> transition to the start state for the <span class="math">\(\epsilon\)</span>-NFA for <span class="math">\(E_2\)</span>.</li>
<li>A regular expression of the form <span class="math">\(E_1^*\)</span> is the <span class="math">\(\epsilon\)</span>-NFA for <span class="math">\(E_1\)</span>, where the start state is a new accepting state with an <span class="math">\(\epsilon\)</span>-transition to the original start state of the <span class="math">\(\epsilon\)</span>-NFA for <span class="math">\(E_1\)</span>, and all accepting states of also have an <span class="math">\(\epsilon\)</span> transition back to the start state of the <span class="math">\(\epsilon\)</span>-NFA for <span class="math">\(E_1\)</span>.</li>
</ul>
<p>As a result, all regular expressions can be converted into an <span class="math">\(\epsilon\)</span>-NFA, which can be converted into a DFA.</p>
<p>Regular expressions and DFAs are useful in writing compilers because we need to do tokenization/scanning/lexing. For example, the lexer needs to be able to scan for keywords, identifiers, literals, operators, comments, and other C tokens, and these are all regular languages. As a result, sequences of C tokens are also regular languages, and we can use finite automata to do tokenization/scanning/lexing.</p>
<p>To do tokenization, we are given a string <span class="math">\(w\)</span>, break it into words <span class="math">\(w_1, \ldots, w_n\)</span> such that each <span class="math">\(w_i\)</span> is in the language of the tokens (or give an error otherwise), and then output each <span class="math">\(w_i\)</span> as the tokens. If <span class="math">\(E_1, \ldots, E_k\)</span> are the regular expressions that match each type of token, then <span class="math">\((E_1 | \ldots | E_k)^*\)</span> is a regular expression that recognizes sequences of valid tokens. If we add an action to output the token after a match of <span class="math">\(E_1 | \ldots | E_k\)</span>, we now have a tokenizer.</p>
<p>Consider an <span class="math">\(\epsilon\)</span>-NFA for parsing a C-style identifier: <span class="math">\([a-zA-Z][a-zA-Z0-9]^*\)</span>. Clearly, if we parse <span class="math">\(abcd\)</span>, this could be interpreted as either 1, 2, 3, or 4 tokens, since the <span class="math">\(\epsilon\)</span> move outputs a token. This ambiguity is problematic because a program written in a programming language should only have one interpretation.</p>
<h1 id="section-11">12/2/15</h1>
<p>We can resolve this by adding add a rule to the NFA that says that we should only take an <span class="math">\(\epsilon\)</span> move if we have no other choice - to always look for the longest possible next token, rather than trying all the options. In effect, we are removing the nondeterminism from the <span class="math">\(\epsilon\)</span>-NFA.</p>
<p>Clearly, this doesn't always work. Consider <span class="math">\(L = \set{aa, aaa}\)</span>, and the word <span class="math">\(w = aaaa\)</span>. Clearly, <span class="math">\(w \in L\)</span>, but an <span class="math">\(\epsilon\)</span>-NFA that always took the longest possible move would not accept it. However, in practice this sort of redundancy doesn't show up, and we can implement this technique.</p>
<p>This can be formalized as the <strong>maximal munch algorithm</strong> for greedy matching using DFAs:</p>
<ol style="list-style-type: decimal">
<li>Run the <span class="math">\(\epsilon\)</span>-NFA without any <span class="math">\(\epsilon\)</span>-moves, until no more non-error moves are possible. This is the greedy part of the algorithm, where we try to consume as much as possible.</li>
<li>If currently in an accepting state, we have found a token.</li>
<li>Otherwise, back up to the most recent accepting state (or error if none), and the input to that point is a token. Scanning resumes from there.</li>
<li>Output the token.</li>
<li>Resume scanning, for the next token, by using an <span class="math">\(\epsilon\)</span>-move back to the starting state.</li>
</ol>
<p>We often just use a simplified maximal munch, which works on a more limited set of languages:</p>
<ol style="list-style-type: decimal">
<li>Run the <span class="math">\(\epsilon\)</span>-NFA without any <span class="math">\(\epsilon\)</span>-moves, until no more non-error moves are possible. This is the greedy part of the algorithm, where we try to consume as much as possible.</li>
<li>If currently in an accepting state, we have found a token.</li>
<li>Output the token.</li>
<li>Resume scanning, for the next token, by using an <span class="math">\(\epsilon\)</span>-move back to the starting state.</li>
</ol>
<p>Basically, the same thing, but without the backtracking step. This means we can't tokenize things like <code>#ifdefg</code> into <code>#ifdef</code> and <code>g</code> - whitespace or another separator is required. However, this is usually good enough because languages are often designed intentionally in such a way that this works.</p>
<p>This sort of parser sometimes causes issues in C++, such as <code>vector&lt;vector&lt;int&gt;&gt; v;</code> not being valid because the <code>&gt;&gt;</code> is interpreted as a single token instead of two <code>&gt;</code> tokens.</p>
<h2 id="parsing">Parsing</h2>
<p>Tokenization is good, but it can't parse everything. For example, consider a language containing all strings consisting entirely of balanced parentheses. Clearly, we need at least one state for each level of nesting in order to keep track of how many parentheses are currently open in the input, but no finite number of states can be used to recognize all levels of nesting, and DFAs must have a finite number of states. They also can't parse palindromes for the same reason.</p>
<p>That means that DFAs can't parse balanced parenthesis, like those in Scheme, and that this is not a regular language. Therefore, we need a higher class of languages - the <strong>context-free languages</strong>.</p>
<p>The context-free languages are those that can be described by a <strong>context-free grammar</strong> (a set of rewrite rules, which can be recursive). For example, a context-free language for strings of balanced parentheses can be written as <span class="math">\(S \to \epsilon; S \to (S); S \to SS\)</span>. As a shorthand, we can write this as <span class="math">\(S \to \epsilon \mid (S) \mid SS\)</span>.</p>
<p>A word is in a context-free language if and only if there exists a way to construct it from the rewrite rules in the grammar. For example, <span class="math">\((())()\)</span> can be generated by applying rule 3 to rule 2 applied to rule 2 applied to rule 1 and rule 2 applied to rule 1.</p>
<p>A context-free grammar consists of:</p>
<ul>
<li>An alphabet <span class="math">\(\Sigma\)</span>, called <strong>terminal symbols</strong>. This never includes <span class="math">\(\epsilon\)</span>.</li>
<li>A finite, non-empty set <span class="math">\(N\)</span> of <strong>nonterminal symbols</strong>, such that <span class="math">\(N \cap \Sigma = \emptyset\)</span>. The <strong>vocabulary</strong> is defined as <span class="math">\(V = N \cup \Sigma\)</span>.</li>
<li>A finite set <span class="math">\(P\)</span> of <strong>productions</strong>, which are rules of the form <span class="math">\(A \to B\)</span> where <span class="math">\(A \in N, B \in V^*\)</span>.</li>
<li>A starting element <span class="math">\(S\)</span> of <span class="math">\(N\)</span>.</li>
</ul>
<p>To make notation easier, we use these conventions:</p>
<ul>
<li><span class="math">\(a, b, c, \ldots \in \Sigma\)</span> - letters from the beginning of the alphabet represent characters.</li>
<li><span class="math">\(z, y, x, \ldots \in \Sigma^*\)</span> - letters from the end of the alphabet represent strings.</li>
<li><span class="math">\(A, B, C, \ldots, Z \in N\)</span> - capital letters represent nonterminals.</li>
<li><span class="math">\(S\)</span> is the starting symbol/nonterminal.</li>
<li><span class="math">\(\alpha, \beta, \ldots \in V^*\)</span> - Greek letters represent sequences of symbols.</li>
</ul>
<p><span class="math">\(\alpha A \beta \implies \alpha \gamma \beta\)</span> means that there is a production <span class="math">\(A \to \beta\)</span> in <span class="math">\(P\)</span> - the right side is derivable from the left side in one step.</p>
<p><span class="math">\(\alpha A \beta \implies^* \alpha \gamma \beta\)</span> means that there exists a sequence of 0 or more derivation steps that convert the left side to the right side - the right side is somehow derivable from the left.</p>
<p>A context-free language is therefore defined as <span class="math">\(L(G) = \set{w \in \Sigma^* \mid S \implies^* w}\)</span> - the set of all strings derivable from <span class="math">\(S\)</span>, the starting state. Here, <span class="math">\(G\)</span> is a context-free grammar and gives the values for <span class="math">\(\Sigma, N, V, P, S\)</span>.</p>
<p>A language <span class="math">\(L\)</span> is context-free if and only if there exists a context-free grammar <span class="math">\(G\)</span> such that <span class="math">\(L = L(G)\)</span>.</p>
<p>For example, a language that consists of palindromes over <span class="math">\(\set{a, b, c}\)</span> can be represented using the grammar <span class="math">\(S \to aSa \mid bSb \mid cSc \mid M; M \to a \mid b \mid c \mid \epsilon\)</span>.</p>
<p>A language that consists of arithmetic expressions over <span class="math">\(\set{a, b, c, +, -, *, /}\)</span> can be written as <span class="math">\(S \to a \mid b \mid c \mid S O S \mid (S); O \to + \mid - \mid * \mid /\)</span>. We can show that <span class="math">\(S \implies^* a + b\)</span> by using <span class="math">\(S \implies S O S \implies a O S \implies a + S \implies a + b\)</span>. This is a <strong>derivation</strong> - we apply rules one by one until we have constructed the expression.</p>
<p>A derivation is a <strong>proof that a word is in the language</strong>.</p>
<p>At our <span class="math">\(S O S\)</span> step, we had a choice as to which rule to expand next - any of the <span class="math">\(S\)</span>, <span class="math">\(O\)</span>, or <span class="math">\(S\)</span> could have been expanded next. We can do a <strong>leftmost derivation</strong>, where we always expand the leftmost symbol (in this case, the first <span class="math">\(S\)</span>), and <strong>rightmost derivation</strong>, where we always expand the rightmost symbol (in this case, the second <span class="math">\(S\)</span>).</p>
<p>We can express the derivation more naturally as a <strong>parse tree</strong>:</p>
<pre><code>  S
 /|\
S O S
| | |
a + b</code></pre>
<h1 id="section-12">24/2/15</h1>
<p>For every leftmost and rightmost derivation, there exists a corresponding <strong>unique parse tree</strong>. A grammar for which some word has more than one distinct leftmost derivation is <strong>ambiguous</strong> - there are multiple possible parse trees for some strings.</p>
<p>A string might have two leftmost derivations, and so would have two unique parse trees. For example, we can derive <span class="math">\(a + b * c\)</span> by starting with <span class="math">\(S \implies S O S \implies S O S O S \implies \impies a O S O S \implies \ldots\)</span>, or with <span class="math">\(S \implies S O S \implies a O S \implies a O S O S \implies \ldots\)</span>. This is equivalent to parsing it as <span class="math">\((a + b) * c\)</span> or <span class="math">\(a + (b * c)\)</span>.</p>
<p>If we just care about whether a word is in a language, then it doesn't matter if we use an ambiguous grammar. However, for compilers, the shape of the parse tree determines the meaning of the program, so words that have two different parse trees would have two different meanings.</p>
<p>To resolve ambiguity, we could use heuristics (often called <strong>precedence</strong>) to guide which rule we expand next. However, most of the time we just want to use an unambiguous grammar instead.</p>
<p>The unambiguous version of the language can be written as <span class="math">\(E \to E O T; T \to a \mid b \mid c \mid (E); O \to + \mid - \mid * \mid /\)</span></p>
<p>To get the multiplicative operators to have precedence over the additive operators, we can use the following grammar: <span class="math">\(E \to E Plus T \mid T; T \to T Mult F \mid F; F \to a \mid b \mid c \mid (E); Plus \to + \mid -; Mult \to * \mid /\)</span>. Note that we used a rule for each level of precedence - <span class="math">\(E\)</span> is the precedence level for additive operators, and <span class="math">\(T\)</span> is for multiplicative operators.</p>
<p>If <span class="math">\(L\)</span> is a context-free language, there may not necessarily be an unambiguous context-free grammar for it. The languages that only have ambiguous grammars are <strong>inherently ambiguous grammars</strong>. ;wip: give example</p>
<p>It is not possible to tell if an arbitrary context-free grammar is ambiguous - this problem is undecidable. However, we can write tools that can ensure that a grammar, constrained in certain ways (like no left recursion), is always unambiguous. Tools like ANTLR can do this.</p>
<p>Testing whether two grammars describe the same language is also an undecidable problem.</p>
<p>A finite language can be recognized with a loop-less program. A regular language can be recognized with a program that uses finite memory (this is because regular languages are recognized with DFAs, and the states of a DFA represent the program's memory). A context-free language can be recognized with an NFA and a stack (this is a program that can use infinite memory, since the stack can grow arbitrarily).</p>
<p>A real compiler needs a bit more than just recognition. If the program is valid, we need a proof in the form of a parse tree. If the program is invalid, we also need a proof in the form of an error message. The process of finding the derivation, and the parse tree, is known as <strong>parsing</strong>.</p>
<p>Parsing is the following problem: given a grammar <span class="math">\(G\)</span>, start symbol <span class="math">\(S\)</span>, and a word <span class="math">\(w\)</span>, determine the derivation <span class="math">\(S \implies \ldots \implies w\)</span>, or prove that there is no possible derivation.</p>
<p>We can do parsing in two ways - finding the derivation forwards (starting from the left, expand until we have <span class="math">\(w\)</span>, this is <strong>top-down parsing</strong>), or finding it backwards (starting from the right, unexpand until we have <span class="math">\(S\)</span>, this is <strong>bottom-up parsing</strong>). Both are used in practice, and they each have advantages for different types of grammars.</p>
<h2 id="top-down-parsing">Top-down Parsing</h2>
<p>The basic idea is to start with <span class="math">\(S\)</span>, apply grammar rules, and eventually reach <span class="math">\(w\)</span>.</p>
<p>We start with <span class="math">\(S\)</span>, and then repeatedly attempt to resolve the leftmost non-terminal until we have obtained the input desired, or an error otherwise. In this way we obtain the leftmost derivation. Potentially, we could also obtain the rightmost derivation by always attempting to resolve the rightmost nonterminal.</p>
<p>We will use a stack to store all the <span class="math">\(\alpha_i\)</span>, in reverse, in the derivation <span class="math">\(S \implies \alpha_1 \implies \ldots \implies \alpha_n \implies w\)</span>. We will then match against characters in <span class="math">\(w\)</span> from left to right. We will also enforce the property that the consumed input concatenated with the reversed stack contents are equal to some <span class="math">\(\alpha_i\)</span>.</p>
<p>We will use <strong>augmented grammars</strong> to make parsing simpler. Basically, we take a normal grammar and add the symbols <span class="math">\(\vdash\)</span> and <span class="math">\(\dashv\)</span> (the beginning and ending symbols), and a new start symbol <span class="math">\(S&#39;\)</span>, defined as <span class="math">\(S&#39; \to \vdash S \dashv\)</span>. Our word is now <span class="math">\(\vdash w \dashv\)</span>.</p>
<h1 id="section-13">26/2/15</h1>
<p>The language of WLP4 is defined using regular expressions to determine the tokenizations, a context-free grammar to define the parsing, context-sensitive code to do things like check types and declarations, and a mapping from parse tree structures to executable output to determine the semantics of the language.</p>
<p>The basic structure of a top-down parser is as follows:</p>
<pre><code># input is given as an array of tokens, which are also terminals
current_input = [START_STATE]
while current_input != full_input:
    let `A` be a nonterminal in `current_input` # usually this would always be the leftmost nonterminal in order to get the leftmost derivation
    find production rules such that $A \to \ldots$, and select one such rule `A \to value`, or give error if none # it is very important to choose the right rule
    replace the instance of `A` chosen earlier with `value` in `current_input`</code></pre>
<p>Basically, we are repeatedly reducing the nonterminals in the input until we have all terminals, or have determined that the word is not in the language.</p>
<p>The step where we find which production rule to use is an important part of top-down parsing. Generally, this is based on</p>
<p>We can implement top-down parsing using a stack <span class="math">\(S = X_n \ldots X_1, X_1, \ldots, X_n \in V\)</span> (stack contains symbols in the vocabulary). The parsing is made possible by reading the input left-to-right into <code>read_input</code> and enforcing the invariant <code>current_input = read_input + S</code> as we read through.</p>
<p>Consider the following grammar: <span class="math">\(S \to A y B; A \to ab; A \to cd; B \to z; B \to wx\)</span>. Consider the word <span class="math">\(w = \vdash abywx \dashv\)</span>. Then to parse it, we do the following:</p>
<table>
<thead>
<tr class="header">
<th align="left">Stack</th>
<th align="left">Read input</th>
<th align="left">Unread input</th>
<th align="left">Next Action</th>
<th align="left">Current Input</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math">\(S&#39;\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
<td align="left">top of stack is nonterminal, pop and pick rule <span class="math">\(S&#39; \to \vdash S \dashv\)</span></td>
<td align="left"><span class="math">\(S&#39;\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\vdash S \dashv\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
<td align="left">read <span class="math">\(\vdash\)</span>, compare to top of stack, pop</td>
<td align="left"><span class="math">\(\vdash S \dashv\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(S \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left"><span class="math">\(abywx \dashv\)</span></td>
<td align="left">top of stack is nonterminal, pop and pick rule <span class="math">\(S \to AyB\)</span></td>
<td align="left"><span class="math">\(\vdash S \dashv\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(AyB \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left"><span class="math">\(abywx \dashv\)</span></td>
<td align="left">top of stack is nonterminal, pop and pick rule <span class="math">\(A \to ab\)</span></td>
<td align="left"><span class="math">\(\vdash AyB \dashv\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(abyB \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left"><span class="math">\(abywx \dashv\)</span></td>
<td align="left">read <span class="math">\(a\)</span>, compare to top of stack, pop</td>
<td align="left"><span class="math">\(\vdash aayB \dashv\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(byB \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash a\)</span></td>
<td align="left"><span class="math">\(bywx \dashv\)</span></td>
<td align="left">read <span class="math">\(b\)</span>, compare to top of stack, pop</td>
<td align="left"><span class="math">\(\vdash aayB \dashv\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(yB \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash ab\)</span></td>
<td align="left"><span class="math">\(ywx \dashv\)</span></td>
<td align="left">read <span class="math">\(y\)</span>, compare to top of stack, pop</td>
<td align="left"><span class="math">\(\vdash aayB \dashv\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(B \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash aby\)</span></td>
<td align="left"><span class="math">\(wx \dashv\)</span></td>
<td align="left">top of stack is nonterminal, pop and pick rule <span class="math">\(B \to wx\)</span></td>
<td align="left"><span class="math">\(\vdash aayB \dashv\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(wx \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash aby\)</span></td>
<td align="left"><span class="math">\(wx \dashv\)</span></td>
<td align="left">read <span class="math">\(w\)</span>, compare to top of stack, pop</td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(x \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash abyw\)</span></td>
<td align="left"><span class="math">\(x \dashv\)</span></td>
<td align="left">read <span class="math">\(x\)</span>, compare to top of stack, pop</td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(\dashv\)</span></td>
<td align="left"><span class="math">\(\vdash abywx\)</span></td>
<td align="left"><span class="math">\(\dashv\)</span></td>
<td align="left">read <span class="math">\(\dashv\)</span>, compare to top of stack, pop</td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left">end of input, accept since stack empty</td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
</tr>
</tbody>
</table>
<p>Note that we only really need to care about the stack and the read input. That means that we can implement a top-down parser using just a stack as storage.</p>
<p>;wip: missed the rest due to sleep</p>
<h1 id="section-14">3/3/15</h1>
<p>;wip: make sure to define nullable</p>
<p>Calculating first sets:</p>
<pre><code># grammar is a list of pairs of nonterminals and their expansions (lists of vocabulary elements)
First = {A: set() for A, expansion in grammar}
do:
    for A, expansion in grammar:
        for element in expansion:
            if element is a terminal:
                First[A].add(element)
                break
            else:
                First[A].extend(First[element])
                if not Nullable[element]: break
until First stops changing</code></pre>
<p>A grammar that has matched <span class="math">\(b\)</span>-<span class="math">\(d\)</span> and <span class="math">\(p\)</span>-<span class="math">\(q\)</span> pairs with <span class="math">\(l\)</span> in between can be written as <span class="math">\(S&#39; \to \vdash S \dashv; S \to b S d; S \to p S q; S \to C; C \to lC; C \to \epsilon\)</span>.</p>
<p>Running this algorithm on the grammar results in the first set <span class="math">\(\set{\vdash}\)</span> for <span class="math">\(S&#39;\)</span>, <span class="math">\(\set{b, p, l}\)</span> for <span class="math">\(S\)</span>, and <span class="math">\(\set{l}\)</span> for <span class="math">\(C\)</span>.</p>
<p>The follow sets for this algorithm are <span class="math">\(\set{\dashv, d, q}\)</span> for <span class="math">\(S\)</span> and <span class="math">\(\set{\dashv, d, q}\)</span> for <span class="math">\(C\)</span>.</p>
<p>With the first and follow sets, we can create a prediction function <span class="math">\(\text{Predict}(A, a) = \set{A \to B \middle| a \in First^*(B)} \cup \set{A \to B \middle| Nullable(B) \wedge a \in \text{Follow}(A)}\)</span>, where <span class="math">\(A\)</span> is a nonterminal and <span class="math">\(a\)</span> is a terminal. Basically, this describes the union of the set of rules for which the right hand side begins with the terminal <span class="math">\(a\)</span>, and the set of rules that can expand to 0 tokens and are followed by <span class="math">\(a\)</span>.</p>
<p>We can also represent the prediction table as a table where the rows are different nonterminals <span class="math">\(A\)</span> and the columns are different terminals <span class="math">\(a\)</span>, with the cells filled with the resulting rules.</p>
<p>This works for <span class="math">\(LL(1)\)</span> grammars. A grammar is <span class="math">\(LL(1)\)</span> if and only if:</p>
<ul>
<li>No two production rules with the same left-hand side can generate the same first symbol - the set <span class="math">\(\set{A \to B \middle| a \in First^*(B)}\)</span> cannot have more than 1 element.</li>
<li>As a result, the grammar <span class="math">\(E \to E + T \mid T; T \to T * F \mid F&#39; F \to a \mid b \mid c\)</span> is not <span class="math">\(LL(1)\)</span> because it is <strong>left recursive</strong> - given an input <span class="math">\(a\)</span>, it is possible for either <span class="math">\(E\)</span> or <span class="math">\(T\)</span> to generate it.</li>
<li>Left recursion will immediately make a grammar not <span class="math">\(LL(1)\)</span>. Right recursion could also do so, but not always.</li>
<li>To make it <span class="math">\(LL(1)\)</span>, we could do something like <span class="math">\(E \to T E&#39;; E&#39; \to \epsilon \mid + E; T \to F T&#39;; T&#39; \to \epsilon \mid * T; F \to a \mid b \mid c\)</span>. This is called <strong>left factoring</strong>.</li>
<li>There is only one way to nullify a nullable symbol - the set <span class="math">\(\set{A \to B \middle| Nullable(B) \wedge a \in \text{Follow}(A)}\)</span> cannot have more than 1 element.</li>
<li>In fact, if there is more than 1 way to nullify a symbol, then the grammar is ambiguous.</li>
</ul>
<h2 id="bottom-up-parsing">Bottom-up Parsing</h2>
<p>The basic idea is to start with <span class="math">\(w\)</span>, and then go back using rules to get <span class="math">\(S&#39;\)</span> from <span class="math">\(S&#39; \implies \alpha_1 \implies \ldots \implies \alpha_k \implies w\)</span>.</p>
<p>Parsing will always require a stack. In our bottom-up parsing, we are going to store partially reduced input read so far.</p>
<p>In our case, we will enforce the invariant <code>stack + unread_input \in \set{S', \alpha_1, \ldots, \alpha_k, w}</code> after reading each terminal from the input.</p>
<p>Consider the following grammar: <span class="math">\(S \to A y B; A \to ab; A \to cd; B \to z; B \to wx\)</span>. Consider the word <span class="math">\(w = abywx\)</span>. Then to parse it, we do the following:</p>
<table>
<thead>
<tr class="header">
<th align="left">Stack</th>
<th align="left">Read input</th>
<th align="left">Unread input</th>
<th align="left">Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
<td align="left">(initial state)</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left"><code>abywx</code></td>
<td align="left">shift <span class="math">\(\vdash\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(\vdash a\)</span></td>
<td align="left"><span class="math">\(\vdash a\)</span></td>
<td align="left"><code>bywx</code></td>
<td align="left">shift <span class="math">\(a\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\vdash ab\)</span></td>
<td align="left"><span class="math">\(\vdash ab\)</span></td>
<td align="left"><code>ywx</code></td>
<td align="left">shift <span class="math">\(b\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(\vdash A\)</span></td>
<td align="left"><span class="math">\(\vdash ab\)</span></td>
<td align="left"><code>ywx</code></td>
<td align="left">reduce <span class="math">\(A \to ab\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\vdash Ay\)</span></td>
<td align="left"><span class="math">\(\vdash aby\)</span></td>
<td align="left"><code>wx</code></td>
<td align="left">shift <span class="math">\(y\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(\vdash Ayw\)</span></td>
<td align="left"><span class="math">\(\vdash abyw\)</span></td>
<td align="left"><span class="math">\(x \dashv\)</span></td>
<td align="left">shift <span class="math">\(w\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\vdash Ayw\)</span></td>
<td align="left"><span class="math">\(\vdash abyw\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left">shift <span class="math">\(x\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(\vdash Aywx\)</span></td>
<td align="left"><span class="math">\(\vdash abywx\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left">reduce <span class="math">\(B \to wx\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\vdash AyB\)</span></td>
<td align="left"><span class="math">\(\vdash abywx\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left">reduce <span class="math">\(AyB \to S\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(\vdash S\)</span></td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left">shift <span class="math">\(\dashv\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(\vdash S \dashv\)</span></td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left">reduce <span class="math">\(S&#39; \to \vdash S \dashv\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(S&#39;\)</span></td>
<td align="left"><span class="math">\(\vdash abywx \dashv\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left">accept</td>
</tr>
</tbody>
</table>
<p>Notice that at each step we have two possible actions:</p>
<ul>
<li>Shift - moving one character from the input to the stack.</li>
<li>Reduce - the top of the stack is the right-hand side of some grammar rule, so replace it with the left hand side.</li>
</ul>
<p>We accept if the stack is <span class="math">\(S&#39;\)</span> and the unread input is <span class="math">\(\epsilon\)</span> (or we have <span class="math">\(\vdash S \dashv\)</span> on an empty input, or when we shift <span class="math">\(\dashv\)</span>).</p>
<p>How do we know whether to shift or reduce? We can decide based on the next token, but in practice this is very difficult.</p>
<p>Donald Knuth is known for proposing big-O notation as an algorithm analysis tool, writing the series of books &quot;The Art Of Computer Programming&quot;, and the TeX typesetting system, among others.</p>
<p>He also invented a theorem: the set <span class="math">\(\set{wa \middle| \exists x, S&#39; \implies^* wax}\)</span> is a regular language. In other words, if <span class="math">\(w\)</span> is our stack and <span class="math">\(a\)</span> is our next input char, then we can use a DFA to describe and decide when to shift or reduce. This technique is called <strong>LR parsing</strong> (L for left-right scanning, R for rightmost derivations).</p>
<h1 id="section-15">5/3/15</h1>
<p>An <strong>item</strong> is a production with a dot (<span class="math">\(\cdot\)</span>) somewhere on the right-hand side, and represents the partially completed rule.</p>
<p>Consider the grammar <span class="math">\(S&#39; \to \vdash E \dashv, E \to E + T, E \to T, T \to id\)</span>.</p>
<p>;wip: define LR(0) and LR parsing</p>
<p>When we are LR parsing, we have a DFA. The states of this DFA are sets of rules, each with a dot inserted in them to represent where we are in parsing the rule.</p>
<p>When we create a state from another state using a transition symbol <span class="math">\(A\)</span>, we take the rules of that state that have their dot followed by <span class="math">\(A\)</span> and move the dot to the right by 1 symbol for every rule. Then, for each rule, if the dot precedes a nonterminal <span class="math">\(A\)</span>, then we include all rules <span class="math">\(A \to \ldots\)</span> as <span class="math">\(A \to \cdot \ldots\)</span>. We do this repeatedly until there are no more dots preceding nonterminals. For example, if we create a state from <span class="math">\(S&#39;\)</span>, we move the dot to the right, include <span class="math">\(E\)</span> rules, and then include <span class="math">\(T\)</span> rules, to get <span class="math">\(S&#39; \to \vdash \cdot E \dashv, E \to \cdot E + T, E \to \cdot T, T \to \cdot id\)</span>.</p>
<p>The start state has only one rule, for the starting rule for the grammar <span class="math">\(S&#39; \to \cdot \vdash S \dashv\)</span>. For the above grammar, we have <span class="math">\(S&#39; \to \cdot \vdash S \dashv\)</span>. We want to buil the entire DFA starting from this start state.</p>
<p>For every state, we now create new states corresponding each symbol following the dots in the rules, with those following symbols as the transitions to those states. For example, a state <span class="math">\(S&#39; \to \vdash \cdot E \dashv, E \to \cdot E + T, E \to \cdot T, T \to \cdot id\)</span> creates 3 states, with transitions <span class="math">\(E, T, id\)</span>. For example, the state <span class="math">\(S&#39; \to \vdash \cdot E \dashv, E \to \cdot E + T, E \to \cdot T, T \to \cdot id\)</span> creates the state <span class="math">\(E \to T \cdot\)</span> using transition <span class="math">\(T\)</span>. If there are no symbols following dots, then we do not create any new states. Duplicate states are combined into one state.</p>
<p>We repeat this for the new states, and their new states, until we no longer have new states. For the above grammar, we get the following DFA:</p>
<pre><code>$S&#39; \to \vdash E \dashv$ &gt; $\vdash$ (1) &gt; $S&#39; \to \vdash \cdot E \dashv, E \to \cdot E + T, E \to \cdot T, T \to \cdot id$ (1)
$S&#39; \to \vdash \cdot E \dashv, E \to \cdot E + T, E \to \cdot T, T \to \cdot id$ &gt; $id$ &gt; $T \to id \cdot$
                                                                                 &gt; $T$ &gt; $E \to T \cdot$
                                                                                 &gt; $E$ &gt; $S&#39; \to \vdash E \cdot \dashv, E \to E \cdot + T$
$S&#39; \to \vdash E \cdot \dashv, E \to E \cdot + T$ &gt; $\dashv$ &gt; $S&#39; \to \vdash E \dashv \cdot$
                                                  &gt; $+$ &gt; $E \to E + \cdot T, T \to \cdot id$
$E \to E + \cdot T, T \to \cdot id$ &gt; $id$ &gt; $E \to T \cdot$
                                    &gt; $T$ &gt; $E \to E + T \cdot$</code></pre>
<p>To use the machine, we start in the start state with a stack that contains only the start state, and step through the DFA either shifting or reducing at each state.</p>
<p>To <strong>shift</strong>, we read a char from the input to stack, and then follow the transition for that char if there is one, and otherwise reduce if the state is a reduce state, and give an error otherwise. A reduce state is a state that contain a single production rule, with the dot at the very right.</p>
<p>To <strong>reduce</strong>, we pop the right-hand side off the stack, backtrack by the size of the right-hand side, then follow the transition for the left-hand side and push the left-hand side onto the stack.</p>
<p>We also first push the DFA state we are transitioning to into the stack before transitioning, in order to be able to backtrack later. We could also use a separate stack for this, but it is more elegant to use just one in theory.</p>
<p>We accept when <span class="math">\(S&#39;\)</span> is on the stack by the end of the DFA.</p>
<p>Parsing <span class="math">\(\vdash id + id + id \dashv\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="left">Stack</th>
<th align="left">Read</th>
<th align="left">Unread</th>
<th align="left">Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math">\(1\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left"><span class="math">\(\vdash id + id + id \dashv\)</span></td>
<td align="left">Shift, go to state 2</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(1 \vdash 2\)</span></td>
<td align="left"><span class="math">\(\vdash\)</span></td>
<td align="left"><span class="math">\(id + id + id \dashv\)</span></td>
<td align="left">Shift, go to state 6</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(1 \vdash 2 id 6\)</span></td>
<td align="left"><span class="math">\(\vdash id\)</span></td>
<td align="left"><span class="math">\(+ id + id \dashv\)</span></td>
<td align="left">Reduce <span class="math">\(T \to id\)</span>: pop 1 symbol and 1 state, so now in state 2, then push <span class="math">\(T\)</span> and go to state 5.</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(1 \vdash 2 T 5\)</span></td>
<td align="left"><span class="math">\(\vdash id\)</span></td>
<td align="left"><span class="math">\(+ id + id \dashv\)</span></td>
<td align="left">Reduce <span class="math">\(E \to T\)</span>: pop 1 symbol and 1 state, so now in state 2, then push <span class="math">\(E\)</span> and go to state 3.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(1 \vdash 2 E 3\)</span></td>
<td align="left"><span class="math">\(\vdash id\)</span></td>
<td align="left"><span class="math">\(+ id + id \dashv\)</span></td>
<td align="left">Shift, go to state 7</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(1 \vdash 2 E 3 + 7\)</span></td>
<td align="left"><span class="math">\(\vdash id +\)</span></td>
<td align="left"><span class="math">\(id + id \dashv\)</span></td>
<td align="left">Shift, go to state 6</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(1 \vdash 2 E 3 + 7 id 6\)</span></td>
<td align="left"><span class="math">\(\vdash id + id\)</span></td>
<td align="left"><span class="math">\(+ id \dashv\)</span></td>
<td align="left">Reduce <span class="math">\(T \to id\)</span>: pop 1 symbol and 1 state, so now in state 2, then push <span class="math">\(T\)</span> and go to state 5.</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(1 \vdash 2 E 3 + 7 T 8\)</span></td>
<td align="left"><span class="math">\(\vdash id + id\)</span></td>
<td align="left"><span class="math">\(+ id \dashv\)</span></td>
<td align="left">Reduce <span class="math">\(E \to E + T\)</span>: pop 3 symbols and 3 states, so now in state 2, then push <span class="math">\(T\)</span> and go to state 3.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(1 \vdash 2 E 3\)</span></td>
<td align="left"><span class="math">\(\vdash id + id\)</span></td>
<td align="left"><span class="math">\(+ id \dashv\)</span></td>
<td align="left">Shift, go to state 7.</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(1 \vdash 2 E 3 + 7\)</span></td>
<td align="left"><span class="math">\(\vdash id + id +\)</span></td>
<td align="left"><span class="math">\(id \dashv\)</span></td>
<td align="left">Shift, go to state 6.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(1 \vdash 2 E 3 + 7 id 6\)</span></td>
<td align="left"><span class="math">\(\vdash id + id + id\)</span></td>
<td align="left"><span class="math">\(\dashv\)</span></td>
<td align="left">Reduce <span class="math">\(T \to id\)</span>: pop 1 symbol and 1 state, so now in state 7, then push <span class="math">\(T\)</span> and go to state 8.</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(1 \vdash 2 E 3 + 7 T 8\)</span></td>
<td align="left"><span class="math">\(\vdash id + id + id\)</span></td>
<td align="left"><span class="math">\(\dashv\)</span></td>
<td align="left">Reduce <span class="math">\(E \to E + T\)</span>: pop 3 symbols and 3 states, so now in state 2, then push <span class="math">\(E\)</span> and go to state 3.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(1 \vdash 2 E 3\)</span></td>
<td align="left"><span class="math">\(\vdash id + id + id\)</span></td>
<td align="left"><span class="math">\(\dashv\)</span></td>
<td align="left">Shift, go to state 4.</td>
</tr>
<tr class="even">
<td align="left"><span class="math">\(1 \vdash 2 E 3 \dashv 4\)</span></td>
<td align="left"><span class="math">\(\vdash id + id + id \dashv\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left">Reduce <span class="math">\(S&#39; \to \vdash E \dashv\)</span>.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(1 S&#39;\)</span></td>
<td align="left"><span class="math">\(\vdash id + id + id \dashv\)</span></td>
<td align="left"><span class="math">\(\epsilon\)</span></td>
<td align="left">Accept.</td>
</tr>
</tbody>
</table>
<p>If a state looks like <span class="math">\(A \to \cdot x, B \to y \cdot\)</span>, we have a <strong>shift-reduce</strong> conflict and don't know whether to shift or reduce at this state.</p>
<p>If a state looks like <span class="math">\(A \to x \cdot, B \to y \cdot\)</span>, we have a <strong>reduce-reduce</strong> conflict, and don't know which rule to reduce by.</p>
<p>If any complete item <span class="math">\(A \to x \cdot\)</span> that isn't the only rule in the state, we will have a shift-reduce conflict or a reduce-reduce conflict. This makes the grammar not <span class="math">\(LR(0)\)</span>.</p>
<p>If we have a right-associative grammar like <span class="math">\(S&#39; \to \vdash E \dashv, E \to T + E, E \to T, T \to id\)</span>, when we build the DFA we notice that there is a shift-reduce conflict when we try to make the state <span class="math">\(E \to T \cdot + E, E \to T \cdot\)</span>.</p>
<p>Basically, if the next input is <span class="math">\(id \dashv\)</span> then we need to reduce, and if the input is <span class="math">\(id + \ldots\)</span> then we should not reduce. Since we can't know this, we can't parse this grammar using LR parsing.</p>
<p>However, we can get around this by using lookahead. For each rule in the DFA <span class="math">\(A \to x \cdot\)</span>, we attach <span class="math">\(\text{Follow}(A)\)</span> - the follow set (<span class="math">\(\set{\dashv}\)</span> for <span class="math">\(A\)</span>, <span class="math">\(\set{+, \dashv}\)</span> for <span class="math">\(T\)</span>). Basically, when we are at that state, we look ahead at the next character <span class="math">\(c\)</span>, and only consider a rule in that state <span class="math">\(A\)</span> if <span class="math">\(c \in \text{Follow}(A)\)</span> - each rule can only be used if its follow set contains the next character.</p>
<p>With lookahead, we can figure out which rule we want to use in the conflicting cases, solving the shift-reduce conflict. This is called <span class="math">\(SLR(1)\)</span> parsing - simple LR parsing with 1 character lookahead. This is pretty good, since it resolves many but not all conflicts.</p>
<p>To build parse trees using bottom-up parsing, we just attach the old nodes as children of the new node when we are reducing, and at the end we have a tree.</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>