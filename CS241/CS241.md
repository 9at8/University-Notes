CS241
=====

Foundations of Sequential programs.

    Instructor: Bradley Lushman
    ISA: Sean Harrap
    Email: CS241@uwaterloo.ca
    Web: http://www.student.cs.uwaterloo.ca/~cs241

$$
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
$$

# 6/1/15

Weekly assignments, 11 total.

A sequential program is an "ordinary" program - one that is not concurrent, parallel, or so on. Sequential programs only do one thing at a time - they are signle threaded.

As this course focuses on the foundation, we will be starting from the hardware, and figure out how sequential program works from there.

This course uses a simulated MIPS machine for the assignment. At the end we will be able to run a relatively complex C-like language that can run on the MIPS machine.

Binary and Hexadecimal Numbers
------------------------------

Refer to earlier notes for background.

A bit is a binary digit - a 0 or a 1. A nibble is a collection of 4 bits. A byte is a collection of 8 bits, although historically it could be any number needed to hold a signle character - sometimes 7. A word is the a machine-specific grouping of bits - the number of bits in an address on the computer.

In this course we will be using a 32-bit architecture.

Data in a computer's memory could mean anything - it is simply data.

There are a number of conventions for representing negative numbers in binary. The **sign magnitude** convention has the leftmost bit of a binary number represent whether it is negative. Although this system is simple, it's difficult to do math with since we have to treat the first bit separately for a lot of operations. Plus, there's two representations for the number 0 - `00...00` and `10...00`.

The **one's complement** convention has positive numbers written normally, and negative numbers are their magnitude with their bits flipped. This works better for certain operations, but there is still two 0 representations, which complicates things unnecessarily.

The **two's complement** convention has positive numbers written normally, and the negative numbers are simply their magnitude with their bits flipped and 1 added. The advantage of this system is that the first bit still denotes the sign. Plus, there is only one representation for 0, and operations like addition work as usual, modulo $2^n$.

To find the numerical value of a number represented using $n$-bit two's complement, we first check the sign bit. If 0, then we simply convert binary to a non-negative number as usual. Otherwise, we convert the binary to a non-negative number as usual, and subtract $2^n$ from that result to get a negative number as a result.

To quickly negate a binary number in two's complement quickly, flip all the bits and add 1. This allows us to quickly convert negative numbers to binary and hex.

For 3-bit numbers, each binary string, in ascneding order, represents 0, 1, 2, 3, -4, -3, -2, -1.

A byte could also potentially be a character. Characters have multiple possibble encodings, such as unicode and ASCII. ASCII is a 7-bit code (for historical reasons, in order to avoid sending expensive bits) that has representations for most America-centric characters, although there are 8-bit extensions like those by IBM that add multilingual characters. In an 8-bit byte world, the eighth bit is 0 in standard ASCII. We will be using ASCII in this course to represent characters.

A byte could also be part of a data structure, or even part of an instruction. In this course, we have 32-bit instructions. A byte could also just be garbage - memory that isn't currently being used for anything and has no meaning.

The idea is that the meaning of a byte is whatever meaning we assign to it, and we have to remember what each little chunk of memory we're using actually means.

# 8/1/15

The Machine
-----------

Computer programs operate on data, and are theselves data. Historically, the program code was stored in special memory such as punch cards or switches, and it operated on separate data memory. This was known as the Harvard architecture.

John Von Neumann proposed that we could just put the program code in the same memory as the rest of the data. This is known as the Von Neumann architecture. The result of program code being treated as just a program was that we can now write programs that operate on programs, even themselves. For example, operating systems, compilers, and viruses that modify existing code to do what it wants.

Machine language is specific to a particular processor type, like MIPS, ARM, and x86. In this course we will be using a simplified version of MIPS.

### Architecture

The MIPS machine consists of a CPU and main memory (RAM), connected via a bus. The CPU contains the ALU, the registers, and the control unit.

The ALU (arithmetic logic unit) is responsible for doing arithmetic and logic, for doing things like addition and subtraction.

Registers are simply small pieces of memory that are really fast to work with since they are so close to the CPU - they are the fastest pieces of memory we have, ad we have relatively little of it. Most instructions can actually only work directly on registers, so we usually need to bring data from the main memory into registers before we work on them.

There are the 32-bit **general purpose registers** R00 to R31, which can be used for general purpose data storage. In our assembly code, we refer to them with \$0, \$1, \$2, and so on, until \$31. However, R00 is always 0, and R30 (stack pointer/frame pointer) and R31 (return pointer) have other purposes in control flow by convention.

The **program counter register** is responsible for keeping track of where we are in the main memory for executing code. It holds an address to the next instruction to run while the current instruction is running. Some commands manipulate this register, so we can actually do control flow by modifying this in a certain way.

By convention, we guarantee that a certain address in memory like 0 always starts off with valid code, and then we initialize PC to that location. So when we start the machine, it will start executing instructions from that location and go onwards.

The **instruction register** IR is closely related to PC. Instead of holding the address of the next instruction, it holds the current instruction itself (not its address).

The HI/LO registers are a pair of registers that are useful for storing the output of operations like division, which results in the quotient and the remainder, stored in this pair of registers. They cannot be accessed directly, but are implicitly used by certain instructions.

The **memory address register/memory data register** MAR and MDR are used for working with memory. MAR is used to store the desired address for us to load, and MDR is used to store the loaded value.

MAR/MDR, PC, and IR are all hidden registers - they are not visible to our programs.

The control unit decodes instructions, and dispatches them to the rest of the computer to actually execute them.

There are many types of memory, such as registers, CPU cache, main memory, disk memory, and network memory, in order of increasing distance from the CPU. The closer the memory to the CPU, the faster it is to work with.

The main memory (RAM) is a large amount of memory stored away from the CPU, and communicates with the CPU over the bus (physically, a bunch of wires). It can be thought of as a huge array of $n$ bytes. Each byte in the RAM has an address, one of $0, \ldots, n - 1$.

Since a word in MIPS is 32 bits (4 bytes), and we can only access memory on word boundaries, we can only get or set addresses that are multiples of 4.

RAM access is much slower than register access - as much as three orders of magnitude. Usually, data in RAM must be transferred to registers before it can be used.

### Instructions

It takes 5 bits to encode the 32 general purpose registers (we can't directly use the other registers as memory). Therefore, we need 15 bits to encode two source registers and a destination register. That means we have 17 bits left to encode the rest of our instruction.

There are actually only two instructions for working with RAM - load word and store word.

If we do something invalid like dereferencing an invalid address, our machine will simply crash. Real-world machines have machine exceptions to deal with this.

The CPU essentially works as follows:

    PC <- 0
    loop:
        IR <- MEM[PC]
        PC <- PC + 4
        decode and execute the instruction in IR

A program gets executed by getting called by a **loader**, which puts the program into memory (from somewhere like the hard drive) and sets PC to the first instruction in the program. This is the role of the operating system when opening a program.

When our program wants to end, we want to return control flow back to the loader. This is done by setting PC to the next instruction of the loader after the one that started our program.

Conventionally, R31 is set to the address of the next instruction in the loader, by the loader - the address we need to jump to in order to return to the loader. Therefore, in our program code, we simply set PC to R31 in order to jump back to the loader. This looks something like `jr $31` in MIPS assembly.

Example code to add value in register 5 to register 7, then return:

    RAM LOCATION  BINARY                                   HEX         MEANING
    ------------  ---------------------------------------  ----------  --------------
    00000000      0000 0000 1010 0111 0001 1000 0010 0000  0x00A71820  add $3, $5, $7
    00000004      0000 0011 1110 0000 0000 0000 0000 1000  0x03e00008  jr $31

The resulting code is therefore the binary representation of the hex value `00A7182003e00008`.

To store a literal value into a register, we can use the **load immediate and skip** (`lis`) instruction. Essentially, ;wip

Note that ASCII is a mapping between characters and binary, and ASCII values are not binary. For example, `0010000100110101` is simply an ASCII string of 1 and 0, while `!5` would be the ASCII encoding the previous string would result in if encoded in binary.

To read the ASCII representation of a given binary file, use `xxd <filename>` or `hexdump -Cv <filename>`, where `<filename>` is the path to the file to represent. `xxd -bits` shows binary, and `xxd -cols n` shows `n` bytes per line.

# 13/1/15

Assembly Language
-----------------

Instead of writing in hex or binary, which is very tedious to convert, we can simply use the mnemonics like `add` and `lis`, and have a program (the **assembler**) translate it to binary for us. This reduces our chances of error and makes programming much simpler.

Essentially, one line of assembly is one word, generally one machine instruction like `jr`. There are also lines that are **directives**, which are instructions to the assembler to do something, rather than the machine. For example, the `.word X` directive tells the assembler that at the current point, we want the resulting binary output to have the specified literal value `X`.

The above code example could therefore be written as:

    add $3, $5, $7
    jr $31

Some instructions unconditionally modify PC, like `jr` - these are known as jumps. Some instructions conditionally modify PC, like `bne` - these are known as branches. Generally, the modification is given in terms of the number of words to move by. Note that it is possible to give negative values.

A quick way to zero a register is `add X, $0, $0`, where `X` is a register. `lw A, i(B)` loads the value in memory at address specified by register `B` with offset specified by literal number `i`, storing the result in register `A`. `sw A, i(B)` does the opposite - writing the value of `A` in memory instead of reading from it.

The `mult A, B` instruction has a somewhat unusual form - it only has the multiplicands, and no output parameter. Instead, since multiplication can result in a 64-bit number, the answer is is stored in the HI and LO registers, the LO bits having the lower 32 bits and the rest in HI. To get values out of HI and LO, we have `mflo A`, which gets the value of LO and stores it in the register `A`, and `mfhi A`, which does the same thing, but for HI.

Division is similar. After dividing, the quotient is in LO, and the remainder is in HI.

Counting the number of instructions to jump by is tedious and error-prone. We can mitigate this using **labels**. For example:

    some_label: add $1, $2, $3
    add $4, $5, $6
    some_other_label: .word 0x12345678

Labels are assembler features that allow us to specify a specific location and use it later on. In any jump or branch instruction, we can simply write the label name rather than the offset, like `bne, $2, $0, some_label`, which will jump to the location specified by `some_label` if and only if `$2` is non-zero.

# 15/1/15

We want to be able to write functions in MIPS assembly - blocks of reusable code that we can call with parameters and have control returned to us afterward.

Our procedures need to guarantee that the registers will remain unchanged - after calling the function, the values of registers will be guaranteed to be as they were before calling. Otherwise, subfunctions will clobber their caller's register values.

The MIPS loader initializes R30 to the address of the word just past the last word in memory - this is the stack pointer, and we are using the end of the memory as a stack that grows downward. This convention allows us to use RAM as a stack for storing our calling contexts - by decreasing the stack pointer, we reveal more memory we can use, and by increasing the stack pointer, we can deallocate that memory.

We will allocate blocks of memory on the stack that we use as stack frames. Stack frames store register values and the return address.

When we call a function, we allocate a frame and store all our register values in it for safekeeping, after which we can use the registers as we want in the function. When the function returns, we restore the values of the registers, and then jump back to the return address.

When we want to return from a function, we need to set the value of PC to after the jump that called the function - the very next instruction after the function call. Since functions can be called from multiple places, the caller must give the return address.

There is a "jump and link register" instruction `jalr $s` that sets R31 to the next instruction (PC), and jumps to the specified address (setting PC to `$s`). This allows us to store the return address in R31, and jump to the function, all in one operation.

However, we also need to save R31 since `jalr` will overwrite it. In other words, we will need to store R31 on the stack.

To pass parameters to functions, we can just use registers (documenting what each parameter register is for is very important). If there are more parameters than available registers, it is also possible to store parameters on the stack.

We can also pass back the return value via an agreed-upon register. We might specify that the return value is in R2, for example.

A template for functions can now be defined:

    some_func:
    ; store the registers we will be using on the next free spaces on the stack
    sw $2, -4($30)
    sw $3, -8($30)
    
    ; decrement the stack pointer by the number of bytes used by our stack frame
    lis $2 ; $2 is free to use now that we have saved its former value
    .word 8
    sub $30, $30, $2
    
    ; body of the function
    
    ; increment the stack pointer again to restore it to its previous value, assuming we didn't change $3
    add $30, $30, $3
    
    ; make sure to pop the registers off in reverse order
    lw $3, -8($30)
    lw $2, -4($30)
    
    jr $31 ; return to the instruction after the one we called the function from

And we can call it as follows:

    main:
    ; body before the function call
    
    ; save the value of $31 on the stack to keep our return address
    sw $31, -4($30)
    lis $31 ; $31 is free to use now that we have saved its former value
    .word 4
    sub $30, $30, $31
    
    ; jump to the function with $31 as the return address
    lis $31
    .word some_func
    jalr $31 ; store the value of PC into $31 as the next instruction, and set PC to $31 - we are storing the return address in $31
    
    ; restore the value of $31 back to its original value
    lis $31
    .word 4
    add $30, $30, $31
    lw $31, -4($30) ; we do this after we're done using $31 as a temporary variable
    
    ; body after the function call
    
    jr $31 ; return back to the loader

Example procedure:

    ; sum_1_to_N: adds natural numbers from 1 to N
    ; Registers:
    ;     $1 - Value of N
    ;     $2 - Output sum
    sum_1_to_N:
    ; save the register values on the stack
      sw $1, -4($30)
      sw $3, -8($30)
      lis $1
      .word 8
      sub $30, $30, $1
      
      ; function body
      add $2, $0, $0 ; zero out the output register
      lis $3 ; set R3 to 1
      .word 1
      sum_1_to_N_loop:
        add $2, $2, $1 ; add the current value to the output
        sub $2, $2, $3 ; decrement the value
        bne $2, $0, sum_1_to_N_loop ; break out of the loop if the value is 0
      
      ; restore the values of the registers
      lis $1
      .word 8
      add $30, $30, $1
      lw $3, -8($30)
      lw $1, -4($30)
      jr $31

Input/Output
------------

To do output, write a word to 0xFFFF000C. The least significant byte will be printed as an ASCII character:

    ; prints a newline
    lis $1
    .word 0xFFFF000C
    lis $2
    .word 65
    sw $2, 0($1)

# 20/1/15

The Assembler
-------------

The assembler is a translater, from assembly code to machine code. Any translation process will involve two phases, analysis (understanding the input) and synthesis (producing the output from the understanding of the input).

Our approach is to first group the characters into meaningful tokens, like a label, a hex number, a register, a directive, or so on. Tokenization is done by something known as a **lexer**, which accepts source code and outputs a stream of tokens.

We already have a tokenizer available to us in `asm.ss` and `asm.cc`. Our goal is to group tokens into actual instructions if possible - the analysis phase. If there is an error, we must output the string "ERROR" to stderr. The assembled MIPS code is printed to stdout.

To parse labels, occasionally we will have the label definition occur after their usages. To properly assemble this, we need to do things like make an extra pass beforehand to create a symbol table, or put in a placeholder value and fix it after parsing all the labels.

Also note that it is also possible to label the address just past the end of the program, and a line may have more than one label associated with it.

Instructions in MIPS start with a 6 bit opcode, followed by their parameters that specify things like registers and offsets.

# 22/1/15

An OS can be as simple as the following:

    repeat:
        p <= the next program to run
        $2 <= loader(p)
        jalr $2
    beq $0, $0, repeat

Note that the OS itself is just a program, which means it needs to be in memory as well. Since execution starts at 0x00000000, we need to put the OS at the very beginning of the memory. That means our program cannot occupy the first location in memory.

That also means that the addresses of the labels will not be correct if we load them directly, since the assembler created the machine code assuming addresses start from the beginning of the program (a label at the very beginning of the assembly should result in a value of 0x00000000). If we load our program at offset $d$, then we must add $d$ to the value of each label or reference to a label, like `.word some_label`. This process is called **relocation** and allows us to load the code anywhere in memory and have it run properly.

Since the machine code is just a stream of bytes, we don't know which words are labels and which are not - the assembler has to give us additional information about what type of thing each word is.

As a result, what most assemblers output is not actually just machine code, but actually **object code** - machine code with additional metadata that lets us load and run the program in a clean way. These assemblers are called **relocatable assemblers**.

Basically, the metadata needed in object files includes the addresses of words that represent label addresses in the code so we can relocate it, plus ;wip

The object file format we will be using is called **MIPS executable relocatable linkable** (MERL), and often has a filename of the form `*.merl`. It looks like this:

1. The literal word 0x10000002, which is the MIPS instruction for `beq $0, $0, $2`, and skips two words ahead, past the rest of the header.
  1. this means that the object file can also be run as a normal MIPS program, since this value causes it to jump past the header and straight to the code if loaded at address 0x00000000.
  2. It also acts as a sanity check to make sure the file is really a MERL file.
2. The length of the entire module in bytes as a word - this is the length of the header (12), the machine code, and the relocation table.
3. The address of the end of the code section in bytes as a word, which is the length of the header (12) and the machine code.
4. The actual MIPS machine code.
5. The **relocation table**, which appears after the code returns and has a sequence of entries for different types of relocatables. An entry in the relocation table consists of two words:
  1. The format code - a word representing the type of entry this is. For example, a value of 1 represents that this entry is a relocation entry.
  2. The actual address, which, if this entry is a relocation entry, is the address of a word holding the address of a label. This is relative to the beginning of the header.

# 27/1/15

The MERL format could also be written as follows:

    ; MERL header
    beq $0, $0, 2
    .word end_of_module
    .word end_of_code
    
    ;code goes here
    add $1, $2, $0
    
    ; MERL footer - relocation table
    end_of_code:
    .word 0x1 ; relocatable address
    .word SOME_ADDRESS
    ; ...
    .word 0x1
    .word SOME_OTHER_ADDRESS
    end_of_module:

The `cs241.merl` tool accepts a MERL file and a relocation address and outputs MIPS machine code relocated at the desired address, ready to be loaded starting at that offset. The `mips.twoints` and `mips.array` tools also support an optional second argument that specifies the address to load the code at.

Relocation is typically done by the loader, automatically. The loader usually has the following structure:

    verify(read_word()) # check cookie
    end_of_module = read_word()
    code_length = read_word() - 12
    
    # load code
    p = allocate_RAM(code_length + stack_space)
    for i in range(0, code_length, 4):
        RAM[p + i] = read_word()
    
    relocation_table = code_length + 12 # start of relocation table
    for i in range(relocation_table, end_of_module, 8):
        format = read_word()
        if format == 1: # we only support format 0x1 for now
            offset = read_word() - 12
            RAM[p + offset] += p - 12 # we subtract 12 since in MERL addresses are relative to the start of the header, and the header wasn't loaded

When we have multiple files, we might have a function that calls a function in another file. However, the assembler can't assemble this directly since the label of the function isn't defined in the file it's being called from.

We could just append all the code together and then assemble them, which works but is not very good, since that requires the assembly code to be available (in practice, people just want to distribute MERL), and we would need to assemble it possibly multiple times in larger projects.

A better solution is a program that accepts multiple MERL files and properly puts them together into one - the **linker**. We want to put MERL files together without ever forcing it to know about ASM.

First, what we could do is have the assembler ignore unknown labels, leaving them to be resolved to actual values later. This is done by simply using a placeholder value like 0, and then storing a list of these addresses in the relocation table along with the labels we want to assign to them.

To make this easier for the assembler, we add the `.import IDENTIFIER` directive, which does not result in any output, but tells the assembler that `IDENTIFIER` is an identifier that is external to the current file and will be linked in later. This allows us to catch typos in our label names more easily - we can give an error if a label isn't defined and isn't imported.

In the MERL files, we define a new format code for the relocation table - 0x11, the **external symbol reference** (ESR) format. The table entry then contains the address of the label reference, and the name of the label:

    ; ESR relocation table entry
    .word 0x11
    .word ADDRESS # location where the label was used
    .word LENGTH # number of words/chars that make up the label name
    .word C[0] # first character of the label as ASCII value
    ; ...
    .word C[LENGTH - 1] # last character of the label as ASCII value

# 29/1/15

However, we also need a way for a module to say what labels it has, so other modules can actually import it. We don't want to just export all the labels in a MERL module - labels are often duplicated, expecially when we start having compilers generating them automatically.

Instead, we have a `.export LABEL` directive, which makes a new entry in the relocation table, in the **external symbol definition** (ESD) format. This contains the address where the label is defined, and the name of the label:

    ; ESD relocation table entry
    .word 0x05
    .word ADDRESS # location where the label was defined
    .word LENGTH # number of words/chars that make up the label name
    .word C[0] # first character of the label as ASCII value
    ; ...
    .word C[LENGTH - 1] # last character of the label as ASCII value

In programming languages like C, each function gets its own `.export` entry, which is how the linker can link C object files together. A `static` function doesn't get an ESD entry, which is why it can't be imported from other files.

Now we can write a linker:

    # Given MERl files m1 and m2 as binary blobs, and we want to make a single MERL file with m2 linked in after m1
    # `get_imports` and `get_exports` get the ESRs and ESDs, respectively, as a dictionary mapping label names to addresses
    
    a <- len(m1) - 12
    relocate m2 higher by `a`
    add `a` to every address in m2's symbol table
    assert(set_conjunction(get_exports(m1).keys(), get_exports(m2).keys()) == empty_set)
    
    # link imports and exports together
    for address1, label in get_imports(m1):
        if label in get_exports(m2):
            m1[address1] = get_exports(m2)[label]
            delete the ESR entry `label` from m1's relocation table
            add relocation entry `address1` to m1's relocation table
    for address2, label in get_imports(m2):
        if label in get_exports(m1):
            m1[address1] = get_exports(m1)[label]
            delete the ESR entry `label` from m2's relocation table
            add relocation entry `address1` to m2's relocation table
        # we don't have an else because this linker only links two MERL files at a time, so it's possible that we run this multiple times for three or more files
    
    # make the final relocation table
    relocations = set_disjunction(get_relocations(m1), get_relocations(m2))
    imports = set_disjunction(get_imports(m1), get_imports(m2))
    exports = set_disjunction(get_exports(m1), get_exports(m2))
    
    # output MERL file
    output MERL cookie
    output total code length plus relocation table size plus 12 (header size)
    output total code length plus 12 (header size)
    output m1
    output m2
    output relocations, exports, and imports

High Level Languages
--------------------

The compiler is responsible for converting a high level language to assembly. Unlike an assembler, the compiled result is not unique - there are multiple possible ways to convert high level languages to assembly.

We want a formal theory of string recognition - general principles that work in the context of any programming language.

An **alphabet** is a finite set of symbols, generally denoted $\Sigma$ ($\Sigma = \set{a, b, c}$, for example). Symbols can be anything we want, and might not even be strings.

A **string/word** is a finite sequence of symbols, each symbol belonging to $\Sigma$ ($abaabbcca$, for example).

The length of a string $s$ is $\abs{s}$. The empty string is usually denoted $\epsilon$. This should not be confused with any symbols in the alphabet - $\abs{\epsilon} = 0$.

Given a string $s$, $s^n$ is equivalent to $s$ repeated $n$ times - $s^n = s \ldots s$.

A **language** is a set of strings ($\set{a^{2n}b \mid n \ge 0}$, for example, which is an even number of $a$ followed by a $b$). The empty language is just an empty set $\emptyset$ - a language with no words. This is different from $\set{\epsilon}$, which is a language with one word, the empty word.

We want to determine if a given string belongs to a given language. How difficult this is depends on the language. For infinite sets of strings, like the set of all valid MIPS or C programs, things get a little trickier. In fact, there exist languages where this is impossible.

Therefore, we want to characterize languages by how hard they are to recognize. Noam Chomsky came up the Chomsky heirarchy to deal with this. The following is based on that, and lists language classes in ascending order of difficulty recognizing:

* The **finite languages** are the easiest to recognize - there are a finite number of strings, so we can just check each one.
* The **regular languages**.
* The **context-free languages**.
* The **context-sensitive languages**.
* The **recursive languages**.
* There are other types that are more difficult to recognize, and some that are even impossible. For example, a language that contains one word, the solution for an unsolvable problem in mathematics.

Our goal is to use as easy a level as possible on the heirarchy, since then the compiler will be simpler. Higher levels are supersets of lower levels.

# 3/2/15

### Finite Languages

We can see if a word is in a finite language by comparing it is in each element of the finite set. However, we can do this more efficiently by going char by char and filtering out the ones that are not matches.

For example, if we have a language with words "cat", "car", "cow", we can follow the following procedure:

1. Scan the input left to right.
2. If the first character is "c", then move on, and otherwise produce false.
3. If the next character is "a", and the next character is "t" or "r", and there is no next character, then produce true, and otherwise produce false if "t", "r", or end failed to match.
4. If the next character is "o", and the next character is "w", and there is no next character, then produce true, and otherwise produce false if "w" or end didn't match.
5. Otherwise, produce false.

This only scans the word once - we read only in one pass. Although the above procedure uses no memory, there is still state - the value of PC is different whenever we read a character.

We can abstract this sort of decision-making program into something like a graph. Every state in the program is represented as a circle, and transitions between states can be represented by drawing unidirectional arrows between them, labelled with what the transition is. In the above example, states might be values of the PC register, and state transitions might be labelled by the character that is read next.

A circle within a circle (or a circle with a double border) accepts - it represents a state that, if the input ends there, we can simply accept it as valid, and we can use it as an end state. A circle with an arrow pointing into it from the left represents the start state.

This sort of program is known as a **state machine**. The circles we draw are **states** - configurations of the program based on the inputs seen. The diagrams we draw are called **deterministic finite automata** (DFAs).

All state machines that check the validity of an arbitrary string in a finite language can be written as a tree. In other words, for any finite language there exists a DFA without an cycles.

We match DFAs as follows:

1. Start at the start state.
2. For each char in the input, follow the correponding arrow to the next state, and produce false if there are no possible transitions from any of the states.
3. We are now at the end of the input. Produce true if the current state is an ending state, and false otherwise.

To make the math more elegant, we define an implicit **error state**, and every single state goes to this state for unlabeled transitions. So if we get an unexpected input for a node (so we don't have a defined transition for the input), we implicitly define a transition to the error state. The error state is not special, but it is always a state that cannot transition to any other states, and it never matches.

### Regular Languages

The regular languages are built from finite languages, plus set operations like union (set union of the words of two languages - $L_1 \cup L_2 = \set{x \mid x \in L_1 \vee x \in L_2}$), concatenation (every possible combination of elements from one set concatenated to another - $L_1 L_2 = \set{xy \mid x \in L_1 \wedge y \in L_2}$), and repetition (a language repeated an arbitrary number of times - $L^* = \set{\epsilon} \cup \set{xy \mid x \in L^* \wedge y \in L} = \set{\epsilon} \cup L \cup LL \cup LLL \cup \ldots$).

Note that repetition generally results in infinite sets - this means that the regular languages contain infinite languages.

We show that a language is regular by building it up from finite languages and the set uperations (union, concatenation, and repetition). For example, $\set{a^{2n}b \mid n \ge 0}$ is regular because $\set{a^{2n}b \mid n \ge 0} = \set{aa}*\set{b}$.

A **regular expression** is a shorthand for all the cumbersome set notation we are using for describing regular languages, and is itself a language we will denote $E$. It works as follows:

* $E = \emptyset$ represents $L = \emptyset$.
* $E = \epsilon$ represents $L = \set{\epsilon}$.
* $E = aaa$ and other sequences of non-special characters represent $L = \set{aaa}$ and themselves in a set.
* $E_1|E_2$ represents $L_1 \cup L_2$.
* $E_1 E_2$ represents $L_1 L_2$.
* $E_1 = E_2^*$ represents $L_1 = L_2^*$.

For example, $\set{a^{2n}b \mid n \ge 0}$ can be represented with $(aa)*b$. Regular languages are, in practice, relatively simple. For example, C is not a regular language. A C program is made up of tokens, each of which is part of a regular language - tokens, string literals, number literals, types, and so on.

For regular languages, DFAs can have cycles - this occurs as a result of repetition.

Consider the language defined as the set containing all strings that are composed of an even number of "a" and an odd number of "b". Clearly, we can write a DFA for this using four states (even "a" and even "b" - start, odd "a" and even "b", odd "a" and odd "b", and even "a" and odd "b" - end/accept). The regular expression for this is rather difficult to write. ;wip: how?

# 5/2/15

Formally, a DFA is a 5-tuple $(Q, \Sigma, q_0, A, \delta)$, where the variables represent the following:

* $Q$ is the set of states of the DFA - all possible states. This is a finite, non-empty set.
* $\Sigma$ is the alphabet of the DFA - all possible input values. This is a finite, non-empty set.
* $q_0$ is the start state - the state at which the DFA is initialized. This is an element of $Q$.
* $A$ is the set of accepting states of the DFA - the states that the input can terminate at. This is a subset of $Q$
* $\delta$ is the transition function - a mapping from a state and input character to a new state. This is a function $q, x \to r, q \in Q, x \in x \in \Sigma, r \in Q$.

DFAs don't always have to just accept characters. It is also possible to consume entire words, using an extension of the original $\delta$ function, which only consumes one character at a time.

This **word consuming function** can be defined as $\delta^*(q, \epsilon) = q, \delta^*(q, cw) = \delta^*(\delta(q, c), w), q \in Q, c \in Sigma, w \in \Sigma^*$. A DFA accepts a word $w$ if $\delta^*(q_0, w) \in A$ - the DFA accepts the input if read from its start state. $\delta^*$ represents a function that gives the end state of the DFA given a start state and input.

Given a DFA $M$, $L(M)$ represents the set of all strings accepted by $M$. This is the set of all elements in of $\Sigma^*$ such that $M$ accepts them.

**Kleene's theorem** states that $L$ is regular if and only if $L = L(M) for some DFA $M$. In other words, the regular languages are exactly the languages for which there exists a DFA that accepts them.

A DFA is still just a representation of a computer program. In practive, these are often implemented using switch statements:

    int state = $q_0$
    char current;
    while (read_char(current)) {
        switch (state) {
            case $q_0$:
                switch (current) {
                    case 'a': state = // the state to transition to
                    // cases should exist for all possible transitions from this state
                    case 'z': state = // the state to transition to
                    default: error("No transitions available");
                }
            case $q_1$:
                // another switch statement like the one for $q_0$, for $q_1$'s transitions
        }
    }

This works but is also long and error-prone. Instead, we can use a state table, where rows are chars and columns are states, and each cell points to the next state. See the assignment 3 starter code for an example.

It is also possible to attach actions to the transitions in the DFA. Potentially, we could use this to do things like get the numerical value of a number literal while we are parsing it.

Every regular expression can be converted into a DFA, and every DFA can be converted into a regular expression. However, the translation is not always straightforward. For example, consider `(a|b)*abb`, the set of all words in $\set{a, b}^*$ that end with "abb". When we generate the DFA, we need to add extra transitions for the states that make up "abb" in order to backtrack, in order to handle inputs like "aba":

    1: START
        "a": 2
        "b": 1
    2:
        "a": 2
        "b": 3
    3:
        "a": 2
        "b": 4
    4: ACCEPT
        "a": 2
        "b": 1

### NFAs

What if we had a DFA that allowed more than one transition from the same state that have the same label? For example, a state with transitions to two other states, each with the same label.

This would mean that when we have an input with multiple possible transitions, the machine would get to choose one of the transitions - the state machine is non-deterministic. This is a **non-deterministic finite automata** (NFA). The difference from a DFA is that the machine gets choices in which transition to use.

We also assume that the machine always makes the correct choice - the state machine accepts if there exists a sequence of choices such that the input is accepted. In practice, we do this with something like backtracking - when we get to the end and we haven't accepted the input, we undo and choose a different set of choices. By using a stack storing when we made choices, we can easily use backtracking search over all possible choices.

With NFAs, the above example becomes a lot simpler, which is the main benefit of this type of machine:

    1: START
        "a": 1, 2
        "b": 1
    2:
        "b": 3
    3:
        "b": 4
    4: ACCEPT

Formally, an NFA is a 5-tuple like a DFA - $(Q, \Sigma, q_0, A, \delta)$, where the variables represent the same things as with a DFA, except the transition function. With an NFA, $\delta$ is a mapping from a state and input character to a set of new states. This is a function $q, x \to R, q \in Q, x \in x \in \Sigma, R \subseteq Q$.

An NFA accepts the input if a path exists such that the NFA results in acceptance, and rejects if none do.

We now want to define a transition function for NFAs that accepts words. What we will do is define a function that accepts a set of possible current states and a word, resulting in a new set of possible states. When we are moving through the NFA, we are basically stepping through all the possible transitions at once when we have a choice. In other words, we step through all the possible choice paths in parallel.

We define this as $\delta^*(S, \epsilon) = S, \delta^*(S, cw) = \delta^*(\set{\delta(q, c) \mid q \in S}, w), S \subseteq Q, c \in \Sigma, w \in \Sigma^*$. This is a function that results in the set of all possible end states of the machine. The NFA accepts a word $w$ if and only if $\delta^*(\set{q_0}, w) \cap A \ne \emptyset$ - if any of the possible end states are accepting.

To simulate a DFA:

    state = $q_0$
    while read_char(current):
        state = $\delta$(state, current)
    return state in $A$

To simulate an NFA:

    states = set($q_0$)
    while read_char(current):
        states = set($\delta$(q, current) for q in states)
    return states & $A$ != set()

If we have an NFA and give each possible set of possible states (which we get from stepping through every possible option) a name, and use these sets as the states instead, we get a DFA. In this way it is possible to convert any NFA into a DFA. This is called **subset construction**.

Note that this means that an NFA with $n$ states can be converted into a DFA with at most $2^n$ states - one for every subset of states. We can convert an NFA to a DFA by stepping through all the possibilities for input. The accepting states of our DFA are the **sets containing accepting states in the original NFA**.

Whenever we step an NFA, we are actually simulating a DFA in a simpler way - we are constructing a DFA on the fly, with only the states that we actually need for a certain input.

# 10/2/15

Every NFA must have a corresponding DFA, and every DFA is also an NFA - the set of NFAs is a superset of the set of DFAs, and every NFA has a corresponding DFA. Therefore, NFAs can accept the same class of languages that DFAs do, the regular languages.

Consider the language $L = \set{cab} \cup \set{w \in \set{a, b, c}^* \mid w \text{ has an even number of } a}$. ;wip; draw an NFA and then the corresponding DFA, make sure to handle blank string and $aacab, bcab$ not acceptable

    1, 2, 3, 4, 5, 6
    1 > c > 2
    2 > a > 3
    3 > b > 4
    1 > a > 5
    1 > b,c > 6
    5 > b,c > 5
    5 > a > 6
    6 > a > 5
    6 > b,c > 6
    1 > accept
    6 > accept

To convert this to an NFA, we start by stepping the NFA with $\set{1}$ (a set containing only the start state), while considering every possible next input for the currently stepped states. Repeating until no new subset states are found, we get the equivalent DFA:

    {1} > a > {5}
    {1} > b > {6}
    {1} > c > {2, 6}
    {5} > b,c > {5}
    {5} > a > {6}
    {6} > a > {5}
    {6} > b,c > {6}
    {2, 6} > b,c > {6}
    {2, 6} > a > {3, 5}
    {3, 5} > a > {6}
    {3, 5} > b > {4, 5}
    {3, 5} > c > {5}
    {4, 5} > a > {6}
    {4, 5} > b, c > {5}
    {1} > accept
    {6} > accept
    {2, 6} > accept
    {3, 5} > accept
    {4, 5} > accept

We will now consider an $\epsilon$-NFA. This is like a normal NFA, but we can change states without reading any input at all. This is represented with arrows labelled with $\epsilon$. This makes many alternation cases easier, because it gives a much easier way of gluing multiple small NFAs together into a bigger NFA. Basically, the $\epsilon$ transition is an optional transition we can take from one state to another.

Using the same argument as before with states being sets of states, the $\epsilon$-NFAs also accept regular languages.

Also, $\epsilon$-NFAs can be easily converted into a DFA using a simple algorithm. Additionally, if we can prove that all regular expressions have an $\epsilon$-NFA, then we have one direction in the proof for Kleene's theorem - that every regular expression has a DFA.

The $\epsilon$-NFA for any regular expression can be found recursively:

* Empty language is a non-accepting start state.
* The regular expression $\epsilon$ is an accepting start state.
* A regular expression with only one letter is a start state that transitions to an accepting state via that letter.
* A regular expression of the form $E_1|E_2$ is a start state, with two $\epsilon$ transitions, one to the $\epsilon$-NFAs for $E_1$ and one to the $\epsilon$-NFA for $E_2$.
* A regular expression of the form $E_1 E_2$ is the $\epsilon$-NFA for $E_1$, where the accepting states are not accepting and instead transition to the start state for the $\epsilon$-NFA for $E_2$.
* A regular expression of the form $E_1^*$ is the $\epsilon$-NFA for $E_1$, where the start state is a new accepting state with an $\epsilon$-transition to the original start state of the $\epsilon$-NFA for $E_1$, and all accepting states of also have an $\epsilon$ transition back to the start state of the $\epsilon$-NFA for $E_1$.

As a result, all regular expressions can be converted into an $\epsilon$-NFA, which can be converted into a DFA.

Regular expressions and DFAs are useful in writing compilers because we need to do tokenization/scanning/lexing. For example, the lexer needs to be able to scan for keywords, identifiers, literals, operators, comments, and other C tokens, and these are all regular languages. As a result, sequences of C tokens are also regular languages, and we can use finite automata to do tokenization/scanning/lexing.

To do tokenization, we are given a string $w$, break it into words $w_1, \ldots, w_n$ such that each $w_i$ is in the language of the tokens (or give an error otherwise), and then output each $w_i$ as the tokens. If $E_1, \ldots, E_k$ are the regular expressions that match each type of token, then $(E_1 | \ldots | E_k)^*$ is a regular expression that recognizes sequences of valid tokens. If we add an action to output the token after a match of $E_1 | \ldots | E_k$, we now have a tokenizer.

Consider a regular expression for parsing a C-style identifier: $[a-zA-Z][a-zA-Z0-9]^*$. Clearly, if we parse $abcd$, this could be interpreted as either 1, 2, 3, or 4 tokens. This ambiguity is problematic because a program written in a programming language should only have one interpretation.