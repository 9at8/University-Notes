<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>MATH136 | Anthony Zhang</title>
  <style type="text/css">
  body {
    font-family: "Segoe UI", Verdana, Arial, Helvetica, sans-serif;
    background: #fffefe;
    padding: 5em;
  }
  
  pre {
    margin-left: 2em;
  }
  
  code {
    border: solid 1px black;
    background: #665555;
    color: white;
    padding: 0.1em;
    border-radius: 0.3em;
    display: inline-block;
  }
  
  pre code {
    padding: 1em;
    border-radius: 0.5em;
  }
  
  h1 {
    font-size: 4em;
  }
  
  table {
    margin: 0 auto;
  }
  
  td, th {
    padding: 0.5em;
    border: 1px solid grey;
  }
  
  tr {
    padding:: 0;
  }
  
  a.button {
    display: inline-block;
    padding: 1em;
    font-family: monospace;
    color: black;
    text-decoration: none;
    border: 0.2em solid black;
    border-radius: 0.5em;
    background: white;
  }
  
  a.button:hover, a.button:focus, a.button:active {
    background: black;
    color: white;
  }
  </style>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript">MathJax.Hub.Queue(["Typeset",MathJax.Hub]);</script>
</head>
<body>
<a class="button" href="..">&#8666; Return to University Notes index</a>
<h1 id="math136">MATH136</h1>
<p>Linear Algebra</p>
<pre><code>Instructor: Dan Wolczuk
Section 081 (online)
Email: dstwolcz@uwaterloo.ca
Office Hours: MC 4013 on Mondays, Wednesdays, Fridays 1:00pm-2:00pm, Thursdays 1:30pm-4:00pm</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l&#39;H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\]</span></p>
<h1 id="im-going-to-stop-procrastinating-this-week-honest">13/1/14 (I'm going to stop procrastinating this week, honest!)</h1>
<h2 id="notation">Notation</h2>
<p>We represent vectors as column vectors (matrices of size <span class="math">\(n \times 1\)</span>) to distinguish them from points: <span class="math">\(\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}, v_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>However, sometimes we will also write them in <span class="math">\(n\)</span>-tuple form (for example, <span class="math">\((3, 7, 2)\)</span>)</p>
<p><span class="math">\(\mb{R}^n, n \in \mathbb{Z}, n &gt; 0\)</span> is the set of all elements of the form <span class="math">\((x_1, \ldots, x_n), x_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>We usually think of these elements as points, but now we will think of them as vectors, abstract objects that we can perform operations on, like adding and subtracting.</p>
<p>This is called an <span class="math">\(n\)</span>-dimensional Euclidean space, and is the set of all vectors that are of length (dimension) <span class="math">\(n\)</span>. For example, <span class="math">\(\mb{R}^3\)</span> (arr-three) is the set of all 3D vectors, and <span class="math">\(\vec{0} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}\)</span>.</p>
<p>So <span class="math">\(\mb{R}^3 = \set{\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \middle| x_1, x_2, x_3 \in \mathbb{R}}\)</span>.</p>
<p>Likewise, <span class="math">\(\mb{R}^n = \set{\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \middle| x_1, \ldots, x_n \in \mathbb{R}}\)</span>.</p>
<p>It might also occasionally be useful to think of these vectors as points, where the vectors are offsets from the origin. For example, a function <span class="math">\(f\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\right) = f(x_1, \ldots, x_n)\)</span>.</p>
<h2 id="operations">Operations</h2>
<p>Let <span class="math">\(\vec{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}, \vec{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, c \in \mb{R}\)</span>.</p>
<h3 id="equality">Equality</h3>
<p><span class="math">\(x = y\)</span> if and only if <span class="math">\(\forall 1 \le i \le n, x_i = y_i\)</span>.</p>
<p>In other words, two vectors are equal if and only if all their components are equal. Note that this is defined only for vectors with the same size (number of components).</p>
<h3 id="additionsubtraction">Addition/Subtraction</h3>
<p><span class="math">\[x + y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} + \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{bmatrix}\]</span></p>
<p>Also, <span class="math">\(\vec{x} - \vec{y} = \vec{x} + (-\vec{y})\)</span>. Therefore, <span class="math">\(x - y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} - \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 - y_1 \\ \vdots \\ x_n - y_n \end{bmatrix}\)</span>.</p>
<h3 id="scalar-multiplication">Scalar Multiplication</h3>
<p><span class="math">\[cx = c\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} cx_1 \\ \vdots \\ cx_n \end{bmatrix}\]</span></p>
<p>If <span class="math">\(c = -1\)</span>, <span class="math">\(c\vec{x} = -x\)</span>.</p>
<h2 id="linear-combination">Linear Combination</h2>
<p>A <strong>linear combination</strong> of vectors <span class="math">\(\vec{v}_1, \ldots, \vec{v}_n\)</span> is the value <span class="math">\(L = c_1 v_1 + \ldots + c_n v_n, c_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>It is a linear expression with each term containing a coefficient and one vector value - a sum of scalar multiples.</p>
<h3 id="theorem-1.1.1">Theorem 1.1.1</h3>
<p>If <span class="math">\(\vec{x}, \vec{y}, \vec{w} \in \mathbb{R}^n, c, d \in \mathbb{R}\)</span>, then:</p>
<ul>
<li>Closure under addition: <span class="math">\(\vec{x} + \vec{y} \in \mathbb{R}^n\)</span>.</li>
<li>Associativity: <span class="math">\((\vec{x} + \vec{y}) + \vec{w} = \vec{x} + (\vec{y} + \vec{w})\)</span></li>
<li>Commutativity: <span class="math">\(\vec{x} + \vec{y} = \vec{y} + \vec{x}\)</span></li>
<li>Additive identity: <span class="math">\(\exists \vec{0} \in \mathbb{R}, \forall \vec{x} \in \mb{R}^n, \vec{x} + \vec{0} = \vec{x}\)</span> (<span class="math">\(\vec{0}\)</span> is called the zero vector).</li>
<li>Additive inverse: <span class="math">\(\forall \vec{x} \in \mathbb{R}^n, \exists -\vec{x} \in \mathbb{R}^n, \vec{x} + (-\vec{x}) = \vec{0}\)</span>.</li>
<li>Closure under scalar multiplication: <span class="math">\(c\vec{x} \in \mathbb{R}^n\)</span>.</li>
<li><span class="math">\(c(d\vec{x}) = (cd)\vec{x}\)</span>.</li>
<li>Scalar distributivity: <span class="math">\((c + d)\vec{x} = c\vec{x} + d\vec{x}\)</span>.</li>
<li>Vector distributivity: <span class="math">\(c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}\)</span>.</li>
<li>Multiplicative identity: <span class="math">\(1\vec{x} = \vec{x}\)</span>.</li>
</ul>
<p>Closure under addition means that when we add two elements in the set, the resulting sum is also in the set.</p>
<p>Closure under scalar multiplication means that when we multiply an element in the set by a scalar, the resulting product is also in the set.</p>
<p>Because these Euclidean spaces are closed under addition and scalar multiplication, they are closed under linear combinations - all linear combinations of vectors in <span class="math">\(\mb{R}^n\)</span> are also in <span class="math">\(\mb{R}^n\)</span>.</p>
<h2 id="span">Span</h2>
<p>The <strong>span</strong> of a set of vectors <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> is the set of all linear combinations of the vectors in the set.</p>
<p>The span of an empty set is the set containing only the zero vector - <span class="math">\(\spn \emptyset = \set{\vec{0}}\)</span>.</p>
<p>In other words, <span class="math">\(\spn \mathcal{B} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k} = \set{c_1 v_1 + \ldots + c_n v_n \middle| c_1, \ldots, c_n \in \mathbb{R}}\)</span>.</p>
<p>A span is always a subset of the space it is in. In other words, <span class="math">\(\spn \mathcal{B} \subseteq \mb{R}^n\)</span>.</p>
<p>A span is just a set. Therefore, we can say that a set <span class="math">\(\mathcal{B}\)</span> <strong>spans</strong> another set <span class="math">\(\mathcal{C}\)</span> if and only if <span class="math">\(\mathcal{C}\)</span> is exactly the set of all linear combinations of the vectors in <span class="math">\(\mathcal{B}\)</span>.</p>
<p>The <strong>vector equation</strong> of a set of vectors is the generalized equation for the linear combinations of each element in the set. For example, <span class="math">\(\vec{x} = c_1 v_1 + \ldots + c_n v_n\)</span> is the vector equation for <span class="math">\(\mathcal{B}\)</span>.</p>
<h3 id="theorem-1.1.2">Theorem 1.1.2</h3>
<p>If <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>, then <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
<p>In other words, if a vector in a set of vectors can already be represented by a linear combination of other vectors in the set, it doesn't affect the span at all.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>.<br />So <span class="math">\(\exists c_1, \ldots, c_k \in \mathbb{R}, \vec{v}_{k + 1} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k\)</span>.<br />We want to show that <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.<br />Let <span class="math">\(\vec{x}\)</span> be an arbitrary element of <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}\)</span>.<br />So <span class="math">\(\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} \vec{v}_{k + 1}\)</span>.<br />So <span class="math">\(\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} (c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k) = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = (d_1 + d_{k + 1} c_1) \vec{v}_1 + \ldots + (d_k + d_{k + 1} c_k) \vec{v}_k\)</span>.<br />Clearly, <span class="math">\(x \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.<br />Clearly, <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}\)</span>.<br />Therefore, <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
</blockquote>
<p>Consider <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}}\)</span>. Clearly, <span class="math">\(\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}\)</span>.</p>
<p>Clearly, <span class="math">\(\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}\)</span>, so <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}} = \spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}}\)</span> represents a line in 3D.</p>
<p>The span of the zero vector is itself. The span of a linearly independent vector is a line. The span of two linearly independent vectors is a plane. The span of 3 linearly independent vectors is a 3D space. The span of 4 linearly independent vectors is a 4D space, and so on.</p>
<h3 id="linear-independence">Linear Independence</h3>
<p>When we say <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>, we can also write <span class="math">\(\vec{v}_{k + 1} \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
<p>Note that <span class="math">\(\vec{0}\)</span> can always be written as a linear combination of any vectors in the same space. The zero vector exists in all dimensions.</p>
<p>It is important to determine which vectors can be written as linear combinations of others.</p>
<p>A set of vectors is <strong>linearly independent</strong> if and only if the vectors in the set cannot be written as linear combinations of each other. Otherwise, the set is <strong>linearly dependent</strong>.</p>
<p>In other words, <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = c_k \vec{v}_k\)</span>. Rearranging, we get <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} - c_k \vec{v}_k + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = \vec{0}\)</span>.</p>
<h3 id="theorem-1.1.3">Theorem 1.1.3</h3>
<p>Therefore, a set of vectors <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> is linearly dependent if and only if <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_n vec{v}_n = \vec{0}\)</span>, and otherwise linearly independent.</p>
<p>The solution where <span class="math">\(c_i = 0, 1 \le i \le n\)</span> is called the trivial solution, because all the coefficients are 0. The trivial solution is always a solution to the above equation.</p>
<p>In other words, the set is linearly independent if and only if the zero vector cannot be written as a linear combination of the vectors in the set, except when all the coefficients are 0.</p>
<p>To prove that a set is linearly independent, we need to prove that the only solution to the above equation is the trivial solution.</p>
<p>To prove that a set is linearly independent, we need to prove that there is a solution to the above equation where at least one of the coefficients is non-aero.</p>
<p>When we find a solution to the equation, we can use this to find the vectors that are in the span of all the others - the vectors that can be written as a linear comnbination of the others. In the solution, <strong>all vectors with non-zero coefficients can be written as linear combinations of all the other vectors</strong>. If we rearrange the solved equation, we can find this linear combination.</p>
<h3 id="theorem-1.1.4">Theorem 1.1.4</h3>
<p>Note that any set of vectors that contains <span class="math">\(\vec{0}\)</span> is linearly dependent.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> and <span class="math">\(\vec{v}_i = \vec{0}, 1 \le i \le n\)</span>.<br /><span class="math">\(\mathcal{B}\)</span> is linearly dependent if and only if <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0} \wedge \exists 1 \le i \le n, c_i \ne 0\)</span>.<br />Construct <span class="math">\(c_1, \ldots, c_{i - 1}, c_{i + 1}, \ldots, c_n = 0, c_i = 1\)</span>.<br />Then <span class="math">\(c_i \ne 0\)</span> and <span class="math">\(c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0}\)</span>.<br />Therefore, <span class="math">\(\mathcal{B}\)</span> is linearly dependent.</p>
</blockquote>
<p>The vector equation of a linearly independent set is known as a simplified vector equation. A vector equation can be converted into a simplified one by removing terms where the vector can be written as a linear combination of the others.</p>
<p>Geometrically, two vectors span a plane if they are not parallel, and three vectors span a 3D space if they do not all lie on the same plane.</p>
<h2 id="bases">Bases</h2>
<p>A linearly independent set of vectors is always simpler than a linearly dependent one. A linearly dependent set of vectors can always be converted into a linearly dependent one with the same span. Therefore, the simplest set of vectors that spans a given set is always linearly independent.</p>
<p>Given <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span>, <span class="math">\(\mathcal{B}\)</span> is a <strong>basis</strong> for <span class="math">\(\spn \mathcal{B}\)</span> if <span class="math">\(\mathcal{B}\)</span> is linearly independent.</p>
<p>In other words, a basis for a set is a linearly independent set of vectors such that its span is exactly the set.</p>
<p>The basis for <span class="math">\(\set{\vec{0}}\)</span> is the empty set (<span class="math">\(\emptyset\)</span>).</p>
<p>The <strong>standard basis</strong> is a basis that is easy to write for any number of dimensions. The standard basis is of the form <span class="math">\(\set{\vec{e}_1, \ldots, \vec{e}_n}\)</span>, where <span class="math">\(\vec{e}_i\)</span> has all components set to 0, except for the <span class="math">\(i\)</span>-th component, for <span class="math">\(1 \le i \le n\)</span>.</p>
<p>For example, the standard basis of <span class="math">\(\mb{R}^4\)</span> is <span class="math">\(\set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}}\)</span>.</p>
<p>In order to prove that a set is the basis of another, we need to prove that it is linearly independent, and that it spans the set (by proving that an arbitrary vector in the second set can be written as a linear combination of the elements of the first).</p>
<p>Prove that <span class="math">\(\mathcal{B} = \set{\begin{bmatrix} 1 \\ 3 \end{bmatrix}, \begin{bmatrix} -1 \\ -1 \end{bmatrix}}\)</span> is a basis for <span class="math">\(\mb{R}^2\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2\)</span>.<br />We will find the coefficients we want by solving <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span> for <span class="math">\(c_1, c_2\)</span>.<br />Construct <span class="math">\(c_1 = \frac{x_2 - x_1}{2}, c_2 = \frac{x_2 - 3x_1}{2}\)</span>.<br />Clearly, <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \frac{x_2 - x_1}{2} \begin{bmatrix} 1 \\ 3 \end{bmatrix} + \frac{x_2 - 3x_1}{2} \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\exists c_1, c_2 \in \mathbb{R}, \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\mathcal{B}\)</span> spans <span class="math">\(\mb{R}^2\)</span>.<br />Note that there is only the trivial solution when <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span>, so <span class="math">\(\mathcal{B}\)</span> is also linearly independent.<br />Therefore, <span class="math">\(\mathcal{B}\)</span> is a basis for <span class="math">\(\mb{R}^2\)</span>.</p>
</blockquote>
<p>A basis for <span class="math">\(\mb{R}^n\)</span> has exactly <span class="math">\(n\)</span> elements.</p>
<h2 id="shapes">Shapes</h2>
<p>Points are conceptually applicable in any dimension above 0.</p>
<p>Lines are conceptually applicable in any dimension above 1.</p>
<p>Planes are conceptually applicable in any dimension above 2.</p>
<p>A line in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + \vec{b}, c_1 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \vec{b} \in \mathbb{R}^n\)</span>. This is a line in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>.</p>
<p>A plane in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + c_2 \vec{v}_2 + \vec{b}, c_1, c_2 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \vec{v}_2, \vec{b} \in \mathbb{R}^n\)</span> and <span class="math">\(\set{\vec{v}_1, \vec{v}_2}\)</span> being a linearly independent set. This is a plane in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>.</p>
<p>A <span class="math">\(k\)</span>-plane in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k + \vec{b}, c_1, c_2 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k, \vec{b} \in \mathbb{R}^n\)</span> and <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> being a linearly independent set. This is a <span class="math">\(k\)</span>-plane in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>. A <span class="math">\(k\)</span>-plane is a <span class="math">\(k\)</span>-dimensional plane.</p>
<p>A hyperplane is a <span class="math">\(k\)</span>-plane, where <span class="math">\(k = n - 1\)</span>. It is an <span class="math">\(n - 1\)</span>-dimensional plane.</p>
<p>For example, <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ −2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 1 \\ −1 \end{bmatrix}}\)</span> defines a hyperplane in <span class="math">\(\mb{R}^n\)</span>, since the set is linearly independent.</p>
<h1 id="ok-so-its-like-week-3-and-im-behind-i-get-it">20/1/14 (OK, so it's like week 3 and I'm behind, I get it)</h1>
<h2 id="subspaces">Subspaces</h2>
<p>A <strong>subspace</strong> of <span class="math">\(\mb{R}^n\)</span> is a non-empty subset <span class="math">\(\mb{S}\)</span> of <span class="math">\(\mb{R}^n\)</span> such that it satisfies all ten properties defined in Theorem 1.1.1.</p>
<p>All subspaces are <strong>spaces</strong>, which are subsets of a vector space.</p>
<p>But since properties 2-5 and 7-10 follow from properties 1 and 6, all we need to do to prove all ten properties is to prove properties 1 and 6 hold.</p>
<h3 id="theorem-1.2.1-subspace-test">Theorem 1.2.1 (Subspace Test)</h3>
<p>Given a set <span class="math">\(\mb{S}\)</span>, <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span> if and only if:</p>
<ul>
<li><span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{R}^n\)</span>.</li>
<li><span class="math">\(\mb{S}\)</span> is non-empty.</li>
<li><span class="math">\(\forall \vec{x}, \vec{y} \in \mb{S}, \vec{x} + \vec{y} \in \mb{S}\)</span>.</li>
<li><span class="math">\(\forall \vec{x} \in \mb{S}, c \in \mb{R}, c\vec{x} \in \mb{S}\)</span></li>
</ul>
<p>To prove a set is a subspace, we need to prove all four properties.</p>
<p>Clearly, if conditions 2 and 4 hold, <span class="math">\(\vec{0} \in \mb{S}\)</span>. So if <span class="math">\(\vec{0} \notin \mb{S}\)</span>, then one or both of the conditions is not met and the set is not a subspace.</p>
<p>We can use this to check if a set is a subspace by seeing if <span class="math">\(\vec{0}\)</span> is in it.</p>
<p>Prove <span class="math">\(\mb{S} = \set{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} \middle| x + y = 0, x - z = 0}\)</span> is a subspace of <span class="math">\(\mb{R}^3\)</span> and write a basis for the subspace:</p>
<blockquote>
<p>Clearly, <span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{R}^3\)</span>.<br />Clearly, <span class="math">\(\vec{0} \in \mb{S}\)</span>, so the set is non-empty.<br />Let <span class="math">\(\vec{a}, \vec{b} \in \mb{S}\)</span>.<br />So <span class="math">\(a_1 + a_2 = a_1 - a_3 = 0\)</span> and <span class="math">\(b_1 + b_2 = b_1 - b_3 = 0\)</span>.<br />Clearly, <span class="math">\((a_1 + b_1) + (a_2 + b_2) = (a_1 + b_1) - (a_3 + b_3) = 0\)</span>.<br />So <span class="math">\(\vec{a} + \vec{b} \in \mb{S}\)</span> and <span class="math">\(\mb{S}\)</span> is closed under addition.<br />Clearly, <span class="math">\(c(a_1 + a_2) = c(a_1 - a_3) = c0\)</span>.<br />So <span class="math">\(c\vec{a} \in \mb{S}\)</span> and <span class="math">\(\mb{S}\)</span> is closed under scalar multiplication.<br />So <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^3\)</span>.<br />Since <span class="math">\(a_1 + a_2 = a_1 - a_3 = 0\)</span>, <span class="math">\(a_2 = -a_1\)</span> and <span class="math">\(a_3 = a_1\)</span>.<br />So the general vector is <span class="math">\(\vec{v}(a_1) = \begin{bmatrix} a_1 \\ -a_1 \\ a_1 \end{bmatrix} = a_1 \begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}\)</span>.<br />So a basis for <span class="math">\(\mb{S}\)</span> is <span class="math">\(\set{\begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}}\)</span>.</p>
</blockquote>
<p>We find a basis of a subspace by finding the general form of a vector in the subspace, using this to find a spanning set for <span class="math">\(S\)</span>, and then reducing it into a linearly independent set.</p>
<h3 id="theorem-1.2.2">Theorem 1.2.2</h3>
<p>If <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k \in \mb{R}^n\)</span>, then <span class="math">\(\mb{S} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, the set is a subset of <span class="math">\(\mb{R}^n\)</span>.<br />Clearly, <span class="math">\(\vec{0} \in \mb{S}\)</span>, since <span class="math">\(\vec{0} = 0\vec{v}_1 + \ldots + 0\vec{v}_k\)</span>.<br />Let <span class="math">\(\vec{a}, \vec{b} \in \mb{S}, w \in \mb{R}\)</span>.<br />Then <span class="math">\(\exists c_1, \ldots, c_k \in \mb{R}, \vec{a} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k\)</span> and <span class="math">\(\exists d_1, \ldots, d_k \in \mb{R}, \vec{b} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k\)</span>.<br />So <span class="math">\(\vec{a} + \vec{b} = (c_1 + d_1) \vec{v}_1 + \ldots + (c_k + d_k) \vec{v}_k\)</span>, and <span class="math">\(\vec{a} + \vec{b} \in \mb{S}\)</span>.<br />So <span class="math">\(w\vec{a} = wc_1 \vec{v}_1 + \ldots + wc_k \vec{v}_k\)</span> and <span class="math">\(w\vec{a} \in \mb{S}\)</span>.<br />So <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
</blockquote>
<h2 id="dot-product">Dot Product</h2>
The <strong>dot product</strong> of two vectors <span class="math">\(\vec{a}\)</span> and <span class="math">\(\vec{b}\)</span> is defined as $  =

<p>\begin{bmatrix} b_1</p>
<p>The dot product also has the geometric interpretation <span class="math">\(\vec{a} \cdot \vec{b} = \abs{a} \abs{b} \cos \theta\)</span>, where <span class="math">\(\theta\)</span> is the angle between the two vectors.</p>
<p>The dot product is also known as the <strong>scalar product</strong> or <strong>standard inner product</strong> of <span class="math">\(\mb{R}^n\)</span>.</p>
<h3 id="theorem-1.3.2">Theorem 1.3.2</h3>
<p>If <span class="math">\(\vec{x}, \vec{y}, \vec{z} \in \mb{R}^n, s, t, \in \mb{R}\)</span>, then:</p>
<ul>
<li><span class="math">\(\vec{x} \cdot \vec{x} \ge 0\)</span>, and <span class="math">\(\vec{x} \cdot \vec{x} = 0 \iff \vec{x} = \vec{0}\)</span>.</li>
<li><span class="math">\(\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}\)</span>.</li>
<li><span class="math">\(\vec{x} \cdot (s\vec{y} + t\vec{z}) = s (\vec{x} \cdot \vec{y}) + t (\vec{x} \cdot \vec{z})\)</span>.</li>
</ul>
<p>The <strong>length</strong> or <strong>norm</strong> of a vector <span class="math">\(\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_k \end{bmatrix}\)</span> is <span class="math">\(\magn{\vec{v}} = \sqrt{\sum_{i = 1}^k v_i^2}\)</span>. Note the similarity to scalars, where <span class="math">\(\abs{x} = \sqrt{x^2}\)</span>.</p>
<p>A vector of length 1 is a unit vector. A unit vector is therefore one such that <span class="math">\(\sum_{i = 1}^k v_i^2 = 1\)</span>.</p>
<p>Also, <span class="math">\(\vec{x} \cdot \vec{x} = \magn{x}^2\)</span>, which should be obvious since <span class="math">\(\theta = 0\)</span> and <span class="math">\(\cos \theta = 1\)</span>, so <span class="math">\(\vec{x} \cdot \vec{x} = \magn{x} \magn{x} = \magn{x}^2\)</span>.</p>
<h3 id="theorem-1.3.3">Theorem 1.3.3</h3>
<p>If <span class="math">\(\vec{x}, \vec{y} \in \mb{R}^n, c \in \vec{R}\)</span>, then:</p>
<ul>
<li><span class="math">\(\magn{\vec{x}} \ge 0\)</span>, and <span class="math">\(\magn{\vec{x}} = 0 \iff \vec{x} = \vec{0}\)</span></li>
<li><span class="math">\(\magn{cx} = \abs{c}\magn{\vec{x}}\)</span></li>
<li><span class="math">\(\abs{\vec{x} \cdot \vec{y}} \le \magn{\vec{x}}\magn{\vec{y}}\)</span> (Cauchy-Schwarz-Buniakowski Inequality)</li>
<li><span class="math">\(\magn{\vec{x} + \vec{y}} \le \magn{\vec{x}} + \magn{\vec{y}}\)</span> (Triangle Inequality)</li>
</ul>
<p>Proof of third consequence:</p>
<blockquote>
<p>Clearly, if <span class="math">\(\vec{x} = \vec{0}\)</span> then the inequality holds. Assume <span class="math">\(\vec{x} \ne \vec{0}\)</span>.<br />Clearly, <span class="math">\(\forall t \in \mb{R}, (t\vec{x} + \vec{y}) \cdot (t\vec{x} + \vec{y}) \ge 0\)</span> (from properties of dot product), so <span class="math">\(t^2(\vec{x} \cdot \vec{x}) + 2t(\vec{x} \cdot \vec{y}) + \vec{y} \cdot \vec{y} \ge 0\)</span>. Clearly, this is a quadratic polynomial where <span class="math">\(t\)</span> is the variable. The polynomial is greater or equal to 0 if and only if it has at most 1 root.<br />So the discriminant in the quadratic formula, <span class="math">\(b^2 - 4ac\)</span>, must be less or equal to 0.<br />So <span class="math">\(4(\vec{x} \cdot \vec{y})^2 - 4(\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y}) \le 0\)</span>.<br />So <span class="math">\((\vec{x} \cdot \vec{y})^2 \le (\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y})\)</span> and <span class="math">\((\vec{x} \cdot \vec{y})^2 \le \magn{x}^2\magn{y}^2\)</span>.<br />So <span class="math">\(\vec{x} \cdot \vec{y} \le \magn{x}\magn{y}\)</span>.</p>
</blockquote>
<h3 id="angle">Angle</h3>
<p>The <strong>angle</strong> between two vectors <span class="math">\(\vec{x}\)</span> and <span class="math">\(\vec{y}\)</span> is defined as the angle <span class="math">\(\theta\)</span> such that <span class="math">\(\vec{x} \cdot \vec{y} = \magn{x} \magn{y} \cos \theta\)</span>.</p>
<p>In other words, <span class="math">\(\theta = \arccos\left(\frac{\vec{x} \cdot \vec{y}}{\magn{x} \magn{y}}\right)\)</span>.</p>
<p>Two vectors are <strong>orthogonal</strong> if the angle between them is 90 degrees - if their dot product is 0.</p>
<p>One of the reasons the standard bases are easy to work with is specifically because they are orthogonal to each other and because they are unit vectors.</p>
<h3 id="planeshyperplanes">Planes/Hyperplanes</h3>
<p>We can use the dot product to derive a <strong>scalar equation</strong> form for a plane, that makes use of the fact that the normal of a plane is just a vector.</p>
<p>Let <span class="math">\(A(a_1, a_2, a_3)\)</span> be a fixed point on the plane. Let <span class="math">\(X(x_1, x_2, x_3)\)</span> be an arbitrary point on the plane.</p>
<p>Then <span class="math">\(\vec{X} - \vec{A}\)</span> is a vector that lies on the plane.</p>
<p>In other words, if <span class="math">\(\vec{x} = s\vec{u} + t\vec{v}, s, t \in \mb{R}\)</span>, then <span class="math">\(\vec{x} - \vec{b}\)</span> is clearly a vector that lies on the plane.</p>
<p>Let <span class="math">\(\vec{n}\)</span> be a vector normal to the plane. Clearly, <span class="math">\(\vec{n} \cdot (\vec{X} - \vec{A}) = 0\)</span>.</p>
<p>So <span class="math">\(n_1 (x_1 - a_1) + n_2 (x_2 - a_2) + n_3 (x_3 - a_3) = 0 = n_1 x_1 + n_2 x_2 + n_3 x_3 - n_1 a_1 - n_2 a_2 - n_3 a_3\)</span>.</p>
<p>So <span class="math">\(n_1 x_1 + n_2 x_2 + n_3 x_3 = n_1 a_1 + n_2 a_2 + n_3 a_3\)</span>. Since <span class="math">\(\vec{A}\)</span> is fixed, <span class="math">\(n_1 a_1 + n_2 a_2 + n_3 a_3\)</span> is constant and the equation is a function of <span class="math">\(x_1, x_2, x_3\)</span>.</p>
<p>Since <span class="math">\(\vec{X}\)</span> is arbitrary, this holds for every point on the plane, and so is an equation of the plane.</p>
<p>Using the above, we can find the scalar equation of any plane given the normal and a fixed point on the plane.</p>
<p>If we extend this to hyperplanes, ;wip</p>
<h3 id="cross-product">Cross Product</h3>
<p>However, we will typically be given the vector equation of the plane, like <span class="math">\(\vec{x} = c_1 \vec{u} + c_2 \vec{v} + \vec{w}\)</span>.</p>
<p>We can calculate the normal of the plane by finding two linearly independent vectors that lie on the plane, and finding a vector that is orthogonal to both of them.</p>
<p>Clearly, <span class="math">\(\vec{u}\)</span> and <span class="math">\(\vec{v}\)</span> lie on the plane, being the solutions when <span class="math">\(c_2 = 0\)</span> or <span class="math">\(c_1 = 0\)</span>, respectively.</p>
<p>Then for the normal <span class="math">\(\vec{n}\)</span> we know <span class="math">\(\vec{n} \cdot \vec{u} = \vec{n} \cdot \vec{v} = 0\)</span>. So <span class="math">\(n_1 u_1 + n_2 u_2 + n_3 u_3 = n_1 v_1 + n_2 v_2 + n_3 v_3 = 0\)</span>.</p>
<p>Solving two equations for three unknowns, we find that one possible solution is <span class="math">\(\vec{n} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}\)</span>.</p>
<p>This problem is so common that we gave its solution a name, the <strong>cross product</strong>. The cross product of two vectors <span class="math">\(\vec{u}\)</span> and <span class="math">\(\vec{v}\)</span> is <span class="math">\(\vec{u} \times \vec{v} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}\)</span>, and is always a vector that is orthogonal to both vectors.</p>
<p>Note that this operation is only defined in <span class="math">\(\mb{R}^3\)</span> and <span class="math">\(\mb{R}^7\)</span>.</p>
<h2 id="projections">Projections</h2>
<p>Given the vectors <span class="math">\(\vec{u}, \vec{v}, \vec{w} \in \mb{R}^n, \vec{v} \ne \vec{0}, \vec{v} \cdot \vec{w} = 0\)</span>, we want to write <span class="math">\(\vec{u}\)</span> as the sum of a scalar multiple of <span class="math">\(\vec{v}\)</span> and <span class="math">\(\vec{w}\)</span>, as the vector <span class="math">\(\vec{u} = c\vec{v} + \vec{w}, c \in \mb{R}\)</span>.</p>
<p>We first need to find out how much of <span class="math">\(\vec{u}\)</span> is in the direction of <span class="math">\(\vec{v}\)</span> - find <span class="math">\(c\)</span>. Clearly, <span class="math">\(\vec{u} \cdot \vec{v} = (c\vec{v} + \vec{w}) \cdot \vec{v} = c\magn{v}^2 + \vec{w} \cdot \vec{v} = c\magn{v}^2\)</span>.</p>
<p>So <span class="math">\(c = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}\)</span>.</p>
<p>The <strong>projection</strong> of <span class="math">\(\vec{u}\)</span> onto <span class="math">\(\vec{v}\)</span> is defined as <span class="math">\(\proj_{\vec{v}} \vec{u} = c\vec{v} = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}\vec{v}\)</span>, and is the vector along the same direction as <span class="math">\(\vec{v}\)</span> such that it has the same extent along <span class="math">\(\vec{v}\)</span> as <span class="math">\(\vec{u}\)</span>.</p>
<p>The <strong>perpendicular</strong> of <span class="math">\(\vec{u}\)</span> onto <span class="math">\(\vec{v}\)</span> is the vector that when added to the projection, results in <span class="math">\(\vec{u}\)</span>. Therefore, the perpendicular is <span class="math">\(\prp_{\vec{v}} \vec{u} = \vec{u} - \proj_{\vec{v}} \vec{u}\)</span>.</p>
<h3 id="planes">Planes</h3>
<p>How do we project a vector onto a plane? We can notice that the projection of a vector onto a plane is the perpendicular of the vector projected onto the normal of the plane.</p>
<p>Therefore, the projection of a vector <span class="math">\(\vec{v}\)</span> onto a plane with normal <span class="math">\(\vec{n}\)</span> is <span class="math">\(\prp_{\vec{n}} \vec{v}\)</span>.</p>
<h2 id="systems-of-linear-equations">Systems of Linear Equations</h2>
<p>A <strong>linear equation</strong> is an equation of the form <span class="math">\(a_1 x_1 + \ldots + a_n x_n = b, a_k, b \in \mb{C}, 1 \le k \le n, n \in \mb{N}\)</span>. This also includes equations that can be rewritten into this form.</p>
<p>A set of linear equations with the same variables <span class="math">\(x_1, \ldots, x_n\)</span> (including those where there are zero coefficients) is called a <strong>system of linear equations</strong>.</p>
<p>A system of linear equations can be written as <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span>. Here, <span class="math">\(a_{i, j}\)</span> represents the <span class="math">\(j\)</span>th coefficient in the <span class="math">\(i\)</span>th equation.</p>
<p>A <strong>solution</strong> to a system of linear equation is a vector <span class="math">\(\vec{s} = \begin{bmatrix} s_1 \\ \vdots \\ s_n \end{bmatrix}, \vec{s} \in \mb{R}^n\)</span> such that if <span class="math">\((\forall 1 \le i \le n, x_i = s_i)\)</span>, all the equations in the system are satisfied.</p>
<p>A system of linear equations is <strong>consistent</strong> if it has at least one solution, and <strong>inconsistent</strong> otherwise.</p>
<p>Two systems of linear equations are <strong>equivalent</strong> if and only if they both have the same solution set.</p>
<p>Interpreted geometrically, a system of <span class="math">\(m\)</span> linear equations with <span class="math">\(n\)</span> variables is a set of <span class="math">\(m\)</span> hyperplanes in <span class="math">\(\mb{R}^n\)</span>. The solution to this system is represented by a vector that lies on all these hyperplanes.</p>
<h3 id="theorem-2.1.1">Theorem 2.1.1</h3>
<p>If a set of linear equations is consistent with more than 1 solution, then it has infinite solutions.</p>
<p>In other words, if <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> has solutions <span class="math">\(\vec{s}\)</span> and <span class="math">\(\vec{t}\)</span> such that <span class="math">\(\vec{s} \ne \vec{t}\)</span>, then <span class="math">\(\vec{s} + c(\vec{s} - \vec{t})\)</span> is a solution for all <span class="math">\(c \in \mb{R}\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> has solutions <span class="math">\(\vec{s}\)</span> and <span class="math">\(\vec{t}\)</span>.<br />Clearly, for all <span class="math">\(1 \le i \le n\)</span>, <span class="math">\(a_{i, 1} (s_1 + c(s_1 - t_1)) + \ldots + a_{i, n} (s_n + c(s_n - t_n)) = (a_{i, 1} s_1 + \ldots + a_{i, n} s_n) + c(a_{i, 1} s_1 + \ldots + a_{i, n} s_n) - c(a_{i, 1} t_1 + \ldots + a_{i, n} t_n) = b_i + cb_i - cb_i = b_i\)</span>.<br />So each equation is satisfied, and the system is consistent.</p>
</blockquote>
<p>The set of all solutions to a system of linear equations is known as a <strong>solution set</strong>.</p>
<h3 id="solving">Solving</h3>
<p>When we use the substitution and elimination techniques for solving systems of linear equations, we are forming new systems of lienar equations that have the same solution set as the original. We aim to obtain one that is easier to find the solution set for, and therefore solve the original system.</p>
<p>Note that when we solve a linear system, we don't really need the <span class="math">\(x\)</span> variables. Instead, we could write the system <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> more concisely as <span class="math">\(\left[\begin{array}{ccc|c} a_{1, 1} &amp; \ldots &amp; a_{1, n} &amp; b_1 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} &amp; b_1 \\ \end{array}\right]\)</span>.</p>
<p>This is called the <strong>augmented matrix</strong> of the system of linear equations.</p>
<p>The <strong>coefficient matrix</strong> of the system of linear equations is the same thing, but without the last column containing the constant value. In this case, it would be <span class="math">\(\left[\begin{array}{ccc} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \\ \end{array}\right]\)</span>.</p>
<p>We can combine a coefficient matrix <span class="math">\(A\)</span> and a constant vector <span class="math">\(\vec{b}\)</span> into an augmented matrix by using the <span class="math">\(\left[A \middle| \vec{b}\right]\)</span>. This simply adds <span class="math">\(\vec{b}\)</span> as a column to the end of <span class="math">\(A\)</span>.</p>
<p>Solve <span class="math">\(\begin{cases} 2x_1 + 3x_2 &amp;= 11 \\ 3x_1 + 6x_2 &amp;= 7 \\ \end{cases}\)</span>:</p>
<blockquote>
<p>We multiply the first equation by <span class="math">\(-3\)</span> to obtain <span class="math">\(-6x_1 - 9x_2 = -33\)</span>.<br />We multiply the second equation by <span class="math">\(2\)</span> to obtain <span class="math">\(6x_1 + 12x_2 = 14\)</span>.<br />Now we add the first equation to the second equation to obtain <span class="math">\(0x_1 + 3x_2 = -19\)</span>.<br />Now we multiply the second equation by <span class="math">\(\frac{1}{3}\)</span> to obtain <span class="math">\(0x_1 + x_2 = -\frac{19}{3}\)</span>.<br />Now we add the second equation, multiplied by <span class="math">\(9\)</span>, to the first equation to obtain <span class="math">\(-6x_1 + 0x_2 = -90\)</span>.<br />Now we multiply the first equation by <span class="math">\(-6\)</span> to obtain <span class="math">\(x_1 + 0x_2 = 15\)</span>.<br />So <span class="math">\(x_1 = 15\)</span> and <span class="math">\(x_2 = -\frac{19}{3}\)</span>.</p>
</blockquote>
<p>Now solve using operations on the matrix form of the equations, <span class="math">\(\left[\begin{array}{cc|c} 2 &amp; 3 &amp; 11 \\ 3 &amp; 6 &amp; 7 \\ \end{array}\right]\)</span>:</p>
<blockquote>
<p>We multiply the first row by <span class="math">\(-3\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 3 &amp; 6 &amp; 7 \\ \end{array}\right]\)</span>.<br />We multiply the second row by <span class="math">\(2\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 6 &amp; 12 &amp; 14 \\ \end{array}\right]\)</span>.<br />We add the first to the seocnd to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 3 &amp; -19 \\ \end{array}\right]\)</span>.<br />We multiply the second equation by <span class="math">\(\frac{1}{3}\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />Now we add the second equation multiplied by 9 to the first equation to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; 0 &amp; -90 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />Now we multiply the first equation by <span class="math">\(-\frac{1}{6}\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} 1 &amp; 0 &amp; 15 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />So <span class="math">\(x_1 = 15\)</span> and <span class="math">\(x_2 = -\frac{19}{3}\)</span>.</p>
</blockquote>
<p>Note that at every step, we had a system of linear equations that had the same solution set as the original system. Eventually, we reduced the matrix down into a form that we could easily read the values off of.</p>
<p>Also note that there were only really two different operations that we used to solve the system. To make it easier for computers to work with, we define an additional swapping operation:</p>
<ul>
<li>Multiplying a row by a non-zero scalar (<span class="math">\(cR_i\)</span> multiplies the <span class="math">\(i\)</span>th row by <span class="math">\(c \in \mb{R}, c \ne 0\)</span>).</li>
<li>Adding a multiple of one row to another (<span class="math">\(R_i + cR_j\)</span> adds the <span class="math">\(j\)</span>th row multiplied by <span class="math">\(c \in \mb{R}, c \ne 0, i \ne j\)</span> to the <span class="math">\(i\)</span>th row).</li>
<li>Swapping two rows (<span class="math">\(R_i \leftrightarrow R_j, i \ne j\)</span>).</li>
</ul>
<p>These operations we call the <strong>elementary row operations</strong> (EROs). Note that they are also fully reversible - all EROs have an inverse that undoes the effect of the operation. This is trivial to prove.</p>
<p>Two matrices are <strong>row equivalent</strong> if one can be transformed into another by application of EROs. Since EROs are reversible, if a matrix <span class="math">\(A\)</span> is row equivalent to <span class="math">\(B\)</span>, then <span class="math">\(B\)</span> is also transformable into <span class="math">\(A\)</span> via the inverse of those EROs, and so <span class="math">\(B\)</span> is row equivalent to <span class="math">\(A\)</span>. In other words, row equivalence is commutative.</p>
<p>A matrix of size <span class="math">\(m \times n\)</span> (&quot;<span class="math">\(m\)</span> by <span class="math">\(n\)</span>&quot;) has <span class="math">\(m\)</span> rows and <span class="math">\(n\)</span> columns. With <span class="math">\(x_{i, j}\)</span>, rows are indexed by <span class="math">\(i\)</span>, and columns by <span class="math">\(j\)</span>. This is called row-major ordering.</p>
<h3 id="row-reduced-echelon-form">Row Reduced Echelon Form</h3>
<p>A matrix is in <strong>reduced row echelon form/row canonical form</strong> (RREF) if and only if:</p>
<ol style="list-style-type: decimal">
<li>All rows with only zeroes are at the bottom. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 2 \end{bmatrix}\)</span>.</li>
<li>The first non-zero entry in each row is 1 (this is called the <strong>leading one</strong>). For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 4 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}\)</span>.</li>
<li>The leading one in each row, if it exists, appears to the right of the leading one of the row above it. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 1 &amp; 0 \end{bmatrix}\)</span>.</li>
<li>The leading one in each row must be the only non-zero entry in its column. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 1 &amp; 3 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}\)</span>.</li>
</ol>
<p>For augmented matrices, we check only the values in all but the last column when seeing if it is in RREF (the values on the left of the vertical line). In other words, we ignore the last column when checking for RREF form.</p>
<p>All matrices are representable in RREF. All matrices have one and only one representation in RREF.</p>
<h3 id="theorem-2.2.2">Theorem 2.2.2</h3>
<p>The RREF of a matrix is unique. In other words, each matrix has at most one possible matrix that it is both row equivalent to, and is in RREF.</p>
<p>Proof:</p>
<blockquote>
<p>;wip: something about proving this in chapter 4</p>
</blockquote>
<p>Row reduce <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \end{array}\right]\)</span>:</p>
<blockquote>
<p><span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\right]\)</span> is row equivalent to <span class="math">\(\left[\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\right]\)</span> via <span class="math">\(R_2 + (-2)R_3\)</span>.<br />This is row equivalent to <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ \end{array}\right]\)</span> via <span class="math">\(R_2 \leftrightarrow R_3\)</span>.<br />This is row equivalent to <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\right]\)</span> via <span class="math">\((-1)R_3\)</span>.<br />This is row equivalent to <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 1 &amp; 16 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\right]\)</span> via <span class="math">\(R_2 - R_1\)</span>.<br />This is row equivalent to <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\right]\)</span> via <span class="math">\(R_2 - R_3\)</span>.<br />This is row equivalent to <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 11 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\right]\)</span> via <span class="math">\(R_2 - R_3\)</span>.<br />This is in RREF.</p>
</blockquote>
<p>Note the general technique used in reducing the matrix. First, we want to obtain a triangle of 0 elements with leading ones diagonally rightwards, by going from top to bottom. Then, we make sure each leading one is the only non-zero element in its column by subtracting from rows below from bottom to top.</p>
<p>This technique is basically modifying the matrix until all the properties except for property 4. Then property 4 can be solved for relatively easily.</p>
<p>It is the basic technique behind <strong>Guass-Jordan elimination</strong>.### Solution Set</p>
<p>A matrix is inconsistent if and only if its RREF contains a row of the form <span class="math">\(\left[\begin{array}{ccc|c} 0 &amp; \ldots &amp; 0 &amp; 1 \end{array}\right]\)</span> - no possible values can satisfy this equation (<span class="math">\(0 = 1\)</span>), so there are no solutions.</p>
<p>Recall that if consistent, a system of linear equations has either one or infinitely many solutions.</p>
<p>Solve <span class="math">\(\begin{cases} x_1 + x_2 + x_3 &amp;= 4 \\ x_2 + x_3 &amp;= 3 \end{cases}\)</span>:</p>
<blockquote>
<p>The augmented matrix is <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 1 &amp; 1 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 3 \end{array}\right]\)</span>.<br />The matrix in RREF is <span class="math">\(\left[\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 &amp; 3 \end{array}\right]\)</span>.<br />Note that this corresponds to <span class="math">\(x_1 = 1\)</span> and <span class="math">\(x_2 + x_3 = 3\)</span>, and that the second equation has infinitely many solutions.<br />We can find the general solution by writing everything in terms of the fewest variables possible.<br />Clearly, <span class="math">\(x_3 = 3 - x_2\)</span>, so the solution is <span class="math">\(\vec{x} = \begin{bmatrix} 1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 1 \\ x_2 \\ 3 - x_2 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 3 \end{bmatrix} + x_2\begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}\)</span>.<br />We can denote this as <span class="math">\(\vec{x} = \begin{bmatrix} 1 \\ 0 \\ 3 \end{bmatrix} + s\begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}, s \in \mb{R}\)</span>, by assigning <span class="math">\(x_2\)</span> to the parameter <span class="math">\(s\)</span>.<br />This represents a line at the intersection of two planes in <span class="math">\(\mb{R}^3\)</span>.</p>
</blockquote>
<p>The solution set is, geometrically, either a point, a line, a plane, a hyperplane, or empty (no solutions). If there is only one solution, the solution set is a point.</p>
<p>If a <strong>column</strong> in a coefficient matrix has <strong>no leading ones</strong>, the corresponding variable is called a <strong>free variable</strong>. A free variable is a variable that can be any value. Free variables exists if and only if there are an infinite number of solutions.</p>
<h3 id="solving-linear-systems">Solving Linear Systems</h3>
<p>We can take this technique and generalize it a bit to solve any linear system, or show it to not have any solutions:</p>
<ol style="list-style-type: decimal">
<li>Write the augmented matrix for the system.</li>
<li>Row reduce the matrix.</li>
<li>Write the linear system for the new, reduced matrix.</li>
<li>Check for equations of the form <span class="math">\(0 = b, b \ne 0\)</span>, and stop and flag as inconsistent if found.</li>
<li>Write all variables in terms of the free variables.</li>
<li>Assign a parameter to each free variable.<br /></li>
<li>Write the components that are not already fully solved in terms of the free variables.</li>
<li>Write the solution as a linear combination of vectors using vector operations.</li>
</ol>
<p>The standard technique for row reducing matrices is Guass-Jordan elimination. However, this is not always the most efficient way to do it, and there are a lot of tricks that can be used to speed it up in some special cases.</p>
<p>;wip: rewrite my linear solver lib to multiply only, and divide only at the end, this is precision-preserving through bignum and gmp</p>
<p>However, we don't always need to solve a system. Sometimes, we just need to figure out if there are zero, one, or infinite solutions. We will now look at some tools that can be used to analyze this.</p>
<p>The <strong>rank</strong> of a matrix is the number of leading ones in the RREF of the matrix. This is equivalent to the number of variables minus the number of free variables.</p>
<p>The rank of a matrix <span class="math">\(A\)</span> is denoted <span class="math">\(\rank A\)</span>.</p>
<h3 id="theorem-2.2.3">Theorem 2.2.3</h3>
<p>Given an <span class="math">\(m \times n\)</span> coefficient matrix <span class="math">\(A\)</span> for a linear system:</p>
<ul>
<li>If <span class="math">\(\rank A &lt; \rank \sys{A}{\vec{b}}\)</span>, then the system is inconsistent, where <span class="math">\(\vec{b}\)</span> is the right hand side vector for the linear system. In other words, if the rank of the coefficent matrix is less than that of the augmented matrix, then the matrix is inconsistent.</li>
<li>If <span class="math">\(\sys{A}{\vec{b}}\)</span> is consistent, then there are <span class="math">\(n - \rank A\)</span> free variables. If there are 0 free variables, then the system has only one solution.</li>
<li><span class="math">\(\rank A = m\)</span> if and only if the system <span class="math">\(\sys{A}{\vec{b}}\)</span> is consistent for every <span class="math">\(\vec{b} \in \mb{R}^m\)</span>. In other words, there are no free variables if and only if the the system is consistent for every possible vector of right hand side values.</li>
</ul>
<p>Proof of first:</p>
<p>(point 1)</p>
<blockquote>
<p>Assume <span class="math">\(\rank A &lt; \rank \sys{A}{\vec{b}}\)</span>.<br />Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />This corresponds to <span class="math">\(0 = 1\)</span>, which makes the system inconsistent.</p>
</blockquote>
<p>Proof:</p>
<p>(point 2)</p>
<blockquote>
<p>Assume <span class="math">\(\sys{A}{\vec{b}}\)</span> is consistent.<br />The number of free variables is the number of variables minus the number of those that are not free.<br />Those that are not free have leading ones, so <span class="math">\(\rank A\)</span> is the number of non-free variables.<br />So the number of free variables is <span class="math">\(n - \rank A\)</span>.</p>
</blockquote>
<p>Proof:</p>
<p>(point 3)</p>
<blockquote>
<p>First, we will prove that if the system <span class="math">\(\sys{A}{\vec{b}}\)</span> is not consistent for every <span class="math">\(\vec{b} \in \mb{R}^m\)</span>, then <span class="math">\(\rank A \ne m\)</span>.<br />Suppose <span class="math">\(\sys{A}{\vec{b}}\)</span> is inconsistent for some <span class="math">\(\vec{b} \in \mb{R}^m\)</span>.<br />Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />This row does not contain a leading one, so <span class="math">\(\rank A &lt; m\)</span>.<br />Now we will prove that if <span class="math">\(\rank A \ne m\)</span>, then the system <span class="math">\(\sys{A}{\vec{b}}\)</span> is not consistent for every <span class="math">\(\vec{b} \in \mb{R}^m\)</span>.<br />Suppose <span class="math">\(\rank A \ne m\)</span>. Then <span class="math">\(\rank A &lt; m\)</span>. Then there is a row of all zeroes in the RREF of A.<br />Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />Then the system is inconsistent.</p>
</blockquote>
<h3 id="homogenous-systems">Homogenous Systems</h3>
<p>A system is <strong>homogenous</strong> if and only if the right-hand side contains only zeros. In other words, it has the form <span class="math">\(\sys{A}{\vec{0}}\)</span>.</p>
<p>A homogenous matrix is one where the last column is all 0 - a matrix corresponding to a homogenous system.</p>
<p>Note that since the right side is always 0, the RREF of a homogenous matrix also has the right side always 0.</p>
<p>Also note that it is impossible to get a row where every element is 0 except the right hand side. Therefore, <strong>homogenous systems are always consistent</strong>.</p>
<p>For example, <span class="math">\(\vec{0}\)</span> is always a solution to a homogenous system. This is called the <strong>trivial solution</strong>.</p>
<p>When we row reduce a homogenous matrix, we can save space by just row reducing the coefficient matrix instead. The last column will all be 0 anyways.</p>
<h3 id="theorem-2.2.4">Theorem 2.2.4</h3>
<p>Given an <span class="math">\(m \times n\)</span> matrix <span class="math">\(A\)</span>, <span class="math">\(\rank A \le \min{m, n}\)</span>.</p>
<h3 id="theorem-2.2.5">Theorem 2.2.5</h3>
<p>The solution set of a homogenous system with <span class="math">\(n\)</span> variables is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Note that the solution set of a homogenous system written as a vector equation always has <span class="math">\(\vec{0}\)</span> as the constant term. That means that all solutions are linear combinations of a few vectors.</p>
<p>As a result, we can always write the solution set of a homogenous system as the span of a set of vectors.</p>
<p>Because all spans are subspaces (by Theorem 1.2.2), the solution set is a subspace.</p>
<p>Because the solution set is a subspace, we often call it the <strong>solution space</strong> of a aystem.</p>
<h1 id="section">30/1/14</h1>
<h2 id="matrices">Matrices</h2>
<p>Matrices are abstract objects like vectors. We usually denote matrix variables with capital letters.</p>
<p>An <span class="math">\(m \times n\)</span> matrix <span class="math">\(A\)</span> is a rectangular array with <span class="math">\(m\)</span> rows and <span class="math">\(n\)</span> columns. We represent the <span class="math">\(i\)</span>th row and <span class="math">\(j\)</span>th column as <span class="math">\(a_{i, j}\)</span>, or <span class="math">\((A)_{i, j}\)</span>.</p>
<p>In other words, <span class="math">\(A = \begin{bmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \end{bmatrix}\)</span>.</p>
<p>All <span class="math">\(m \times n\)</span> matrices belong to the set <span class="math">\(M_{m \times n}\)</span>, similar to how all <span class="math">\(n\)</span>-dimensional vectors belong to the set <span class="math">\(\mb{R}^n\)</span>.</p>
<h3 id="operations-1">Operations</h3>
<p>Let <span class="math">\(A, B, C\)</span> be matrices.</p>
<p>Equality: <span class="math">\(A = B \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (A)_{i, j} = (B)_{i, j}\)</span>.</p>
<p>In other words, two matrices are equal if and only if they are both the same size and contain the same entries.</p>
<p>Addition/subtraction: <span class="math">\(A \pm B = C \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (C)_{i, j} = (A)_{i, j} \pm (B)_{i, j}\)</span>.</p>
<p>In other words, when we add matrices (only defined for matrices of the same size), we simply add their corresponding entries.</p>
<p>Scalar multiplication: <span class="math">\(t \in \mb{R}; tA = C \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (C)_{i, j} = t(A)_{i, j}\)</span>.</p>
<p>In other words, when we multiply a matrix by a scalar or a scalar by a matrix, we simply multiply each entry in the matrix by the scalar.</p>
<p>With these operations, we can take linear combinations of matrices.</p>
<h3 id="theorem-3.1.1">Theorem 3.1.1</h3>
<p>For all <span class="math">\(A, B, C \in M_{m \times n}, s, t, \in \mb{R}\)</span>:</p>
<ul>
<li>Closure under addition: <span class="math">\(A + B \in M_{m \times n}\)</span>.</li>
<li>Associativity: <span class="math">\((A + B) + C = A + (B + C)\)</span></li>
<li>Commutativity: <span class="math">\(A + B = B + A\)</span></li>
<li>Additive identity: <span class="math">\(\exists O \in M_{m \times n}, \forall A \in M_{m, n}, A + O = A\)</span> (<span class="math">\(O\)</span> is called the zero matrix).</li>
<li>Additive inverse: <span class="math">\(\forall A \in M_{m, n}, \exists -A \in M_{m, n}, A + (-A) = O\)</span>.</li>
<li>Closure under scalar multiplication: <span class="math">\(cA \in M_{m, n}\)</span>.</li>
<li><span class="math">\(c(dA) = (cd)A\)</span></li>
<li>Scalar distributivity: <span class="math">\((c + d)A = cA + dA\)</span>.</li>
<li>Vector distributivity: <span class="math">\(c(A + B) = cA + cB\)</span>.</li>
<li>Multiplicative identity: <span class="math">\(1A = A\)</span>.</li>
</ul>
<h3 id="transpose">Transpose</h3>
<p>Vectors and matrices are closely related. We have been writing vectors as columns for a while now. In fact, we can use vectors to represent the columns of a matrix.</p>
<p>For example, if <span class="math">\(\vec{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \vec{v} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}\)</span>, then <span class="math">\(\begin{bmatrix} \vec{u} &amp; \vec{v} \end{bmatrix} = \begin{bmatrix} 1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6 \end{bmatrix}\)</span>.</p>
<p>The <strong>transpose</strong> of a matrix <span class="math">\(A \in M_{m, n}\)</span> is <span class="math">\(A^T \in M_{n, m}\)</span> (&quot;A transposed&quot;), a matrix such that <span class="math">\((A)_{i, j} = (A^T)_{j, i}\)</span>.</p>
<p>Note that the dimensions were swapped around. Basically, the rows became the columns, and the columns became the rows - we reflected the entries around a diagonal line starting from the top left.</p>
<ul>
<li><span class="math">\(A = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}; A^T = \begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix}\)</span>.</li>
<li><span class="math">\(B = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{bmatrix}; B^T = \begin{bmatrix} 1 &amp; 3 &amp; 5 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}\)</span>.</li>
<li><span class="math">\(C = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}; C^T = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}\)</span>.</li>
</ul>
<p>Some matrices satisfy the property <span class="math">\(A = A^T\)</span>. These matrices are called <strong>symmetric</strong>, because they are symmetric about the diagonal line. Note that only square matrices can be symmetric.</p>
<p>Transposes are useful for representing the rows of a matrix with vectors, rather than columns.</p>
<p>For example, if <span class="math">\(\vec{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \vec{v} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}\)</span>, then <span class="math">\(\begin{bmatrix} \vec{u}^T \\ \vec{v}^T \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}\)</span>.</p>
<h3 id="theorem-3.1.2">Theorem 3.1.2</h3>
<p>For all <span class="math">\(A, B \in M_{m, n}, c \in \mb{R}\)</span>:</p>
<ul>
<li><span class="math">\((A^T)^T = A\)</span></li>
<li><span class="math">\((A + B)^T = A^T + B^T\)</span></li>
<li><span class="math">\((cA)^T = c(A^T)\)</span></li>
</ul>
<p>Also, <span class="math">\((AB)^T = B^T A^T\)</span>. Note that the order reversed.</p>
<p>Proof:</p>
<p>(proof of first)</p>
<blockquote>
<p>Clearly, <span class="math">\(\forall 1 \le i \le m, \forall 1 \le j \le n, ((A^T)^T)_{i, j} = (A^T)_{j, i} = (A)_{i, j}\)</span>.<br />So <span class="math">\((A^T)^T = A\)</span>.</p>
</blockquote>
<p>(proof of second)</p>
<blockquote>
<p>Clearly, <span class="math">\(\forall 1 \le i \le m, \forall 1 \le j \le n, ((A + B)^T)_{i, j} = (A + B)_{j, i} = (A)_{j, i} + (B)_{j, i} = (A^T)_{i, j} + (B^T)_{i, j}\)</span>.<br />So <span class="math">\((A + B)^T = A^T + B^T\)</span>.</p>
</blockquote>
<p>(proof of third)</p>
<blockquote>
<p>Clearly, <span class="math">\(\forall 1 \le i \le m, \forall 1 \le j \le n, ((cA)^T)_{i, j} = (cA)_{j, i} = c((A)_{j, i}) = c((A^T)_{i, j})\)</span>.<br />So <span class="math">\((cA)^T = c(A^T)\)</span>.</p>
</blockquote>
<h1 id="section-1">1/2/14</h1>
<h2 id="matrix-multiplication">Matrix Multiplication</h2>
<h3 id="matrix-vector-multiplication">Matrix-Vector Multiplication</h3>
<p>Consider the linear system <span class="math">\(\sys{A}{\vec{B}}\)</span> with the solution <span class="math">\(\vec{x}\)</span>.</p>
<p>When we multiply a matrix by a vector, we want to have it so that <span class="math">\(A\vec{x} = \vec{b}\)</span>. This is because it would be very useful to help solve systems of linear equations.</p>
<p>Matrix-vector multiplication can be defined in terms of matrices with represented with row vectors or column vectors.</p>
<p>We will first define it in terms of row vectors.</p>
<p>Let <span class="math">\(A = \begin{bmatrix} \vec{a}_1^T \\ \vdots \\ \vec{a}_m^T \end{bmatrix}\)</span> and <span class="math">\(A \in M_{m, n}\)</span>.</p>
<p>Note that <span class="math">\(\vec{a}_i^T\)</span> represents the coefficients of the <span class="math">\(i\)</span>th equation of the system. So if and only if <span class="math">\(\vec{x}\)</span> is a solution, <span class="math">\(\vec{x} \cdot \vec{a}_i^T = b_i\)</span>.</p>
<p>Now we can define matrix multiplication: <span class="math">\(A\vec{x} = \begin{bmatrix} \vec{a}_1^T \cdot \vec{x} \\ \vdots \\ \vec{a}_m^T \cdot \vec{x} \end{bmatrix} = \vec{b}\)</span>.</p>
<p>Basically, we take the dot product of each row and assemble it together into a vector. The product of a matrix and a vector is therefore a vector.</p>
<p>Note that in order for matrix-vector multiplication to be defined, the number of columns in <span class="math">\(A\)</span> must equal the number of rows/components in <span class="math">\(\vec{x}\)</span> in order for the dot product to work. An <span class="math">\(m \times n\)</span> matrix can only be multiplied by vectors in <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Now we can also define it in terms of column vectors.</p>
<p>Note that the system <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> has the coefficient matrix <span class="math">\(A = \begin{bmatrix} \begin{bmatrix} a_{1, 1} \\ \vdots \\ a_{m, 1} \end{bmatrix} \cdots \begin{bmatrix} a_{1, n} \\ \vdots \\ a_{m, n} \end{bmatrix} \end{bmatrix} = \begin{bmatrix} \vec{c}_1 \ldots \vec{c}_n \end{bmatrix}\)</span>, where <span class="math">\(\vec{c}_i\)</span> represents the <span class="math">\(i\)</span>th column of the matrix.</p>
<p>Note that the system can be written as <span class="math">\(x_1\begin{bmatrix} a_{1, 1} \\ \vdots \\ a_{m, 1} \end{bmatrix} + \ldots + x_n\begin{bmatrix} a_{1, n} \\ \vdots \\ a_{m, n} \end{bmatrix} = \vec{b}\)</span>.</p>
<p>So we can write the system as <span class="math">\(x_1 \vec{c}_1 + \ldots + x_n \vec{c}_n = \vec{b}\)</span>.</p>
<p>Now we define matrix-vector multiplication in terms of columns as <span class="math">\(A\vec{x} = x_1 \vec{c}_1 + \ldots + x_n \vec{c}_n\)</span>, where <span class="math">\(\vec{c}_i\)</span> represents the <span class="math">\(i\)</span>th column of <span class="math">\(A\)</span>.</p>
<p>We define it this way because now we can represent the system with <span class="math">\(A\vec{x} = \vec{b}\)</span>.</p>
<p>Note that we can use this to select one particular column of the matrix using a standard basis vector (a vector of all zeros except a lone one).</p>
<p>For example, multiplying an <span class="math">\(m \times n\)</span> matrix by the <span class="math">\(i\)</span>th standard basis vector in <span class="math">\(\mb{R}^n\)</span> (the vector with all components set to 0 except for the <span class="math">\(ith\)</span> component, which is set to 1) will result in the <span class="math">\(i\)</span>th column of the matrix.</p>
<h3 id="matrix-matrix-multiplication">Matrix-Matrix Multiplication</h3>
<p>Now we can define the product of a matrix and a matrix.</p>
<p>Given an <span class="math">\(m \times n\)</span> matrix <span class="math">\(A = \begin{bmatrix} \vec{a}_1^T \\ \vdots \\ \vec{a}_n^T \end{bmatrix}\)</span> and an <span class="math">\(n \times p\)</span> matrix <span class="math">\(B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_p \end{bmatrix}\)</span>, <span class="math">\(AB = A\begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_p \end{bmatrix} = \begin{bmatrix} A\vec{b}_1 &amp; \ldots &amp; A\vec{b}_p \end{bmatrix}\)</span> is an <span class="math">\(m \times p\)</span> matrix.</p>
<p>Basically, when we multiply a matrix by a matrix, we multiply the first matrix by each of the column vectors in the second matrix.</p>
<p>Note that this requires that the number of columns in <span class="math">\(A\)</span> must be the same as the number of rows in <span class="math">\(B\)</span>, since the dot product must be defined.</p>
<p>Note that <span class="math">\((AB)_{i, j} = \vec{a}_i^T \vec{b}_j = \vec{a}_i \cdot \vec{b}_j = \sum_{k = 1}^n (a_i^T)_k (\vec{a}_k)_j = \sum_{k = 1}^n (A)_{i, k} (B)_{k, j}\)</span>.</p>
<h3 id="theorem-3.1.3-properties-of-matrix-multiplication">Theorem 3.1.3 (Properties of Matrix Multiplication)</h3>
<p>If <span class="math">\(A, B, C\)</span> are matrices of the required dimensions, <span class="math">\(t \in \mb{R}\)</span>:</p>
<ul>
<li>Distributivity: <span class="math">\(A(B + C) = AB + AC\)</span> (left distributive) and <span class="math">\((A + B)C = AC + BC\)</span> (right distributive).</li>
<li>Scalar multiplication: <span class="math">\(t(AB) = (tA)B = A(tB)\)</span>.</li>
<li>Associativity: <span class="math">\((AB)C = A(BC)\)</span>.</li>
<li>Transposition distributivity: <span class="math">\((AB)^T = A^T B^T\)</span>.</li>
</ul>
<p>Note that matrix multiplication is not commutative over matrices: <span class="math">\(AB \ne BA\)</span> for some matrices.</p>
<h3 id="theorem-3.1.4-matrix-multiplication-cancellation">Theorem 3.1.4 (Matrix Multiplication Cancellation)</h3>
<p>Note that <span class="math">\(A = B \implies AC = BC \wedge CA = BA\)</span>. However, <span class="math">\(AC = BC \vee CA = BA\)</span> <strong>does not imply</strong> <span class="math">\(A = B\)</span> - there is no cancellation law for matrix multiplication.</p>
<p>That said, given <span class="math">\(A, B \in M_{m, n}\)</span>, <span class="math">\((\forall \vec{x} \in \mb{R}^n, A\vec{x} = B\vec{x}) \implies A = B\)</span>.</p>
<p>In other words, if the product of a matrix with an arbitrary vector are equal to each other, then the matrices are themselves equal to each other.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(A = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}, B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_n \end{bmatrix}\)</span>.<br />Let <span class="math">\(i \in \mb{N}, i \le n\)</span>.<br />Let <span class="math">\(\vec{e}_i\)</span> be the <span class="math">\(i\)</span>th standard basis vector in <span class="math">\(\mb{R}^n\)</span> (the vector with all zeroes except the <span class="math">\(i\)</span>th element, which is 1).<br />Let <span class="math">\(\vec{x} \in \mb{R}^n\)</span>. Construct <span class="math">\(\vec{x} = \vec{e}_i\)</span>.<br />Assume <span class="math">\(A\vec{x} = B\vec{x}\)</span>. Then <span class="math">\(A\vec{e}_i = B\vec{e}_i = \vec{a}_i = \vec{b}_i\)</span>.<br />Since <span class="math">\(i\)</span> is arbitrary, <span class="math">\(forall 1 \le i \le n, \vec{a}_i = \vec{b}_i\)</span>.<br />So <span class="math">\(A = B\)</span>.</p>
</blockquote>
<h1 id="section-2">3/2/14</h1>
<h2 id="identity-matrix">Identity Matrix</h2>
<p>The identity matrix is the matrix such that, when multiplied with any matrix, or when any matrix is multiplied by it, results in that same matrix.</p>
<p>Formally, the <strong>identity matrix</strong> is the unique matrix <span class="math">\(I \in M_{n, n}\)</span> such that <span class="math">\(I_{i, j} = \begin{cases} 1 &amp;\text{if } i = j \\ 0 &amp;\text{if } i \ne j \end{cases}\)</span>.</p>
<p>We denote this with <span class="math">\(I_n\)</span> to represent the square matrix of dimensions <span class="math">\(n \times n\)</span>. We can also write it as <span class="math">\(I_n = \begin{bmatrix} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n \end{bmatrix}\)</span>.</p>
<p>In other words, we want to find <span class="math">\(I\)</span> such that <span class="math">\(\forall A \in M_{m, n}, AI = IA = A\)</span>.</p>
<p>Note that this is only possible when <span class="math">\(m = n\)</span>, since if it wasn't, the product of the matrices would result in a matrix of a different size.</p>
<p>We will now find the identity matrix. We want <span class="math">\(A = AI\)</span>. So <span class="math">\(\begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix} = A\begin{bmatrix} \vec{i}_1 &amp; \ldots &amp; \vec{i}_n \end{bmatrix} = \begin{bmatrix} A\vec{i}_1 &amp; \ldots &amp; A\vec{i}_n \end{bmatrix}\)</span>.</p>
<p>So <span class="math">\(\vec{a}_c = A\vec{i}_c\)</span> for all <span class="math">\(1 \le c \le n\)</span>. Recall that <span class="math">\(\vec{i}_c = \vec{e}_c \implies \vec{a}_c = A\vec{i}_c\)</span>, where <span class="math">\(\vec{e}_c\)</span> is the <span class="math">\(c\)</span>th standard basis vector. So <span class="math">\(\vec{i}_c = \vec{e}_c\)</span>.</p>
<p>Therefore, one possible value of <span class="math">\(I\)</span> is <span class="math">\(\begin{bmatrix} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n \end{bmatrix}\)</span>.</p>
<p>We can also construct a similar proof for <span class="math">\(A = IA\)</span>.</p>
<h3 id="thoerem-3.1.5">Thoerem 3.1.5</h3>
<p>If <span class="math">\(I = \begin{bmatrix} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n \end{bmatrix}\)</span>, then <span class="math">\(A = AI = IA\)</span>.</p>
<h3 id="theorem-3.1.6">Theorem 3.1.6</h3>
<p>The multiplicative identity for <span class="math">\(M_{n, n}\)</span> is unique.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(I_1, I_2\)</span> be two possibly equal multiplicative identities in <span class="math">\(M_{n, n}\)</span>.<br />Since <span class="math">\(I_1\)</span> is a multiplicative identity, <span class="math">\(I_2 = I_1 I_2\)</span>.<br />Since <span class="math">\(I_2\)</span> is a multiplicative identity, <span class="math">\(I_1 = I_1 I_2\)</span>.<br />So <span class="math">\(I_1 = I_2\)</span> and all multiplicative identities are equal to each other.<br />Therefore, multiplicative identities are unique.</p>
</blockquote>
<p>For example, for <span class="math">\(I \in M_{4, 4}\)</span>, <span class="math">\(I = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>.</p>
<h2 id="block-matrices">Block Matrices</h2>
<p>Previously, we defined multiplication by splitting up a matrix into its columns. As it turns out, it's also useful to split a matrix up in other ways.</p>
<p>When we write <span class="math">\(A = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}\)</span>, we actually split up <span class="math">\(A\)</span> into <span class="math">\(n\)</span> different blocks, each one with only one column (so we can represent them using vectors).</p>
<p>A <strong>block matrix</strong> is a matrix that contains <strong>blocks</strong>, which are simply smaller matrices that we build the larger matrix out of.</p>
<p>Formally, if <span class="math">\(A\)</span> is an <span class="math">\(m \times n\)</span> matrix, then <span class="math">\(A\)</span> can be written as a <span class="math">\(u \times v\)</span> block matrix <span class="math">\(A = \begin{bmatrix} A_{1, 1} &amp; \ldots &amp; A_{1, v} \\ \vdots &amp; \vdots &amp; \vdots \\ A_{u, 1} &amp; \ldots &amp; A_{u, v} \end{bmatrix}\)</span>, where <span class="math">\(A_{x, y}\)</span> is a block and all blocks in any given row in <span class="math">\(A\)</span> have the same number of rows, and all blocks in any given column in <span class="math">\(A\)</span> have the same number of columns.</p>
<p>For example, given <span class="math">\(A = \begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 5 &amp; 6 &amp; 7 &amp; 8 \end{bmatrix}\)</span>, we can split it up to form a block matrix <span class="math">\(A = \begin{bmatrix} B &amp; C \end{bmatrix}\)</span>, where <span class="math">\(B = \begin{bmatrix} 1 &amp; 2 \\ 5 &amp; 6 \end{bmatrix}\)</span> and <span class="math">\(C = \begin{bmatrix} 3 &amp; 4 \\ 7 &amp; 8 \end{bmatrix}\)</span>.</p>
<p>When we multiply block matrices, we can actually treat the blocks as simple values and multiply them using the same rules as we would use for numbers.</p>
<p>For example, <span class="math">\(\begin{bmatrix} A_{1, 1} &amp; A_{1, 2} \end{bmatrix} \begin{bmatrix} B_{1, 1} \\ B_{2, 1} \end{bmatrix} = \begin{bmatrix} A_{1, 1} B_{1, 1} + A_{1, 2} B_{2, 1} \end{bmatrix}\)</span>.</p>
<h1 id="section-3">8/2/14</h1>
<h2 id="matrix-mappings">Matrix Mappings</h2>
<p>A <strong>function</strong> is a rule that associates <span class="math">\(x \in A\)</span> to <span class="math">\(f(x) \in B\)</span>. This is written as <span class="math">\(f: A \to B\)</span>, where <span class="math">\(A\)</span> is the <strong>domain</strong> of <span class="math">\(f\)</span>, and <span class="math">\(B\)</span> is the <strong>codomain</strong> of <span class="math">\(f\)</span>.</p>
<p><span class="math">\(f(a)\)</span> is the &quot;<strong>image</strong> of <span class="math">\(a\)</span> under <span class="math">\(f\)</span>&quot;. An <strong>image</strong> is a subset of the codomain corresponding to the function values for a subset of the domain.</p>
<p>Formally, <span class="math">\(I = \set{y \in C : \exists x \in D, f(x) = y}\)</span> where <span class="math">\(C\)</span> is the codomain, <span class="math">\(D\)</span> is a subset of the domain, and <span class="math">\(I\)</span> is the image,</p>
<p>A <strong>matrix mapping</strong> is a function <span class="math">\(f: \mb{R}^n \to \mb{R}^m\)</span> where <span class="math">\(f(\vec{x}) = A\vec{x}\)</span> for some matrix <span class="math">\(A \in M_{m, n}\)</span>.</p>
<p>In other words, it is a function over vectors that multiplies matrix by the given vector.</p>
<p>We can write this as <span class="math">\(f\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\right) = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix}\)</span>, or with the more aesthetically pleasing notation <span class="math">\(f(x_1, \ldots, x_n) = (y_1, \ldots, y_m)\)</span>.</p>
<p>What is the domain/codomain of <span class="math">\(f(\vec{x}) = A\vec{x}, A = \begin{bmatrix} 2 &amp; 3 \\ -4 &amp; 0 \\ 5 &amp; 1 \end{bmatrix}\)</span>.</p>
<blockquote>
<p>Clearly, <span class="math">\(f\)</span> is defined iff and only if the matrix multiplication is defined, which is when <span class="math">\(\vec{x}\)</span> has 2 components.<br />So <span class="math">\(\vec{x} \in \mb{R}^2\)</span>.<br />Since <span class="math">\(A\)</span> has 3 rows, <span class="math">\(f(\vec{x})\)</span> also has 3 rows.<br />So <span class="math">\(f(\vec{x}) \in \mb{R}^3\)</span>.</p>
</blockquote>
<p>We can also write the function in a simpler form, <span class="math">\(f(\vec{x}) = \begin{bmatrix} 2x_1 + 3x_2 \\ -4x_1 \\ 5x_1 + x_2 \end{bmatrix}\)</span>.</p>
<h1 id="section-4">10/2/14</h1>
<p>Note that if <span class="math">\(f(\vec{x}) = A\vec{x}\)</span> is a matrix mapping, then <span class="math">\(A = \begin{bmatrix} f(1, 0, \ldots, 0) &amp; f(0, 1, \ldots, 0) &amp; \ldots &amp; f(0, \ldots, 1, 0) &amp; f(0, \ldots, 0, 1) \end{bmatrix}\)</span>.</p>
<h3 id="theorem-3.3.1-linearity-of-matrix-mappings">Theorem 3.3.1 (Linearity of Matrix Mappings)</h3>
<p>Given <span class="math">\(A \in M_{m, n}, f(\vec{x}) = A\vec{x}\)</span>, <span class="math">\(\forall \vec{x}, \vec{y} \in \mb{R}^n, \forall s, t \in \mb{R}, f(s\vec{x} + t\vec{y}) = sf(\vec{x}) + tf(\vec{y})\)</span>.</p>
<p>In other words, when we multiply a matrix by a linear combination of vectors, it is equivalent to the linear combination of multiplying the matrix by the vectors directly.</p>
<p>This is the <strong>linearity property</strong>. What this theorem states is that all matrix mappings have the linearity property.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(A \in M_{m, n}, f(\vec{x}) = A\vec{x}, \vec{x}, \vec{y} \in \mb{R}^n, s, t \in \mb{R}\)</span>.<br />Then <span class="math">\(f(s\vec{x} + t\vec{y}) = A(s\vec{x} + t\vec{y}) = As\vec{x} + At\vec{y} = sA\vec{x} + tA\vec{y} = sf(\vec{x}) + tf(\vec{y})\)</span>.</p>
</blockquote>
<h2 id="linear-mappings">Linear Mappings</h2>
<p>A <strong>linear mapping/linear transformation/linear operator</strong> is a function <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> that has the linearity property.</p>
<p>In other words, given <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span>, <span class="math">\(L\)</span> is a linear mapping if and only if <span class="math">\(L(s\vec{x} + t\vec{y}) = sL(\vec{x}) + tL(\vec{y}), \vec{x}, \vec{y} \in \mb{R}^n, s, t \in \mb{R}\)</span>.</p>
<p>To prove that a function is a lienar mapping, we simply need to prove that it has the inearity property.</p>
<p>Prove that <span class="math">\(\proj_{\vec{a}}\)</span> is a linear operator:</p>
<blockquote>
<p><span class="math">\[
\begin{align}
\proj_{\vec{a}} (s\vec{x} + t\vec{y}) &amp;= \left((s\vec{x} + t\vec{y}) \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} \\
&amp;= \left(s\vec{x} \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} + \left(t\vec{y} \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} \\
&amp;= s\proj_{\vec{a}} \vec{x} + t\proj_{\vec{a}} \vec{y} \\
\end{align}
\]</span> Therefore, <span class="math">\(\proj_{\vec{a}}\)</span> has the linearity property and is a linear mapping.</p>
</blockquote>
<h3 id="theorem-3.2.2">Theorem 3.2.2</h3>
<p>If <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is a linear mapping, then <span class="math">\(L(\vec{x}) = A\vec{x}\)</span>, where <span class="math">\(A = \begin{bmatrix} L(\vec{e}_1) &amp; \ldots &amp; L(\vec{e}_n) \end{bmatrix}\)</span> and <span class="math">\(\vec{e}_i\)</span> is the <span class="math">\(i\)</span>th standard basis vector in <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(\vec{x} \in \mb{R}^n\)</span> and <span class="math">\(\vec{e}_i\)</span> be the <span class="math">\(i\)</span>th standard basis vector. Then <span class="math">\(\vec{x} = x_1 \vec{e}_1 + \ldots + x_n \vec{e}_n\)</span>.<br />Then <span class="math">\(L(\vec{x}) = L(x_1 \vec{e}_1 + \ldots + x_n \vec{e}_n) = x_1 L(\vec{e}_1) + \ldots + x_n L(\vec{e}_n) = \begin{bmatrix} L(\vec{e}_1) &amp; \ldots &amp; L(\vec{e}_n) \end{bmatrix} \vec{x}\)</span>.<br />Let <span class="math">\(A = \begin{bmatrix} L(\vec{e}_1) &amp; \ldots &amp; L(\vec{e}_n) \end{bmatrix}\)</span>.<br />Then <span class="math">\(L(\vec{x}) = A\vec{x}\)</span>.</p>
</blockquote>
<p>The matrix <span class="math">\(A\)</span> in the theorem above is called the <strong>standard matrix</strong> of the linear mapping <span class="math">\(L\)</span>. It is the matrix such that <span class="math">\(L(\vec{x}) = A\vec{x}\)</span></p>
<p>We denote this as <span class="math">\(A = [L]\)</span>, where <span class="math">\(L\)</span> is a linear mapping. Additionally, <span class="math">\(L(\vec{x}) = [L]\vec{x}\)</span></p>
<p>Find the standard matrix of <span class="math">\(\proj_{\vec{a}}\)</span>:</p>
<blockquote>
<p><span class="math">\[
\begin{align}
\proj_{\vec{a}} \vec{x} &amp;= \left(\vec{x} \cdot \frac{\vec{a}}{\magn{a}}\right) \frac{\vec{a}}{\magn{a}} \\
\proj_{\vec{a}} \vec{e}_i &amp;= \frac{\vec{a}_i}{\magn{a}} \frac{\vec{a}}{\magn{a}} = \vec{a}_i \frac{\vec{a}}{\magn{a}^2} \\
f(\vec{x}) &amp;= A\vec{x} = \proj_{\vec{a}} \\
A &amp;= \begin{bmatrix} \vec{a}_1 \frac{\vec{a}}{\magn{a}^2} &amp; \ldots &amp; \vec{a}_n \frac{\vec{a}}{\magn{a}^2} \end{bmatrix} \\
\end{align}
\]</span></p>
</blockquote>
<h1 id="section-5">11/2/14</h1>
<h3 id="rotations">Rotations</h3>
<p><span class="math">\(R_\theta: \mb{R}^2 \to \mb{R}^2\)</span> is a linear mapping <span class="math">\(R_\theta(x_1, x_2)\)</span> that rotates a vector by an angle <span class="math">\(\theta\)</span> counterclockwise.</p>
<p>We can geometrically determine that the location of <span class="math">\(\vec{x}\)</span> after rotation is <span class="math">\(R_\theta(x_1, x_2) = \begin{bmatrix} x_1 \cos \theta - x_2 \sin \theta \\ x_1 \sin \theta + x_2 \cos \theta \end{bmatrix}\)</span>.</p>
<p>Now we can convert it into a matrix mapping: <span class="math">\(R_\theta(\vec{e}_1) = x_1 \begin{bmatrix} \cos \theta \\ \sin \theta \end{bmatrix}, R_\theta(\vec{e}_2) = x_2 \begin{bmatrix} -\sin \theta \\ \cos \theta \end{bmatrix}\)</span>.</p>
<p>So <span class="math">\(R_\theta(\vec{x}) = \begin{bmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix}\vec{x}\)</span>.</p>
<p><span class="math">\(\begin{bmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix}\)</span> is called a <strong>rotation matrix</strong>.</p>
<h3 id="reflections">Reflections</h3>
<p><span class="math">\(\refl_P: \mb{R}^n \to \mb{R}^n\)</span> is a linear mapping <span class="math">\(\refl_P(\vec{x})\)</span> that reflects <span class="math">\(\vec{x}\)</span> about the hyperplane <span class="math">\(P\)</span>.</p>
<p>We geometrically determine that the relection of a vector about a hyperplane is the projection of the vector on the hyperplane minus the perpendicular.</p>
<p>In other words, <span class="math">\(\refl_P \vec{x} = \proj_P \vec{x} - \perp_P \vec{x} = (\vec{x} - \perp_P \vec{x}) - \perp_P \vec{x} = \vec{x} - 2\perp_P \vec{x} = \vec{x} - 2\proj_{\vec{n}} \vec{x}\)</span>, where <span class="math">\(\vec{n}\)</span> is the normal of the hyperplane.</p>
<p>Now we find the standard matrix of this operation. Let <span class="math">\(\vec{n}\)</span> be the normal of the plane. Then <span class="math">\(\refl_P \vec{e}_i = \vec{e}_i - 2\proj_{\vec{n}} \vec{e}_i\)</span>.</p>
<p>So <span class="math">\(\refl_P \vec{x} = \begin{bmatrix} \vec{e}_1 - 2\proj_{\vec{n}} \vec{e}_1 &amp; \ldots &amp; \vec{e}_n - 2\proj_{\vec{n}} \vec{e}_n \end{bmatrix} \vec{x}\)</span>.</p>
<h1 id="section-6">14/2/14</h1>
<h2 id="range">Range</h2>
<p>The <strong>range</strong> of a function <span class="math">\(f: \mb{R}^n \to \mb{R}^m\)</span> is the set of all images (function values) that <span class="math">\(f\)</span> produces for any <span class="math">\(\vec{x} \in \mb{R}^n\)</span>.</p>
<p>Range in mathematics can refer to this set of images, or to the codomain. The difference between the image and the codomain is that the image is always a subset of the codomain - the codomain also may include values that <span class="math">\(f(\vec{x})\)</span> cannot possibly produce. The codomain is something we define for the function, while the image is a property of the function itself.</p>
<p>For example, the image of <span class="math">\(f(\vec{x}) = \vec{0}\)</span> (<span class="math">\(f: \mb{R}^2 \to \mb{R}^3\)</span>) is <span class="math">\(\set{\vec{0}}\)</span>, while the codomain is <span class="math">\(\mb{R}^3\)</span>. The domain is <span class="math">\(\mb{R}^2\)</span>.</p>
<p>Formally, we denote range as <span class="math">\(\range f = \set{f(\vec{x}) \middle| \vec{x} \in \mb{R}^n}\)</span>. It is basically the set of all values the function can produce.</p>
<p>A value <span class="math">\(\vec{y}\)</span> is in the range (image) if <span class="math">\(\exists \vec{x} \in \mb{R}^n, f(\vec{x}) = \vec{y}\)</span>. We simply prove that <span class="math">\(\vec{y} \in \range f\)</span></p>
<h3 id="theorem-3.3.2">Theorem 3.3.2</h3>
<p>If <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is linear, then <span class="math">\(\range L\)</span> is a subspace of the codomain, <span class="math">\(\mb{R}^m\)</span>.</p>
<p>Also, if <span class="math">\(L\)</span> is linear, then <span class="math">\(L(\vec{0}) = \vec{0}\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is linear.<br />By definition, <span class="math">\(\range L \subseteq \mb{R}^m\)</span>.<br />Since <span class="math">\(L(\vec{0}) = \vec{0}\)</span>, <span class="math">\(\vec{0} \in \range L\)</span>.<br />Let <span class="math">\(\vec{y}, \vec{z} \in \range L\)</span>.<br />Then <span class="math">\(\exists \vec{w} \in \mb{R}^n, L(\vec{w}) = \vec{y}\)</span> and <span class="math">\(\exists \vec{x} \in \mb{R}^n, L(\vec{x}) = \vec{z}\)</span>.<br />Clearly, <span class="math">\(\vec{y} + \vec{z} = L(\vec{w}) + L(\vec{x}) = L(\vec{w} + \vec{x})\)</span> and <span class="math">\(L(\vec{w} + \vec{x}) \in \range L\)</span>, so <span class="math">\(\vec{y} + \vec{z} \in \range L\)</span>.<br />Clearly, <span class="math">\(c\vec{y} = cL(\vec{w}) = L(c\vec{w})\)</span> and <span class="math">\(L(c\vec{w}) \in \range L\)</span>, so <span class="math">\(c\vec{y} \in \range L\)</span>.<br />So <span class="math">\(\range L\)</span> satisfies the subspace test, and is a subspace of <span class="math">\(\mb{R}^m\)</span>.</p>
</blockquote>
<p>Find a basis of <span class="math">\(\range L\)</span> where <span class="math">\(L(x_1, x_2, x_3) = (x_1 + x_2, 0, x_3)\)</span>:</p>
<blockquote>
<p>Clearly, <span class="math">\(L\left(\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}\right) = \begin{bmatrix} x_1 + x_2 \\ 0 \\ x_3 \end{bmatrix} = (x_1 + x_2)\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + x_3\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\set{\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}}\)</span> spans <span class="math">\(\range L\)</span>.<br />Clearly, this set is linearly independent. So it is a basis of <span class="math">\(\range L\)</span>.</p>
</blockquote>
<h2 id="kernel">Kernel</h2>
<p>The <strong>kernel</strong> of a linear mapping <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is the set of all vectors in the domain <span class="math">\(\mb{R}^n\)</span> that have an image of <span class="math">\(\vec{0}\)</span> under <span class="math">\(L\)</span> (<span class="math">\(L(\vec{x}) = 0\)</span>).</p>
<p>Formally, <span class="math">\(\ker L = \set{\vec{x} \in \mb{R}^n \middle| L(\vec{x}) = \vec{0}}\)</span>. Basically, it is the set of vectors that make the function produce the zero vector.</p>
<p>A vector <span class="math">\(\vec{x}\)</span> is in the kernel of the function if and only if <span class="math">\(L(\vec{x}) = \vec{0}\)</span>.</p>
<h3 id="theorem-3.3.2-1">Theorem 3.3.2</h3>
<p>If <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is linear, then <span class="math">\(\ker L\)</span> is a subspace of the domain, <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is linear.<br />By definition, <span class="math">\(\ker L \subseteq \mb{R}^n\)</span>.<br />Since <span class="math">\(L(\vec{0}) = \vec{0}\)</span>, <span class="math">\(\vec{0} \in \range L\)</span>.<br />Let <span class="math">\(\vec{y}, \vec{z} \in \ker L\)</span>.<br />Clearly, <span class="math">\(L(\vec{y} + \vec{z}) = L(\vec{y}) + L(\vec{z}) = \vec{0} + \vec{0} = \vec{0}\)</span>, so <span class="math">\(\vec{y} + \vec{z} \in \ker L\)</span>.<br />Clearly, <span class="math">\(L(c\vec{y}) = cL(\vec{y}) = c\vec{0} = \vec{0}\)</span> and <span class="math">\(\vec{y} \in \ker L\)</span>, so <span class="math">\(c\vec{y} \in \range L\)</span>.<br />So <span class="math">\(\ker L\)</span> satisfies the subspace test, and is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
</blockquote>
<p>Find a basis of the kernel of <span class="math">\(L(x_1, x_2, x_3) = (x_1 + x_2, 0, x_3)\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(\vec{x} \in \ker L\)</span>.<br />By definition, <span class="math">\(L(x_1, x_2, x_3) = (0, 0, 0)\)</span>.<br />So <span class="math">\(\begin{cases} x_1 + x_2 &amp;= 0 \\ x_3 &amp;= 0 \end{cases}\)</span>, and <span class="math">\(x_2 = -x_1\)</span>, so <span class="math">\(\vec{x} = \begin{bmatrix} x_1 \\ -x_1 \\ 0 \end{bmatrix} = x_1\begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}\)</span>.<br />Clearly, <span class="math">\(\set{\begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}}\)</span> spans <span class="math">\(\ker L\)</span> and is linearly independent, so it is a basis.</p>
</blockquote>
<h3 id="theorem-3.3.4">Theorem 3.3.4</h3>
<p>Given a linear mapping <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> with a standard matrix <span class="math">\(A = [L]\)</span>, <span class="math">\(\vec{x} \in \ker L \iff A\vec{x} = \vec{0}\)</span>.</p>
<p>The <strong>nullspace</strong> of a matrix <span class="math">\(A\)</span> is the set <span class="math">\(\operatorname{Null}(A) = \set{\vec{x} \in \mb{R}^n \middle| A\vec{x} = \vec{0}}\)</span>. In other words, the set of all vectors that make its corresponding linear mapping produce the zero vector.</p>
<p>Also, if <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is linear, then <span class="math">\(\ker L = \operatorname{Null}([L])\)</span>.</p>
<h3 id="theorem-3.3.5">Theorem 3.3.5</h3>
<p>Given a linear mapping <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> with a standard matrix <span class="math">\(A = [L] = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}\)</span>, then <span class="math">\(\range L = \spn \set{\vec{a}_1, \ldots, \vec{a}_n}\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(\vec{y} \in \range L\)</span>. Then <span class="math">\(\exists \vec{x} \in \mb{R}^n, L(\vec{x}) = \vec{y}\)</span>.<br />Clearly, <span class="math">\(\vec{y} = L(\vec{x}) = A\vec{x} = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}\vec{x} = x_1\vec{a}_1 + \ldots + x_n\vec{a}_n\)</span>.<br />Since <span class="math">\(\vec{x}\)</span> is arbitrary, <span class="math">\(x_1\vec{a}_1 + \ldots + x_n\vec{a}_n \in \spn \set{\vec{a}_1, \ldots, \vec{a}_n}\)</span>.<br />So <span class="math">\(\range L \subseteq \spn \set{\vec{a}_1, \ldots, \vec{a}_n}\)</span>.<br />Clearly, <span class="math">\(\spn \set{\vec{a}_1, \ldots, \vec{a}_n} \subseteq \range L\)</span>, so <span class="math">\(\range L = \spn \set{\vec{a}_1, \ldots, \vec{a}_n}\)</span>.</p>
</blockquote>
<p>The <strong>column space</strong> of a matrix <span class="math">\(A = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}\)</span> is <span class="math">\(\operatorname{Col}(A) = \set{A\vec{x} \middle| \vec{x} \in \mb{R}^n} = \spn \set{\vec{a}_1, \ldots, \vec{a}_n}\)</span>.</p>
<p>Also, if <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> is linear, then <span class="math">\(\range L = \operatorname{Col}([L])\)</span>.</p>
<p>The <strong>row space</strong> of a matrix <span class="math">\(A\)</span> is <span class="math">\(\operatorname{Row}(A) = \operatorname{Col}(A^T)\)</span>. It is simply the span of the rows of the matrix instead of the columns.</p>
<p>The <strong>left nullspace</strong> of a matrix <span class="math">\(A\)</span> is <span class="math">\(\operatorname{Null}(A^T)\)</span>. It is simply the nullspace of the matrix transposed.</p>
<h2 id="operations-on-linear-mappings">Operations on Linear Mappings</h2>
<p>Let <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> and <span class="math">\(M: \mb{R}^n \to \mb{R}^m\)</span> be linear mappings. Let <span class="math">\(t \in \mb{R}\)</span>.</p>
<p>Equality: <span class="math">\(L = M \iff \forall \vec{x} \in \mb{R}^n, L(\vec{x}) = M(\vec{x})\)</span>.</p>
<p>Addition/subtraction: we can define <span class="math">\(L \pm M: \mb{R}^n \to \mb{R}^m\)</span>, and <span class="math">\(\forall \vec{x} \in \mb{R}^n, (L \pm M)(\vec{x}) = L(\vec{x}) \pm M(\vec{x})\)</span>.</p>
<p>Scalar multiplication: we can define <span class="math">\(tL: \mb{R}^n \to \mb{R}^m\)</span>, and <span class="math">\(\forall \vec{x} \in \mb{R}^n, (tL)(\vec{x}) = tL(\vec{x})\)</span>.</p>
<p>The set <span class="math">\(\mb{L}\)</span> is the set of all possible linear mappings. The domain is <span class="math">\(\mb{R}^n\)</span> and the codomain is <span class="math">\(\mb{R}^m\)</span> for all linear mappings in this set.</p>
<h3 id="theorem-3.4.1">Theorem 3.4.1</h3>
<p><span class="math">\([L \pm M] = [L] \pm [M]\)</span> and <span class="math">\([tL] = t[L]\)</span>. In other words, addition and scalar multiplication over linear mappings are equivalent to the same operation applied to their standard matrices.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(\vec{x} \in \mb{R}^n\)</span>.<br />Clearly, <span class="math">\((L + M)(\vec{x}) = L(\vec{m}) + M(\vec{x}) = ([L] + [M])\vec{x}\)</span>.<br />So <span class="math">\([L + M] = [L] + [M]\)</span>.<br />Clearly, <span class="math">\((tL)(\vec{x}) = tL(\vec{x}) = (t[L])\vec{x}\)</span>.<br />So <span class="math">\([tL] = t[L]\)</span>.</p>
</blockquote>
<h3 id="theorem-3.4.2">Theorem 3.4.2</h3>
<p>Let <span class="math">\(L, M, N \in \mb{L}\)</span> and <span class="math">\(c, d \in \mb{R}\)</span>:</p>
<ul>
<li>Closure under addition: <span class="math">\(L + M \in \mb{L}\)</span>.</li>
<li>Associativity: <span class="math">\((L + M) + N = L + (M + N)\)</span>.</li>
<li>Commutativity: <span class="math">\(L + M = M + L\)</span>.</li>
<li>Additive identity: <span class="math">\(\exists O \in \mb{L}, L + O = L\)</span> (<span class="math">\(O(\vec{x}) = \vec{0} = O\vec{x}\)</span>).</li>
<li>Additive inverse: <span class="math">\(\exists (-L) \in \mb{L}, L + (-L) = O\)</span>.</li>
<li>Closure under scalar multiplication: <span class="math">\(cL \in \mb{L}\)</span>.</li>
<li><span class="math">\(c(dL) = (cd)L\)</span>.</li>
<li><span class="math">\((c + d)L = cL + dL\)</span>.</li>
<li><span class="math">\(c(L + M) = cL + cM\)</span>.</li>
<li>Multiplicative identity: <span class="math">\(1L = L\)</span>.</li>
</ul>
<h3 id="composition">Composition</h3>
<p>Given <span class="math">\(L: \mb{R}^n \to \mb{R}^m\)</span> and <span class="math">\(M: \mb{R}^m \to \mb{R}^p\)</span>, <span class="math">\(M \circ L = M(L(\vec{x})) = [M][L]\vec{x}\)</span>.</p>
<p>Also, <span class="math">\([M \circ L] = [M][L]\)</span>.</p>
<p>Find the standard matrix of <span class="math">\(R_\frac{\pi}{2} \circ R_\frac{3\pi}{2}\)</span>:</p>
<blockquote>
<p>Recall that <span class="math">\(R_\theta = \begin{bmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix}\)</span>.<br />So <span class="math">\(R_\frac{\pi}{2} \circ R_\frac{3\pi}{2} = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}\vec{x} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \vec{x}\)</span>.<br />So the standard matrix is <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span></p>
</blockquote>
<p>Note that this is the identity matrix. This is because we rotated the vector by 90 degrees, then 270 degrees - back to where we started.</p>
<p>The <strong>identity mapping</strong> <span class="math">\(I \in \mb{L}\)</span> is the mapping such that <span class="math">\(I(\vec{x}) = I\vec{x}\)</span>. Note that the standard matrix of this linear mapping is the identity matrix.</p>
<p>A linear mapping <span class="math">\(L\)</span> is <strong>invertible</strong> if and only if there exists another linear mapping <span class="math">\(M\)</span> such that <span class="math">\(L \circ M = I\)</span> (<span class="math">\(L(M(\vec{x})) = \vec{x}\)</span>). If <span class="math">\(M\)</span> exists, then <span class="math">\(M = L^{-1}\)</span>.</p>
<p>The standard matrix of <span class="math">\(M\)</span> is such that <span class="math">\([L][M] = I\)</span>. Note that in order for the inverse to exist, <span class="math">\(L\)</span> must therefore have the same number of rows as <span class="math">\(M\)</span> has columns.</p>
<p>In summary, the four <strong>fundamental subspaces</strong> of a matrix are:</p>
<ul>
<li>Row space</li>
<li>Column space</li>
<li>Nullspace</li>
<li>Left Nullspace</li>
</ul>
<h1 id="section-7">1/3/14</h1>
<h2 id="vector-spaces">Vector Spaces</h2>
<p>A <strong>real vector space</strong> is a set <span class="math">\(\mb{V}\)</span> of vectors put together with an addition operation, denoted <span class="math">\(\vec{x} \oplus \vec{y}\)</span>, and a scalar multiplication operation, denoted <span class="math">\(t \odot \vec{x}\)</span>, such that for any <span class="math">\(\vec{a}, \vec{b}, \vec{c} \in \mb{V}, u, v \in \mb{V}\)</span>:</p>
<ul>
<li>Closure under addition: <span class="math">\(\vec{a} \oplus \vec{b} \in \mb{V}\)</span></li>
<li>Associativity: <span class="math">\((\vec{a} \oplus \vec{b}) \oplus \vec{c} = \vec{a} \oplus (\vec{b} \oplus \vec{c})\)</span></li>
<li>Commutativity: <span class="math">\(\vec{a} \oplus \vec{b} = \vec{b} \oplus \vec{a}\)</span></li>
<li>Additive identity: <span class="math">\(\exists \vec{0} \in \mb{V}, \vec{a} \oplus \vec{0} = \vec{a}\)</span>. This is known as the zero vector.</li>
<li>Additive inverse: <span class="math">\(\exists (-\vec{a}) \in \mb{V}, \vec{a} \oplus (-\vec{a}) = \vec{0}\)</span></li>
<li>Closure under scalar multiplication: <span class="math">\(u \odot \vec{a} \in \mb{V}\)</span></li>
<li><span class="math">\(u \odot (v \odot \vec{a}) = (uv) \odot \vec{a}\)</span></li>
<li>Scalar distributivity: <span class="math">\((u + v) \odot \vec{a} = (u \odot \vec{a}) \oplus (v \odot \vec{a})\)</span></li>
<li>Vector distributivity: <span class="math">\(u \odot (\vec{a} \oplus \vec{b}) = u \odot \vec{a} \oplus u \odot \vec{b}\)</span></li>
<li>Scalar multiplicative identity: <span class="math">\(1 \odot \vec{a} = \vec{a}\)</span></li>
</ul>
<p>The &quot;vectors&quot; in a vector space don't necessarily have to be traditional vectors. They can also be things like polynomials or even sets.</p>
<p>Unlike with subspaces, we cannot assume that a few properties will imply the rest. So in order to prove that something is a vector space, we have to prove all ten properties are satisfied.</p>
<p>In this course, a <strong>vector space</strong> is the same thing as a real vector space. In practice, there are other types of vector spaces, like complex vector spaces.</p>
<p>We are using <span class="math">\(\oplus\)</span> and <span class="math">\(\odot\)</span> rather than <span class="math">\(+\)</span> and <span class="math">\(\cdot\)</span> because it makes it easier to see that the operations are not necessarily the same thing as real addition or multiplication - that they are non-standard.</p>
<p><span class="math">\(+\)</span> and <span class="math">\(\cdot\)</span> are called the <strong>standard addition</strong> and <strong>standard multiplication</strong>, respectively.</p>
<p>We can denote the zero vector of a vector space with <span class="math">\(\vec{0}_{\mb{V}}\)</span> to show which vector space it is in.</p>
<p>An example of a vector space is <span class="math">\(\mb{R}^n\)</span> with standard addition and standard multiplication. It is obvious that this satisfies all ten properties of the definition of a vector space.</p>
<p>Another example of a vector space is <span class="math">\(P_n(\mb{R})\)</span>, which is the set of all polynomials with degree less than or equal to <span class="math">\(n\)</span> with real coefficients. Here, the addition operation is standard addition of polynomials, and the scalar multiplication operation is the standard scalar multiplication of polynomials.</p>
<p>Another example of a vector space is <span class="math">\(M_{m \times n}\)</span>. Together with standard matrix multiplication and scalar multiplication of the matrices, this forms a vetor space. Likewise with linear mappings, with the standard linear mapping operations.</p>
<h3 id="theorem-4.1.1">Theorem 4.1.1</h3>
<p>If <span class="math">\(\mb{V}\)</span> is a vector space and <span class="math">\(\vec{v} \in \mb{V}\)</span>, then <span class="math">\(\vec{0} = 0 \odot \vec{v}\)</span> and <span class="math">\(-\vec{v} = (-1) \odot \vec{v}\)</span>.</p>
<p>In other words, scalar multiplication by 0 results in the additive identity and scalar multiplication by -1 results in the additive inverse.</p>
<p>Using this, we can always find the additive identity by picking any vector and multiplying by 0, and always find the additive inverse by multiplying by -1.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, <span class="math">\(0 \odot \vec{v} = 0 \odot \vec{v} \oplus \vec{0} = 0 \odot \vec{v} \oplus 1\vec{v} \oplus (-\vec{v}) = (0 + 1) \odot \vec{v} \oplus (-\vec{v}) = \vec{v} \oplus (-\vec{v}) = \vec{0}\)</span>.<br />Clearly, <span class="math">\((-1) \odot \vec{v} = \vec{0} \oplus (-1) \odot \vec{v} = 1\vec{v} + (-\vec{v}) \oplus (-1) \odot \vec{v} = 0\vec{v} + (-\vec{v}) = -\vec{1\vec{v} + (-\vec{v}) \oplus (-1) \odot \vec{v}}\)</span>.</p>
</blockquote>
<p>Prove that <span class="math">\(\vec{D} = \set{\vec{x} \in \mb{R} \middle| x &gt; 0}\)</span>, <span class="math">\(x, y \in \mb{D}\)</span>, where <span class="math">\(x \oplus y = xy\)</span> and <span class="math">\(t \odot x = x^t\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(x, y, z \in \mb{D}, c, d \in \mb{R}\)</span>.<br />So <span class="math">\(xy &gt; 0\)</span>, and <span class="math">\(x \oplus y \in \mb{D}\)</span>.<br />So <span class="math">\((x \oplus y) \oplus z = (xy)z = x(yz) = x \oplus (y \oplus z)\)</span>.<br />So <span class="math">\(x \oplus y = xy = yx = y \oplus x\)</span>.<br />So <span class="math">\(\vec{0} = 0 \odot x = x^0 = 1\)</span>, by theorem 4.1.1.<br />So <span class="math">\(-x = -1 \odot x = x^{-1} = \frac{1}{x}\)</span>, by theorem 4.1.1.<br />So <span class="math">\(x^c &gt; 0\)</span>, and <span class="math">\(c \odot x \in \mb{D}\)</span>.<br />So <span class="math">\(c \odot (d \cdot x) = x^{cd} = x^{dc} = d \odot (c \odot x)\)</span>.<br />So <span class="math">\((c + d) \odot x = x^{c + d} = x^c x^d = c \odot x \oplus d \odot x\)</span>.<br />So <span class="math">\(c \odot (x \oplus y) = (xy)^c = x^c y^c = c \odot x \oplus c \odot y\)</span>.<br />So <span class="math">\(1 \odot x = x^1 = x\)</span>.<br />So all ten properties are satisfied and this is a vector space.</p>
</blockquote>
<p>Note that all <span class="math">\(\vec{x} \in \mb{R}\)</span> can be related to <span class="math">\(\vec{y} \in \mb{D}\)</span> using <span class="math">\(\vec{y} = a^\vec{x}\)</span>, where <span class="math">\(a &gt; 1\)</span>.</p>
<p>For example, the empty set is not a vector space because all vector spaces must have at least one element, the zero vector.</p>
<h3 id="subspaces-1">Subspaces</h3>
<p>A set <span class="math">\(\mb{S}\)</span> is a <strong>subspace</strong> of <span class="math">\(\mb{V}\)</span> if and only if:</p>
<ul>
<li><span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{V}\)</span>.</li>
<li><span class="math">\(\mb{S}\)</span> is a vector space using the same operations as <span class="math">\(\mb{V}\)</span>.</li>
</ul>
<p>Since we know more about <span class="math">\(\mb{S}\)</span> and <span class="math">\(\mb{V}\)</span> - that they are vector spaces - we can again use the <strong>subspace test</strong> to see if some <span class="math">\(\mb{S}\)</span> is a subspace of some <span class="math">\(\mb{V}\)</span>.</p>
<h3 id="theorem-4.1.2-subspace-test-for-vector-spaces">Theorem 4.1.2 (Subspace Test for Vector Spaces)</h3>
<p>A set <span class="math">\(\mb{S}\)</span> is a subspace of a set <span class="math">\(\mb{V}\)</span> if and only if, for any <span class="math">\(\vec{x}, \vec{y} \in \mb{S}, t \in \mb{R}\)</span>:</p>
<ul>
<li><span class="math">\(\mb{S}\)</span> is not empty (this allows us to instantiate <span class="math">\(\vec{x}\)</span> and <span class="math">\(\vec{y}\)</span>). We can test this by checking if the zero vector is in the set.</li>
<li><span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{V}\)</span>.</li>
<li>Closure under addition: <span class="math">\(\vec{x} \oplus \vec{y} \in \mb{S}\)</span>.</li>
<li>Closure under scalar multiplication: <span class="math">\(t \odot \vec{x} \in \mb{S}\)</span>.</li>
</ul>
<p>The proof of this is roughly the same as the proof for the subspace test for <span class="math">\(\mb{R}\)</span>.</p>
<h3 id="spanning">Spanning</h3>
<p>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> where <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k \in \mb{V}\)</span> and <span class="math">\(\mb{V}\)</span> is a vector space.</p>
<p>The <strong>span</strong> of <span class="math">\(\mathcal{B}\)</span> is <span class="math">\(\spn \mathcal{B} = \set{c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k \middle| c_1, \ldots, c_k \in \mb{R}}\)</span>.</p>
<p>This is just a generalization of the concept of a span in <span class="math">\(\mb{R}^n\)</span> into all vector spaces.</p>
<p>As with subspaces in <span class="math">\(\mb{R}^n\)</span>, we can prove a vector <span class="math">\(\vec{x}\)</span> is in a span if the linear system <span class="math">\(\vec{x} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k\)</span> has at least one solution. Here, we are solving for <span class="math">\(c_1, \ldots, c_k\)</span>.</p>
<h3 id="linear-independence-1">Linear Independence</h3>
<p>If <span class="math">\(c_1 \odot \vec{v}_1 \oplus \ldots \oplus c_k \odot \vec{v}_k = \vec{0}_{\mb{V}}\)</span> has any solutions where <span class="math">\(c_i \ne 0\)</span> for <span class="math">\(1 \le i \le k\)</span>, then <span class="math">\(\mathcal{B}\)</span> is <strong>linearly dependent</strong>. Otherwise, it is <strong>linearly independent</strong>.</p>
<p>This is just a generalization of the concept of linear independence in <span class="math">\(\mb{R}^n\)</span> into all vector spaces.</p>
<p>Also, we can find which vectors are causing the set to be linearly dependent by following these steps:</p>
<ol style="list-style-type: decimal">
<li>Write the solution to <span class="math">\(c_1 \odot \vec{v}_1 \oplus \ldots \oplus c_k \odot \vec{v}_k = \vec{0}_{\mb{V}}\)</span> as a vector equation in the form of <span class="math">\(\begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix} = a_1 \vec{v}_1 + \ldots + a_n \vec{v}_n, a_1, \ldots, a_n \in \mb{R}\)</span>.</li>
<li>For each <span class="math">\(\vec{v}_i, 1 \le i \le n\)</span>, let <span class="math">\(\vec{c} = \vec{v}_i\)</span> by setting <span class="math">\(a_i = 1\)</span> and all other coefficients to 0.</li>
<li>For each <span class="math">\(\vec{c} = \vec{v}_i\)</span>, write each <span class="math">\(c_i, 1 \le i \le n\)</span> as a linear equation. For example, <span class="math">\(c_4 = c_1 + c_2 + c_3\)</span>.</li>
<li>Find the vectors that can be written in terms of the other vectors.</li>
<li>These are the vectors that can be removed to make the set linearly independent.</li>
</ol>
<h3 id="theorem-4.1.3">Theorem 4.1.3</h3>
<p>Then <span class="math">\(\spn \mathcal{B}\)</span> is a subspace of <span class="math">\(\mb{V}\)</span>.</p>
<p>In other words, the span of a set of vectors that are in a vector space is always a subspace of that vector space.</p>
<h3 id="theorem-4.1.4">Theorem 4.1.4</h3>
<p>Then if for some <span class="math">\(1 \le i \le k\)</span>, <span class="math">\(v_i \in \spn \set{\vec{v}_1, \ldots, \vec{v}_{i - 1}, \vec{v}_{i + 1}, \ldots, \vec{v}_k}\)</span>, <span class="math">\(\spn \mathcal{B} = \spn \set{\vec{v}_1, \ldots, \vec{v}_{i - 1}, \vec{v}_{i + 1}, \ldots, \vec{v}_k}\)</span>.</p>
<p>In other words, if a vector in a set can be written as a linear combination of the other vectors in the set (it is in the span), then the span of the set is the same as the span of the set without that vector.</p>
<h3 id="theorem-4.1.5">Theorem 4.1.5</h3>
<p>Then <span class="math">\(\mathcal{B}\)</span> is linearly dependent if and only if for some <span class="math">\(1 \le i \le k\)</span>, <span class="math">\(v_i \in \spn \set{\vec{v}_1, \ldots, \vec{v}_{i - 1}, \vec{v}_{i + 1}, \ldots, \vec{v}_k}\)</span>.</p>
<h3 id="theorem-4.1.6">Theorem 4.1.6</h3>
<p>If <span class="math">\(\vec{0}_{\mb{V}} \in \mathcal{B}\)</span>, then <span class="math">\(\mathcal{B}\)</span> is linearly independent.</p>
<h3 id="bases-1">Bases</h3>
<p>A set <span class="math">\(\mathcal{B}\)</span> is a <strong>basis</strong> for the vector space <span class="math">\(\mb{V}\)</span> if and only if it spans <span class="math">\(\mb{V}\)</span> and is linearly independent.</p>
<p>In other words, given <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> and a vector space <span class="math">\(\mb{V}\)</span>, <span class="math">\(\mathcal{B}\)</span> is a basis for <span class="math">\(\mb{V}\)</span> if and only if <span class="math">\(\spn \mathcal{B} = \mb{V}\)</span> and <span class="math">\(\mb{B}\)</span> is linearly independent.</p>
<p>Also, the span of an empty set is <span class="math">\(\vec{0}_\mb{V}\)</span>.</p>
<p>The vectors in a set that is a basis are known as <strong>basis vectors</strong> and are often given by <span class="math">\(\vec{e}_i\)</span> where <span class="math">\(` \le i \le n\)</span>. The basis can then be represented as <span class="math">\(\mathcal{B} = \set{\vec{e}_1, \ldots, \vec{e}_n}\)</span>.</p>
<p>We can find bases given a vector space <span class="math">\(\mb{V}\)</span> by using the following steps:</p>
<ol style="list-style-type: decimal">
<li>Write <span class="math">\(\vec{v} \in \mb{V}\)</span> in its most general form.</li>
<li>Write <span class="math">\(\vec{v}\)</span> as a linear combination of vectors <span class="math">\(\vec{v}_1, \ldots, \vec{v}_n \in \mb{V}\)</span>.</li>
<li>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span>.</li>
<li>Remove vectors from <span class="math">\(\mathcal{B}\)</span> until it is linearly independent.</li>
<li><span class="math">\(\mathcal{B}\)</span> is now a basis for <span class="math">\(\mb{V}\)</span>.</li>
</ol>
<p>Find a basis for the subspace <span class="math">\(\mb{P} = \set{a + bx + cx^2 \in P_2(\mb{R}) \middle| a + c = b}\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(f(x) \in \mb{P}\)</span>. Clearly, <span class="math">\(f(x) = a + bx + cx^2\)</span> where <span class="math">\(a + c = b\)</span>.<br />Then <span class="math">\(f(x) = a + (a + c)x + cx^2 = a(1 + x) + c(x + x^2)\)</span>.<br />So <span class="math">\(f(x)\)</span> is a linear combination of <span class="math">\(1 + x \in \mb{P}\)</span> and <span class="math">\(x + x^2 \in \mb{P}\)</span>, so <span class="math">\(\spn \set{1 + x, x + x^2} = \mb{P}\)</span>.<br />Clearly, <span class="math">\(\set{1 + x, x + x^2}\)</span> is lienarly independent, so it is a basis for <span class="math">\(\mb{P}\)</span>.</p>
</blockquote>
<h3 id="unique-representation-theorem">Unique Representation Theorem</h3>
<p>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> be a basis for vector space <span class="math">\(\mb{V}\)</span>.</p>
<p>Then for every <span class="math">\(\vec{v} \in \mb{V}\)</span> can be written as a unique linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span> (there is only one of these linear combinations).</p>
<blockquote>
<p>Since <span class="math">\(\mathcal{B}\)</span> spans <span class="math">\(\mb{V}\)</span>, by definition <span class="math">\(\exists c_1, \ldots, c_n \in \mb{R}, c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{v}\)</span>.<br />Let <span class="math">\(\vec{v} = d_1 \vec{v}_1 + \ldots + d_n \vec{v}_n\)</span> for some <span class="math">\(d_1, \ldots, d_n \in \mb{R}\)</span>.<br />Clearly, <span class="math">\(d_1 \vec{v}_1 + \ldots + d_n \vec{v}_n = c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n\)</span>, so <span class="math">\((c_1 - d_1) \vec{v}_1 + \ldots + (c_n - d_n) \vec{v}_n = \vec{0}_\mb{V}\)</span>.<br />Since <span class="math">\(\mathcal{B}\)</span> is linearly independent, the only solution is <span class="math">\(c_1 - d_1 = \ldots = c_n - d_n = 0\)</span>, the trivial solution.<br />So <span class="math">\(c_i = d_i\)</span> for all <span class="math">\(1 \le i \le n\)</span> and the linear combination is unique.</p>
</blockquote>
<p>For example, consider the polynomials vector space, <span class="math">\(P_n(\mb{R})\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(\mathcal{B} = \set{1, x, x^2, \ldots, x^n}\)</span>.<br />Clearly, <span class="math">\(\spn \mathcal{B} = P_n(\mb{R})\)</span>.<br />Clearly, <span class="math">\(\mathcal{B}\)</span> is linearly independent because <span class="math">\(c_1 1 + \ldots c_n x^n = 0 + \ldots + 0x^n \implies c_1 = \ldots = c_n = 0\)</span>.<br />In fact, we define the standard basis of <span class="math">\(P_n(\mb{R})\)</span> to be <span class="math">\(\set{1, x, x^2, \ldots, x^n}\)</span>.</p>
</blockquote>
<h2 id="dimension">Dimension</h2>
<h3 id="theorem-4.2.1">Theorem 4.2.1</h3>
<p>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> be a basis for vector space <span class="math">\(\mb{V}\)</span>.</p>
<p>If <span class="math">\(\set{\vec{w}_1, \ldots, \vec{w}_k}\)</span> is linearly independent, then <span class="math">\(k \le n\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Suppose <span class="math">\(k &gt; n\)</span>.<br />Clearly, <span class="math">\(\forall 1 \le i \le k, \vec{w}_i = a_{i, 1} \vec{v}_1 + \ldots + a_{i, n} \vec{v}_n\)</span>.<br />So <span class="math">\(c_1 \vec{w}_1 + \ldots + c_k \vec{w}_k = \vec{0}_\mb{V} = c_1 (a_{1, 1} \vec{v}_1 + \ldots + a_{1, n}) + \ldots + c_k (a_{k, 1} \vec{v}_1 + \ldots + a_{k, n}) = (c_1 a_{1, 1} + \ldots + c_k a_{k, 1}) \vec{v}_1 + \ldots + (c_1 a_{1, n} + \ldots + c_k a_{k, n}) \vec{v}_1\)</span>.<br />Since <span class="math">\(\mathcal{B}\)</span> is linearly independent, the only solution for the coefficients of <span class="math">\(\vec{v}_i\)</span> is the trivial solution, which is <span class="math">\(\begin{cases} c_1 a_{1, 1} + \ldots + c_k a_{k, 1} &amp;= 0 \\ \vdots \\ c_1 a_{1, n} + \ldots + c_k a_{k, n} &amp;= 0 \end{cases}\)</span>.<br />This solution is itself a system of linear equations that we can represent with the <span class="math">\(n \times k\)</span> coefficient matrix <span class="math">\(A\)</span>.<br />Clearly, <span class="math">\(\rank A \le n\)</span> since <span class="math">\(A\)</span> has <span class="math">\(k\)</span> columns, so there are infinite solutions.<br />So <span class="math">\(c_1 \vec{w}_1 + \ldots + c_k \vec{w}_k = \vec{0}_\mb{V}\)</span> has infinite solutions.<br />This is a contradiction because the set is linearly independent. Therefore, <span class="math">\(k &lt; n\)</span>.</p>
</blockquote>
<h3 id="theorem-4.2.2">Theorem 4.2.2</h3>
<p>If <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> and <span class="math">\(\set{\vec{w}_1, \ldots, \vec{w}_k}\)</span> are both bases in <span class="math">\(\mb{V}\)</span>, then <span class="math">\(k = n\)</span>.</p>
<p>This is easily proves using theorem 4.2.1.</p>
<h3 id="definition">Definition</h3>
<p>The <strong>dimension</strong> of a vector space <span class="math">\(\mb{V}\)</span> is the number of elements in a basis <span class="math">\(\mathcal{B}\)</span> of <span class="math">\(\mb{V}\)</span>.</p>
<p>In other words, if <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> is a basis for <span class="math">\(\mb{V}\)</span>, then the dimension of <span class="math">\(\mb{V}\)</span> is <span class="math">\(\dim \mb{V} = n\)</span> and <span class="math">\(\mb{V}\)</span> is an <span class="math">\(n\)</span>-dimensional vector space.</p>
<p>Since <span class="math">\(\set{\vec{0}_\mb{V}}\)</span> has only the basis <span class="math">\(\emptyset\)</span>, the dimension is <span class="math">\(\dim \set{\vec{0}_\mb{V}} = 0\)</span>.</p>
<p>If the basis of a vector space has infinite elements, then the vector space is <strong>infinite-dimensional</strong>.</p>
<p>For example:</p>
<ul>
<li><span class="math">\(\dim \mb{R}^n = n\)</span> since the standard basis is <span class="math">\(\set{\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \ldots, \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{bmatrix}}\)</span>.</li>
<li><span class="math">\(\dim M_{m \times n}(\mb{R}) = mn\)</span> since the standard basis is a set of <span class="math">\(mn\)</span> matrices with a single 1 in unique locations.</li>
<li><span class="math">\(\dim P_n(\mb{R}) = n + 1\)</span> since the standard basis is <span class="math">\(\set{1, x, x^2, \ldots, x^n}\)</span>.</li>
</ul>
<p>To find the dimension of a vector space, we simply need to find a basis.</p>
<p>For example, find <span class="math">\(\dim \mb{W}\)</span> where <span class="math">\(\mb{W} = \set{p(x) \in P_3(\mb{R}) \middle| p(3) = 0}\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(p(x) \in \mb{W}\)</span>. Then <span class="math">\(p(3) = 0\)</span>.<br />So <span class="math">\(\exists a, b, c \in \mb{R}, p(x) = (x - 3)(a + bx + cx^2) = a(x - 3) + bx(x - 3) + cx^2(x - 3) = a(x - 3) + b(x^2 - 3x) + c(x^3 - 3x^2)\)</span>.<br />So <span class="math">\(\set{x - 3, x^2 - 3x, x^3 - 3x^2}\)</span> spans <span class="math">\(\mb{W}\)</span> and since it is linearly independent, it is a basis for <span class="math">\(\mb{W}\)</span>.<br />So <span class="math">\(\dim \mb{W} = 3\)</span>, since there are 3 elements in the basis.</p>
</blockquote>
<h3 id="theorem-4.2.3">Theorem 4.2.3</h3>
<p>Given a vector space <span class="math">\(\mb{V}\)</span>:</p>
<ul>
<li>A set with more than <span class="math">\(\dim \mb{V}\)</span> elements is linearly dependent.</li>
<li>A set with less than <span class="math">\(\dim \mb{V}\)</span> elements cannot span <span class="math">\(\mb{V}\)</span>.</li>
<li>Given a set <span class="math">\(\mathcal{B}\)</span> with <span class="math">\(n\)</span> elements, <span class="math">\(\mathcal{B}\)</span> spans <span class="math">\(\mb{V}\)</span> if and only if <span class="math">\(\mathcal{B}\)</span> is linearly independent.</li>
</ul>
<p>;wip: prove this</p>
<p>Let <span class="math">\(A_1 = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}, A_2 = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}\)</span>. Extend <span class="math">\(\set{A_1, A_2}\)</span> to be a basis for <span class="math">\(M_{2 \times 2}(\mb{R})\)</span>:</p>
<blockquote>
<p>Clearly, <span class="math">\(\set{E_1, E_2, E_3, E_4}\)</span> spans <span class="math">\(M_{2 \times 2}(\mb{R})\)</span> where <span class="math">\(E_i\)</span> is the <span class="math">\(i\)</span>th standard basis matrix (the <span class="math">\(i\)</span>th entry in the matrix is 1 and all the others are 0).<br />So <span class="math">\(\set{A_1, A_2, E_1, \ldots, E_4}\)</span> spans <span class="math">\(M_{2 \times 2}(\mb{R})\)</span>. Let <span class="math">\(A \in M_{2 \times 2}(\mb{R})\)</span>.<br />Then <span class="math">\(\exists c_1, c_2, c_3, c_4, c_5, c_6 \in \mb{R}, A = c_1 \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix} + c_2 \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix} + c_3 \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix} + c_4 \begin{bmatrix} 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix} + c_5 \begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 0 \end{bmatrix} + c_6 \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span>.<br />The augmented matrix for this system is <span class="math">\(\left[\begin{array}{cccccc|c} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ \end{array}\right]\)</span>, and the RREF is <span class="math">\(\left[\begin{array}{cccccc|c} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\ \end{array}\right]\)</span>.<br />So <span class="math">\(\begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \\ c_6 \end{bmatrix} = s\begin{bmatrix} -1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix} + t\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \\ -1 \\ 1 \end{bmatrix}, s, t \in \mb{R}\)</span>.<br />So <span class="math">\(\begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \\ c_6 \end{bmatrix} = \begin{bmatrix} -1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}\)</span> and <span class="math">\(\begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \\ c_6 \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \\ -1 \\ 1 \end{bmatrix}\)</span> are solutions.<br />From this we determine that <span class="math">\(E_2 = A_1 - E_1\)</span> and <span class="math">\(E_4 = A_1 - A_2 + E_3\)</span>.<br />So <span class="math">\(\spn \set{A_1, A_2, E_1, \ldots, E_4} = \spn \set{A_1, A_2, E_1, E_3}\)</span>.<br />Clearly, <span class="math">\(\set{A_1, A_2, E_1, E_3}\)</span> is linearly independent and spans <span class="math">\(M_{2 \times 2}(\mb{R})\)</span>, so it is a basis.</p>
</blockquote>
<h3 id="theorem-4.2.4">Theorem 4.2.4</h3>
<p>Given <span class="math">\(n\)</span>-dimensional vector space <span class="math">\(\mb{V}\)</span> and linearly independent set <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> where <span class="math">\(k \le n\)</span>, then there exist <span class="math">\(n - k\)</span> vectors <span class="math">\(\vec{w}_{k + 1}, \ldots, \vec{w}_n\)</span> such that <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_k, \vec{w}_{k + 1}, \ldots, \vec{w}_n}\)</span> is a basis for <span class="math">\(\mb{V}\)</span>.</p>
<p>Also, all linearly independent sets of vectors in <span class="math">\(\mb{V}\)</span> with <span class="math">\(n\)</span> elements are bases for <span class="math">\(\mb{V}\)</span>.</p>
<p>Also, if <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{V}\)</span>, then <span class="math">\(\dim \mb{S} \le \dim \mb{V}\)</span>.</p>
<h2 id="coordinates">Coordinates</h2>
<p>Let <span class="math">\(\mb{V}\)</span> be an <span class="math">\(n\)</span>-dimensional vector space. Let <span class="math">\(\mathcal{B} = \set{\vec{e}_1, \ldots, \vec{e}_n}\)</span> be a basis for <span class="math">\(\mb{V}\)</span>. Let <span class="math">\(\vec{x} \in \mb{V}\)</span>.</p>
<p>Then <span class="math">\(\exists c_1, \ldots, c_n \in \mb{R}, \vec{x} = c_1 \vec{e}_1 + \ldots + c_n \vec{e}_n\)</span>.</p>
<p>The <strong>coordinate vector</strong> of <span class="math">\(\vec{x}\)</span> with respect to <span class="math">\(\mathcal{B}\)</span> is defined as <span class="math">\([\vec{x}]_\mathcal{B} = \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}\)</span>. The <strong>coordinates</strong> are simply <span class="math">\(c_1, \ldots, c_n\)</span> - the values of the variables themselves.</p>
<p>A <span class="math">\(\mathcal{B}\)</span>-coordinate is a coordinate with respect to <span class="math">\(\mathcal{B}\)</span>.</p>
<p>Note that <span class="math">\(\vec{x}\)</span> is not the same thing as <span class="math">\([\vec{x}]_\mathcal{B}\)</span> - <span class="math">\(\vec{x}\)</span> can be whatever type of thing exists in the vector space, like polynomials or matrices, but <span class="math">\([\vec{x}]_\mathcal{B}\)</span> is always a vector in <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Note that all coordinates are relative to a vector, so it is important to know which basis a coordinate vector is written with respect to. The order of the basis vectors matter too, so we must give the vectors in the basis a particular fixed order.</p>
<h3 id="theorem-4.3.2">Theorem 4.3.2</h3>
<p>For all <span class="math">\(\vec{v}, \vec{w} \in \mb{V}, s, t \in \mb{R}\)</span>, <span class="math">\([s\vec{v} + t\vec{w}]_\mathcal{B} = s[\vec{v}]_\mathcal{B} + t[\vec{w}]_\mathcal{B}\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, <span class="math">\(\exists b_1, \ldots, b_n \in \mb{R}, \vec{v} = b_1 \vec{e}_1 + \ldots + b_n \vec{e}_n\)</span>.<br />Clearly, <span class="math">\(\exists c_1, \ldots, c_n \in \mb{R}, \vec{w} = c_1 \vec{e}_1 + \ldots + c_n \vec{e}_n\)</span>.<br />So <span class="math">\(s\vec{v} + t\vec{w} = (sb_1 + tc_1)\vec{v}_1 + \ldots + (sb_n + tc_n)\vec{v}_n\)</span>.<br />So <span class="math">\([s\vec{v} + t\vec{w}] = \begin{bmatrix} sb_1 + tc_1 \\ \vdots \\ sb_n + tc_n \end{bmatrix}_\mathcal{B} = \begin{bmatrix} sb_1 + tc_1 \\ \vdots \\ sb_n + tc_n \end{bmatrix}_\mathcal{B} = s\begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix} + t\begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix} = s[\vec{v}] + t[\vec{w}]\)</span>.</p>
</blockquote>
<h3 id="change-of-coordinates">Change of Coordinates</h3>
<p>Sometimes, we want to change coordinates from one basis to another.</p>
<p>Let <span class="math">\(\mb{V}\)</span> be an <span class="math">\(n\)</span>-dimensional vector space. Let <span class="math">\(\vec{x} \in \mb{V}\)</span>.</p>
<p>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> and <span class="math">\(\mathcal{C}\)</span> be bases for <span class="math">\(\mb{V}\)</span>.</p>
<p>Given a coordinate with respect to <span class="math">\(\mathcal{B}\)</span>, we want to find a coordinate with respect to <span class="math">\(\mathcal{C}\)</span> that represents the same vector. In other words, given <span class="math">\([\vec{x}]_\mathcal{B}\)</span>, we want to quickly find <span class="math">\([\vec{x}]_\mathcal{C}\)</span>.</p>
<p>Clearly, <span class="math">\(\exists b_1, \ldots, b_n \in \mb{R}, \vec{x} = b_1 \vec{v}_1 + \ldots + b_n \vec{v}_n\)</span>. So <span class="math">\([\vec{x}]_\mathcal{B} = \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}\)</span>.</p>
<p>Clearly, <span class="math">\([\vec{x}]_\mathcal{C} = [b_1 \vec{v}_1 + \ldots + b_n \vec{v}_n]_\mathcal{C} = b_1 [\vec{v}_1]_\mathcal{C} + \ldots + b_n [\vec{v}_n]_\mathcal{C}\)</span>.</p>
<p>We can write this using vectors or matrices: <span class="math">\([\vec{x}]_\mathcal{C} = \begin{bmatrix} [\vec{v}_1]_\mathcal{C} \\ \vdots \\ [\vec{v}_n]_\mathcal{C} \end{bmatrix} \cdot [\vec{x}]_\mathcal{B} = \begin{bmatrix} [\vec{v}_1]_\mathcal{C} &amp; \ldots &amp; [\vec{v}_n]_\mathcal{C} \end{bmatrix} [\vec{x}]_\mathcal{B}\)</span>.</p>
<p>The matrix <span class="math">\({}_\mathcal{C}P_\mathcal{B} = \begin{bmatrix} [\vec{v}_1]_\mathcal{C} &amp; \ldots &amp; [\vec{v}_n]_\mathcal{C} \end{bmatrix}\)</span> is called the <strong>change of coordinates matrix/change of basis matrix</strong> from <span class="math">\(\mathcal{B}\)</span> to <span class="math">\(\mathcal{C}\)</span>.</p>
<p>This is the matrix that satisfies <span class="math">\([\vec{x}]_\mathcal{C} = {}_\mathcal{C}P_\mathcal{B} [\vec{x}]_\mathcal{B}\)</span>.</p>
<h3 id="theorem-4.3.3">Theorem 4.3.3</h3>
<p><span class="math">\({}_\mathcal{C}P_\mathcal{B} {}_\mathcal{B}P_\mathcal{C} = I = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B}\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(A = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B}\)</span>.<br />Then <span class="math">\(A[\vec{x}]_\mathcal{B} = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B} [\vec{x}]_\mathcal{B} = {}_\mathcal{B}P_\mathcal{C} [\vec{x}]_\mathcal{C} = [\vec{x}]_\mathcal{B}\)</span>.<br />So <span class="math">\(A[\vec{x}]_\mathcal{B} = [\vec{x}]_\mathcal{B}\)</span>, so <span class="math">\(A = {}_\mathcal{B}P_\mathcal{C} {}_\mathcal{C}P_\mathcal{B} = I\)</span>.<br />The same technique can be used for for <span class="math">\({}_\mathcal{C}P_\mathcal{B} {}_\mathcal{B}P_\mathcal{C} = I\)</span>.</p>
</blockquote>
<p>Given <span class="math">\(\mathcal{B} = \set{1, x, x^2}, \mathcal{C} = \set{1, x + 1, (x + 1)^2}\)</span>, find <span class="math">\({}_\mathcal{C}P_\mathcal{B}\)</span>:</p>
<blockquote>
<p>Clearly, <span class="math">\({}_\mathcal{C}P_\mathcal{B} = \begin{bmatrix} [1]_\mathcal{C} &amp; [x]_\mathcal{C} &amp; [x^2]_\mathcal{C} \end{bmatrix}\)</span>.<br />Clearly, <span class="math">\([1]_\mathcal{C} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, [x]_\mathcal{C} = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}, [x^2]_\mathcal{C} = \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}\)</span>, by inspection.<br />So <span class="math">\({}_\mathcal{C}P_\mathcal{B} = \begin{bmatrix} 1 &amp; -1 &amp; 1 \\ 0 &amp; 1 &amp; -2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>.</p>
</blockquote>
<h1 id="section-8">17/3/14</h1>
<h2 id="inverses-of-matrices">Inverses of Matrices</h2>
<p>Let <span class="math">\(A \in M_{m \times n}, B, C \in M_{n \times m}\)</span>.</p>
<p>Then <span class="math">\(B\)</span> is the <strong>right inverse</strong> of <span class="math">\(A\)</span> if and only if <span class="math">\(AB = I_m\)</span>.</p>
<p>Then <span class="math">\(C\)</span> is the <strong>left inverse</strong> of <span class="math">\(A\)</span> if and only if <span class="math">\(CA = I_n\)</span></p>
<p>A matrix <span class="math">\(A\)</span> is <strong>square</strong> if and only if <span class="math">\(A \in M_{n \times n}\)</span>.</p>
<h3 id="finding-inverses">Finding Inverses</h3>
<p>Suppose we wanted to find the right inverse <span class="math">\(B\)</span> of <span class="math">\(A\)</span>.</p>
<p>Clearly, <span class="math">\(B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_n \end{bmatrix}\)</span>. So <span class="math">\(AB = I = A\begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_n \end{bmatrix} = \begin{bmatrix} A\vec{b}_1 &amp; \ldots &amp; A\vec{b}_n \end{bmatrix}\)</span>.</p>
<p>So <span class="math">\(A\vec{b}_i = \vec{e}_i\)</span>. We can then find the inverse by solving each system.</p>
<p>There is also a faster way. We can simply put the system <span class="math">\(\sys{A}{I}\)</span> into RREF and read off the inverse directly from the resulting matrix.</p>
<p>In fact, this is a useful method for solving multiple systems of linear equations with the same coefficients but different right hand sides at the same time.</p>
<p>For example, find the right inverse of <span class="math">\(A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span>:</p>
<blockquote>
<p>Clearly, <span class="math">\(\left[\begin{array}{cc|cc} 1 &amp; 2 &amp; 1 &amp; 0 \\ 3 &amp; 4 &amp; 0 &amp; 1 \\ \end{array}\right]\)</span> in RREF is <span class="math">\(\left[\begin{array}{cc|cc} 1 &amp; 0 &amp; -2 &amp; 1 \\ 0 &amp; 1 &amp; \frac{3}{2} &amp; -\frac{1}{2} \\ \end{array}\right]\)</span>.<br />So the inverse is <span class="math">\(\begin{bmatrix} -2  1 \\ \frac{3}{2} &amp; -\frac{1}{2} \end{bmatrix}\)</span>. To verify, <span class="math">\(\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} -2  1 \\ \frac{3}{2} &amp; -\frac{1}{2} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span>, as required.</p>
</blockquote>
<h3 id="theorem-5.1.1">Theorem 5.1.1</h3>
<p>If <span class="math">\(A \in M_{m \times n}\)</span> where <span class="math">\(m &gt; n\)</span>, then <span class="math">\(A\)</span> cannot have a right inverse.</p>
<p>If <span class="math">\(A \in M_{m \times n}\)</span> where <span class="math">\(m &lt; n\)</span>, then <span class="math">\(A\)</span> cannot have a left inverse.</p>
<h3 id="inverse-matrices">Inverse Matrices</h3>
<p>Let <span class="math">\(A\)</span> be an <span class="math">\(n \times n\)</span> square matrix. If <span class="math">\(BA = I = AB\)</span>, then <span class="math">\(B\)</span> is the <strong>inverse</strong> of <span class="math">\(A\)</span>. If <span class="math">\(B\)</span> exists, then <span class="math">\(A\)</span> is <strong>invertible</strong>.</p>
<p>The inverse of the matrix is denoted <span class="math">\(B = A^{-1}\)</span> - <span class="math">\(B\)</span> is the inverse of <span class="math">\(A\)</span>.</p>
<p>In other words, only square matrices can have inverses, but not all of them do.</p>
<h3 id="theorem-5.1.3">Theorem 5.1.3</h3>
<p>The inverse of a matrix is always unique.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(B, C\)</span> be inverses of <span class="math">\(A\)</span>. Then <span class="math">\(B = BI = B(AC) = (BA)C = IC = C\)</span>.</p>
</blockquote>
<h3 id="theorem-5.1.4">Theorem 5.1.4</h3>
<p>If <span class="math">\(A, B \in M_{n \times n}\)</span> such that <span class="math">\(AB = I\)</span>, then <span class="math">\(A = B^{-1}\)</span> and <span class="math">\(B = A^{-1}\)</span>, and <span class="math">\(BA = I\)</span>.</p>
<p>Also, the RREF of <span class="math">\(A\)</span>, and the RREF of <span class="math">\(B\)</span>, are both <span class="math">\(I\)</span>.</p>
<p>Basically, if a square matrix has a left or right inverse, then this is also the right or left inverse, and is simply the inverse of the matrix.</p>
<p>So we can simply use the algorithm for finding left or right inverses to find the multiplciative inverse of the matrix.</p>
<p>So if <span class="math">\(A\)</span> is invertible, then row reducing <span class="math">\(\sys{A}{I}\)</span> results in <span class="math">\(\sys{I}{A^{-1}}\)</span> - <strong>row reducing the system results in the inverse</strong>.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(B\vec{x} = \vec{0}\)</span>. Then <span class="math">\(\vec{0} = A\vec{0} = A(B\vec{x}) = (AB)\vec{x} = I\vec{x} = \vec{x}\)</span>.<br />So <span class="math">\(\vec{x} = 0\)</span> and <span class="math">\(\vec{x}\)</span> is the unique solution to the system. Since the solution is unique, <span class="math">\(\rank B = n\)</span> and there is a leading one in each column of the RREF, so the RREF of <span class="math">\(B\)</span> is <span class="math">\(I\)</span>.<br />Since the RREF of <span class="math">\(B\)</span> is <span class="math">\(I\)</span>, <span class="math">\(\forall \vec{b} \in \mb{R}^n, \exists \vec{x} \in \mb{R}^n, B\vec{x} = \vec{b}\)</span>.<br />Clearly, <span class="math">\(BA\vec{b} = (BA)(B\vec{x}) = B(AB)\vec{x} = BI\vec{x} = B\vec{x} = \vec{b}\)</span>.<br />By theorem 3.1.4, <span class="math">\(BA = I\)</span>.<br />Now the same argument used to prove <span class="math">\(\rank B = n\)</span> can then be used to probe <span class="math">\(\rank A = n\)</span>.</p>
</blockquote>
<h3 id="theorem-5.1.5">Theorem 5.1.5</h3>
<p>If the RREF of a matrix <span class="math">\(A \in M_{n \times n}\)</span> is <span class="math">\(I\)</span>, then the <span class="math">\(A^{-1}\)</span> exists.</p>
<p>In other words, if <span class="math">\(\rank A = n\)</span>, then <span class="math">\(A\)</span> is invertible.</p>
<p>Find the general inverse of <span class="math">\(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span>:</p>
<blockquote>
<p>Clearly, <span class="math">\(\left[\begin{array}{cc|cc} a &amp; b &amp; 1 &amp; 0 \\ c &amp; d &amp; 0 &amp; 1 \\ \end{array}\right]\)</span> in RREF is <span class="math">\(\left[\begin{array}{cc|cc} 1 &amp; 0 &amp; \frac{d}{ad - bc} &amp; -\frac{b}{ad - bc} \\ 0 &amp; 1 &amp; -\frac{c}{ad - bc} &amp; \frac{a}{ad - bc} \\ \end{array}\right]\)</span>.<br />So <span class="math">\(A^{-1} = \begin{bmatrix} \frac{d}{ad - bc} &amp; -\frac{b}{ad - bc} \\ -\frac{c}{ad - bc} &amp; \frac{a}{ad - bc} \end{bmatrix}\)</span>, and is defined whenever <span class="math">\(ad - bc \ne 0\)</span>.</p>
</blockquote>
<h3 id="theorem-5.1.6">Theorem 5.1.6</h3>
<p>Given <span class="math">\(A, B \in M_{n \times n}\)</span> such that <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are invertible and <span class="math">\(k \in \mb{R}\)</span>:</p>
<ul>
<li><span class="math">\((kA)^{-1} = \frac{1}{k}A^{-1}\)</span></li>
<li><span class="math">\((AB)^{-1} = B^{-1}A^{-1}\)</span></li>
<li><span class="math">\((A^T)^{-1} = (A^{-1})^T\)</span></li>
</ul>
<p>Proof:</p>
<p>(proof of first)</p>
<blockquote>
<p>Clearly, <span class="math">\(kA(kA)^{-1} = I\)</span>. Clearly, <span class="math">\(kA\frac{1}{k}A^{-1} = AA^{-1} = I\)</span>.<br />Since the inverse is always unique, <span class="math">\((kA)^{-1} = \frac{1}{k}A^{-1}\)</span>.</p>
</blockquote>
<p>(proof of second)</p>
<blockquote>
<p>Clearly, <span class="math">\(AB(AB)^{-1} = I\)</span>. Clearly, <span class="math">\(ABB^{-1}A^{-1} = AIA^{-1} = I\)</span>.<br />Since the inverse is always unique, <span class="math">\((AB)^{-1} = B^{-1}A^{-1}\)</span>.</p>
</blockquote>
<p>(proof of third)</p>
<blockquote>
<p>Clearly, <span class="math">\(A^T(A^T)^{-1} = I\)</span>. Clearly, <span class="math">\(A^T(A^{-1})^T = (A^{-1}A)^T = I^T = I\)</span>.<br />Since the inverse is always unique, <span class="math">\((A^T)^{-1} = (A^{-1})^T\)</span>.</p>
</blockquote>
<h3 id="theorem-5.1.7-invertible-matrix-theorem">Theorem 5.1.7 (Invertible Matrix Theorem)</h3>
<p>Given <span class="math">\(A \in M_{n \times n}\)</span>, all of the following are equivalent:</p>
<ul>
<li><span class="math">\(A\)</span> is invertible.</li>
<li>The RREF of <span class="math">\(A\)</span> is <span class="math">\(I\)</span>.</li>
<li><span class="math">\(\rank A = n\)</span>.</li>
<li>The system of equations <span class="math">\(A\vec{x} = \vec{b}\)</span> has unique solutions for all <span class="math">\(\vec{b} \in \mb{R}^n\)</span>.</li>
<li>The nullspace of <span class="math">\(A\)</span> is <span class="math">\(\set{\vec{0}}\)</span>.</li>
<li>The rows or columns of <span class="math">\(A\)</span> form a basis for <span class="math">\(\mb{R}^n\)</span>.</li>
<li><span class="math">\(A^T\)</span> is invertible.</li>
</ul>
<p>Proof of third:</p>
<blockquote>
<p>Clearly, <span class="math">\(\rank A = n\)</span> if and only if the RREF of <span class="math">\(A\)</span> is <span class="math">\(I\)</span>.<br />Clearly, the RREF is <span class="math">\(I\)</span> if and only if there are a sequence of elementary matrices <span class="math">\(E_k \cdots E_1\)</span> such that <span class="math">\(E_k \cdots E_1 = I\)</span>.<br />Clearly, this the inverse, so <span class="math">\(A\)</span> is invertible if and only if <span class="math">\(E_k \cdots E_1\)</span> exist.</p>
</blockquote>
<p>Proof of fourth:</p>
<blockquote>
<p>Clearly, <span class="math">\(A\vec{x} = \vec{b}\)</span> has unique solutions for all <span class="math">\(\vec{b} \in \mb{R}^n\)</span> if and only if there are no free variables.<br />Clearly, there are no free variables if and only if <span class="math">\(\rank A = n\)</span>.</p>
</blockquote>
<p>Proof of fifth:</p>
<blockquote>
<p>Clearly, the nullspace of <span class="math">\(A\)</span> is <span class="math">\(\set{\vec{0}}\)</span> if and only if <span class="math">\(\vec{x} = \vec{0}\)</span> is the only solution to <span class="math">\(A\vec{x} = \vec{0}\)</span>.<br />Clearly, the solution is unique if and only if <span class="math">\(\rank A = n\)</span>.</p>
</blockquote>
<p>Proof of sixth:</p>
<blockquote>
<p>Clearly, the rows or columns of <span class="math">\(A form a basis for \)</span>^n$ if and only if they span <span class="math">\(\mb{R}^n\)</span> and are linearly independent.<br />Since the rows or columns are linearly independent, the nullspace of <span class="math">\(A\)</span> is <span class="math">\(\set{\vec{0}}\)</span>.</p>
</blockquote>
<p>Proof of seventh:</p>
<blockquote>
<p>Clearly, <span class="math">\(A^T\)</span> is invertible if and only if <span class="math">\((A^{-1})^T\)</span> exists.<br />CLearly, <span class="math">\((A^{-1})^T\)</span> exists if and only if <span class="math">\(A\)</span> is invertible.</p>
</blockquote>
<p>Also note that <span class="math">\(A\vec{x} = \vec{b}\)</span> if and only if <span class="math">\(A^{-1}A\vec{x} = \vec{x} = A^{-1}\vec{b}\)</span>.</p>
<p>In other words, we can solve linear systems this way. If <span class="math">\(A\vec{x} = \vec{b}\)</span>, then <span class="math">\(\vec{x} = A^{-1}\vec{b}\)</span>, and the converse is true as well.</p>
<h2 id="elementary-matrices">Elementary Matrices</h2>
<p>When we found the inverse of a matrix <span class="math">\(A\)</span> by row reducing <span class="math">\(#\sys{A}{I}\)</span> into <span class="math">\(\sys{I}{A^{-1}}\)</span>, we applied a series of elementary row operations in a certain order, and this resulted in finding <span class="math">\(A^{-1}\)</span>.</p>
<p>Since we can, given any <span class="math">\(A^{-1}\)</span>, find <span class="math">\(A\)</span> again, those elementary row operations must have been encoded in <span class="math">\(A^{-1}\)</span> somehow.</p>
<p>An <strong>elementary matrix</strong> is the result of applying an elementary row operation on the <span class="math">\(n \times n\)</span> identity matrix. The elementary matrix <strong>represents</strong> the elementary row operation.</p>
<h3 id="theorem-5.2.1-5.2.3">Theorem 5.2.1-5.2.3</h3>
<p>Let <span class="math">\(A \in M_{m \times n}\)</span>. Let <span class="math">\(E\)</span> be the elementary matrix representing an elementary row operation (<span class="math">\(R_i + cR_j, c \ne 0, i \ne j\)</span>, <span class="math">\(cR_i, c \ne 0\)</span>, or <span class="math">\(R_i \leftrightarrow R_j, i \ne j\)</span>).</p>
<p>Then <span class="math">\(EA\)</span> is the matrix obtained by applying that elementary row operation directly onto <span class="math">\(A\)</span>.</p>
<p>In other words, applying elementary row operations to identity matrices allow the matrices to represent the operations in a more elegant way.</p>
<p>Since the elementary row operations cannot change the rank of the matrix, <span class="math">\(\rank EA = \rank A\)</span></p>
<p>Since all elementary row operations are reversible, likewise all elementary matrices are invertible. In fact, the inverse of an elementary matrix is the reverse of the elementar row operation applied to the identity matrix.</p>
<p>For example, <span class="math">\(E = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -5 \end{bmatrix}\)</span> represents the elementary row operation <span class="math">\(-5R_2\)</span>, and the reverse operation is <span class="math">\(-\frac{1}{5}R_2\)</span>. So <span class="math">\(E^{-1} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -\frac{1}{5} \end{bmatrix}\)</span>.</p>
<h2 id="matrix-decomposition">Matrix Decomposition</h2>
<h3 id="theorem-5.2.5">Theorem 5.2.5</h3>
<p>If <span class="math">\(A\)</span> is an <span class="math">\(m \times n\)</span> matrix, and <span class="math">\(R\)</span> is the RREF of <span class="math">\(A\)</span>, then <span class="math">\(\exists E_1, \ldots, E_k \in M_{m \times m}, E_k \cdots E_1 A = R\)</span>.</p>
<p>Also, if <span class="math">\(E_k \cdots E_1 A = R\)</span>, then <span class="math">\(A = E_1^{-1} \cdots E_k^{-1} R\)</span>.</p>
<p>This can be derived by multiplying both sides of <span class="math">\(E_k \cdots E_1 A = R\)</span> by <span class="math">\(E_k^{-1}, \ldots, E_1^{-1}\)</span> repeatedly.</p>
<p>Convert <span class="math">\(A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span> into RREF using elementary matrices:</p>
<blockquote>
<p>Clearly, <span class="math">\(A = \begin{bmatrix} 1 &amp; 0 \\ -3 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 \\ 0 &amp; -2 \end{bmatrix}\)</span> Clearly, <span class="math">\(\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 2 \\ 0 &amp; -2 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -2 \end{bmatrix}\)</span> Clearly, <span class="math">\(\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; -\frac{1}{2} \end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -2 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; -\frac{1}{2} \end{bmatrix} \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ -3 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span>.</p>
</blockquote>
<h3 id="inverses">Inverses</h3>
<p>If <span class="math">\(A is invertible, then the RREF \)</span>R$ is <span class="math">\(I\)</span>, so <span class="math">\(E_k \cdots E_1 A = I\)</span> and <span class="math">\(A = E_1^{-1} \cdots E_k^{-1} I\)</span>.</p>
<p>So <span class="math">\(E_k \cdots E_1\)</span> is the left inverse of <span class="math">\(A\)</span>, by definition. So <span class="math">\(A^{-1} = E_k \cdots E_1\)</span>. Also, <span class="math">\(A = E_1^{-1} \cdots E_k^{-1}\)</span>.</p>
<p>In other words, if <span class="math">\(A\)</span> is invertible, then <span class="math">\(A and \)</span>A^{-1}$ can both be written as products of elementary matrices.</p>
<p>Also, since we are row reducing <span class="math">\(\sys{A}{I}\)</span> into <span class="math">\(\sys{I}{A^{-1}}\)</span>, and <span class="math">\(I = E_k \cdots E_1 A\)</span>, then <span class="math">\(A^{-1} = E_k \cdots E_1 I = E_k \cdots E_1\)</span>. In other words, we are getting the inverse written in terms of elementary row operations.</p>
<h2 id="matrix-determinant">Matrix Determinant</h2>
<p>The <strong>determinant</strong> of a matrix is an expression over the values within the matrix. The value of the determinant provides important information about it, such as whether it is invertible. The determinant <strong>only applies to square matrices</strong>.</p>
<p>We denote the determinant of a matrix <span class="math">\(A\)</span> as <span class="math">\(\det A = \abs{\begin{array}{ccc} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \end{array}}\)</span>. Importantly, <span class="math">\(A\)</span> is invertible if and only if <span class="math">\(\det A \ne 0\)</span>.</p>
<p>The determinant of a <span class="math">\(1 \times 1\)</span> matrix is <span class="math">\(\det \begin{bmatrix} a \end{bmatrix} = a\)</span>. This is because the matrix is invertible if and only if <span class="math">\(a \ne 0\)</span>.</p>
<p>The determinant of the <span class="math">\(2 \times 2\)</span> matrix <span class="math">\(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span> is <span class="math">\(\det A = ad - bc\)</span>.</p>
<hr>
<p>Copyright 2013 Anthony Zhang</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>.
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>