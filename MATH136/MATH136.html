<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>MATH136 | Anthony Zhang</title>
  <style type="text/css">
  body {
    font-family: "Segoe UI", Verdana, Arial, Helvetica, sans-serif;
    background: #fffefe;
    padding: 5em;
  }
  
  pre {
    margin-left: 2em;
  }
  
  code {
    border: solid 1px black;
    background: #665555;
    color: white;
    padding: 0.1em;
    border-radius: 0.3em;
    display: inline-block;
  }
  
  pre code {
    padding: 1em;
    border-radius: 0.5em;
  }
  
  h1 {
    font-size: 4em;
  }
  
  table {
    margin: 0 auto;
  }
  
  td, th {
    padding: 0.5em;
    border: 1px solid grey;
  }
  
  tr {
    padding:: 0;
  }
  
  a.button {
    display: inline-block;
    padding: 1em;
    font-family: monospace;
    color: black;
    text-decoration: none;
    border: 0.2em solid black;
    border-radius: 0.5em;
    background: white;
  }
  
  a.button:hover, a.button:focus, a.button:active {
    background: black;
    color: white;
  }
  </style>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript">MathJax.Hub.Queue(["Typeset",MathJax.Hub]);</script>
</head>
<body>
<a class="button" href="..">&#8666; Return to University Notes index</a>
<h1 id="math136">MATH136</h1>
<p>Linear Algebra</p>
<pre><code>Instructor: Dan Wolczuk
Section 081 (online)
Email: dstwolcz@uwaterloo.ca</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\]</span></p>
<h1 id="im-going-to-stop-procrastinating-this-week-honest">13/1/14 (I'm going to stop procrastinating this week, honest!)</h1>
<h2 id="notation">Notation</h2>
<p>We represent vectors as column vectors (matrices of size <span class="math">\(n \times 1\)</span>) to distinguish them from points: <span class="math">\(\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}, v_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>However, sometimes we will also write them in <span class="math">\(n\)</span>-tuple form (for example, <span class="math">\((3, 7, 2)\)</span>)</p>
<p><span class="math">\(\mb{R}^n, n \in \mathbb{Z}, n &gt; 0\)</span> is the set of all elements of the form <span class="math">\((x_1, \ldots, x_n), x_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>We usually think of these elements as points, but now we will think of them as vectors, abstract objects that we can perform operations on, like adding and subtracting.</p>
<p>This is called an <span class="math">\(n\)</span>-dimensional Euclidean space, and is the set of all vectors that are of length (dimension) <span class="math">\(n\)</span>. For example, <span class="math">\(\mb{R}^3\)</span> (arr-three) is the set of all 3D vectors, and <span class="math">\(\vec{0} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}\)</span>.</p>
<p>So <span class="math">\(\mb{R}^3 = \set{\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \middle| x_1, x_2, x_3 \in \mathbb{R}}\)</span>.</p>
<p>Likewise, <span class="math">\(\mb{R}^n = \set{\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \middle| x_1, \ldots, x_n \in \mathbb{R}}\)</span>.</p>
<p>It might also occasionally be useful to think of these vectors as points, where the vectors are offsets from the origin. For example, a function <span class="math">\(f\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\right) = f(x_1, \ldots, x_n)\)</span>.</p>
<h2 id="operations">Operations</h2>
<p>Let <span class="math">\(\vec{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}, \vec{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, c \in \mb{R}\)</span>.</p>
<h3 id="equality">Equality</h3>
<p><span class="math">\(x = y\)</span> if and only if <span class="math">\(\forall 1 \le i \le n, x_i = y_i\)</span>.</p>
<p>In other words, two vectors are equal if and only if all their components are equal. Note that this is defined only for vectors with the same size (number of components).</p>
<h3 id="additionsubtraction">Addition/Subtraction</h3>
<p><span class="math">\[x + y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} + \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{bmatrix}\]</span></p>
<p>Also, <span class="math">\(\vec{x} - \vec{y} = \vec{x} + (-\vec{y})\)</span>. Therefore, <span class="math">\(x - y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} - \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 - y_1 \\ \vdots \\ x_n - y_n \end{bmatrix}\)</span>.</p>
<h3 id="scalar-multiplication">Scalar Multiplication</h3>
<p><span class="math">\[cx = c\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} cx_1 \\ \vdots \\ cx_n \end{bmatrix}\]</span></p>
<p>If <span class="math">\(c = -1\)</span>, <span class="math">\(c\vec{x} = -x\)</span>.</p>
<h2 id="linear-combination">Linear Combination</h2>
<p>A <strong>linear combination</strong> of vectors <span class="math">\(\vec{v}_1, \ldots, \vec{v}_n\)</span> is the value <span class="math">\(L = c_1 v_1 + \ldots + c_n v_n, c_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>It is a linear expression with each term containing a coefficient and one vector value - a sum of scalar multiples.</p>
<h3 id="theorem-1.1.1">Theorem 1.1.1</h3>
<p>If <span class="math">\(\vec{x}, \vec{y}, \vec{w} \in \mathbb{R}^n, c, d \in \mathbb{R}\)</span>, then:</p>
<ul>
<li>Closure under addition: <span class="math">\(\vec{x} + \vec{y} \in \mathbb{R}^n\)</span>.</li>
<li>Associativity: <span class="math">\((\vec{x} + \vec{y}) + \vec{w} = \vec{x} + (\vec{y} + \vec{w})\)</span></li>
<li>Commutativity: <span class="math">\(\vec{x} + \vec{y} = \vec{y} + \vec{x}\)</span></li>
<li>Additive identity: <span class="math">\(\exists \vec{0} \in \mathbb{R}, \vec{x} + \vec{0} = \vec{x}\)</span></li>
<li>Additive inverse: <span class="math">\(\forall \vec{x} \in \mathbb{R}^n, \exists -\vec{x} \in \mathbb{R}^n, \vec{x} + (-\vec{x}) = \vec{0}\)</span>.</li>
<li><span class="math">\(c\vec{x} \in \mathbb{R}^n\)</span></li>
<li><span class="math">\(c(d\vec{x}) = (cd)\vec{x}\)</span></li>
<li>Scalar distributivity: <span class="math">\((c + d)\vec{x} = c\vec{x} + d\vec{x}\)</span></li>
<li>Vector distributivity: <span class="math">\(c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}\)</span></li>
<li>Multiplicative identity: <span class="math">\(1\vec{x} = \vec{x}\)</span></li>
</ul>
<p>Closure under addition means that when we add two elements in the set, the resulting sum is also in the set.</p>
<p>Closure under scalar multiplication means that when we multiply an element in the set by a scalar, the resulting product is also in the set.</p>
<p>Because these Euclidean spaces are closed under addition and scalar multiplication, they are closed under linear combinations - all linear combinations of vectors in <span class="math">\(\mb{R}^n\)</span> are also in <span class="math">\(\mb{R}^n\)</span>.</p>
<h2 id="span">Span</h2>
<p>The <strong>span</strong> of a set of vectors <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> is the set of all linear combinations of the vectors in the set.</p>
<p>In other words, <span class="math">\(\spn \mathcal{B} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k} = \set{c_1 v_1 + \ldots + c_n v_n \middle| c_1, \ldots, c_n \in \mathbb{R}}\)</span>.</p>
<p>A span is always a subset of the space it is in. In other words, <span class="math">\(\spn \mathcal{B} \subseteq \mb{R}^n\)</span>.</p>
<p>A span is just a set. Therefore, we can say that a set <span class="math">\(\mathcal{B}\)</span> <strong>spans</strong> another set <span class="math">\(\mathcal{C}\)</span> if and only if <span class="math">\(\mathcal{C}\)</span> is exactly the set of all linear combinations of the vectors in <span class="math">\(\mathcal{B}\)</span>.</p>
<p>The <strong>vector equation</strong> of a set of vectors is the generalized equation for the linear combinations of each element in the set. For example, <span class="math">\(\vec{x} = c_1 v_1 + \ldots + c_n v_n\)</span> is the vector equation for <span class="math">\(\mathcal{B}\)</span>.</p>
<h3 id="theorem-1.1.2">Theorem 1.1.2</h3>
<p>If <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>, then <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
<p>In other words, if a vector in a set of vectors can already be represented by a linear combination of other vectors in the set, it doesn't affect the span at all.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>.<br />So <span class="math">\(\exists c_1, \ldots, c_k \in \mathbb{R}, \vec{v}_{k + 1} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k\)</span>.<br />We want to show that <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.<br />Let <span class="math">\(\vec{x}\)</span> be an arbitrary element of <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}\)</span>.<br />So <span class="math">\(\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} \vec{v}_{k + 1}\)</span>.<br />So <span class="math">\(\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} (c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k) = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = (d_1 + d_{k + 1} c_1) \vec{v}_1 + \ldots + (d_k + d_{k + 1} c_k) \vec{v}_k\)</span>.<br />Clearly, <span class="math">\(x \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.<br />Clearly, <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}\)</span>.<br />Therefore, <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
</blockquote>
<p>Consider <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}}\)</span>. Clearly, <span class="math">\(\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}\)</span>.</p>
<p>Clearly, <span class="math">\(\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}}\)</span>, so <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}} = \spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}}\)</span> represents a line in 3D.</p>
<p>The span of the zero vector is itself. The span of a linearly independent vector is a line. The span of two linearly independent vectors is a plane. The span of 3 linearly independent vectors is a 3D space. The span of 4 linearly independent vectors is a 4D space, and so on.</p>
<h3 id="linear-independence">Linear Independence</h3>
<p>When we say <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>, we can also write <span class="math">\(\vec{v}_{k + 1} \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
<p>Note that <span class="math">\(\vec{0}\)</span> can always be written as a linear combination of any vectors in the same space. The zero vector exists in all dimensions.</p>
<p>It is important to determine which vectors can be written as linear combinations of others.</p>
<p>A set of vectors is <strong>linearly independent</strong> if and only if the vectors in the set cannot be written as linear combinations of each other. Otherwise, the set is <strong>linearly dependent</strong>.</p>
<p>In other words, <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = c_k \vec{v}_k\)</span>. Rearranging, we get <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} - c_k \vec{v}_k + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = \vec{0}\)</span>.</p>
<h3 id="theorem-1.1.3">Theorem 1.1.3</h3>
<p>Therefore, a set of vectors <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> is linearly dependent if and only if <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_n vec{v}_n = \vec{0}\)</span>, and otherwise linearly independent.</p>
<p>The solution where <span class="math">\(c_i = 0, 1 \le i \le n\)</span> is called the trivial solution, because all the coefficients are 0. The trivial solution is always a solution to the above equation.</p>
<p>In other words, the set is linearly independent if and only if the zero vector cannot be written as a linear combination of the vectors in the set, except when all the coefficients are 0.</p>
<p>To prove that a set is linearly independent, we need to prove that the only solution to the above equation is the trivial solution.</p>
<p>To prove that a set is linearly independent, we need to prove that there is a solution to the above equation where at least one of the coefficients is non-aero.</p>
<p>When we find a solution to the equation, we can use this to find the vectors that are in the span of all the others - the vectors that can be written as a linear comnbination of the others. In the solution, <strong>all vectors with non-zero coefficients can be written as linear combinations of all the other vectors</strong>. If we rearrange the solved equation, we can find this linear combination.</p>
<h3 id="theorem-1.1.4">Theorem 1.1.4</h3>
<p>Note that any set of vectors that contains <span class="math">\(\vec{0}\)</span> is linearly dependent.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> and <span class="math">\(\vec{v}_i = \vec{0}, 1 \le i \le n\)</span>.<br /><span class="math">\(\mathcal{B}\)</span> is linearly dependent if and only if <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0}, c_1 \cdot \ldots \cdot c_n = 0\)</span>.<br />Construct <span class="math">\(c_1, \ldots, c_{i - 1}, c_{i + 1}, \ldots, c_n = 0, c_i = 1\)</span>.<br />Then <span class="math">\(c_1 \cdot \ldots \cdot c_n = 0\)</span> and <span class="math">\(c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0}\)</span>.<br />Therefore, <span class="math">\(\mathcal{B}\)</span> is linearly dependent.</p>
</blockquote>
<p>The vector equation of a linearly independent set is known as a simplified vector equation. A vector equation can be converted into a simplified one by removing terms where the vector can be written as a linear combination of the others.</p>
<p>Geometrically, two vectors span a plane if they are not parallel, and three vectors span a 3D space if they do not all lie on the same plane.</p>
<h2 id="bases">Bases</h2>
<p>A linearly independent set of vectors is always simpler than a linearly dependent one. A linearly dependent set of vectors can always be converted into a linearly dependent one with the same span. Therefore, the simplest set of vectors that spans a given set is always linearly independent.</p>
<p>Given <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span>, <span class="math">\(\mathcal{B}\)</span> is a <strong>basis</strong> for <span class="math">\(\spn \mathcal{B}\)</span> if <span class="math">\(\mathcal{B}\)</span> is linearly independent.</p>
<p>In other words, a basis for a set is a linearly independent set of vectors such that its span is exactly the set.</p>
<p>The basis for <span class="math">\(\set{\vec{0}}\)</span> is the empty set (<span class="math">\(\emptyset\)</span>).</p>
<p>The <strong>standard basis</strong> is a basis that is easy to write for any number of dimensions. The standard basis is of the form <span class="math">\(\set{\vec{e}_1, \ldots, \vec{e}_n}\)</span>, where <span class="math">\(\vec{e}_i\)</span> has all components set to 0, except for the <span class="math">\(i\)</span>-th component, for <span class="math">\(1 \le i \le n\)</span>.</p>
<p>For example, the standard basis of <span class="math">\(\mb{R}^4\)</span> is <span class="math">\(\set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}}\)</span>.</p>
<p>In order to prove that a set is the basis of another, we need to prove that it is linearly independent, and that it spans the set (by proving that an arbitrary vector in the second set can be written as a linear combination of the elements of the first).</p>
<p>Prove that <span class="math">\(\mathcal{B} = \set{\begin{bmatrix} 1 \\ 3 \end{bmatrix}, \begin{bmatrix} -1 \\ -1 \end{bmatrix}}\)</span> is a basis for <span class="math">\(\mb{R}^2\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2\)</span>.<br />We will find the coefficients we want by solving <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span> for <span class="math">\(c_1, c_2\)</span>.<br />Construct <span class="math">\(c_1 = \frac{x_2 - x_1}{2}, c_2 = \frac{x_2 - 3x_1}{2}\)</span>.<br />Clearly, <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \frac{x_2 - x_1}{2} \begin{bmatrix} 1 \\ 3 \end{bmatrix} + \frac{x_2 - 3x_1}{2} \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\exists c_1, c_2 \in \mathbb{R}, \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\mathcal{B}\)</span> spans <span class="math">\(\mb{R}^2\)</span>.<br />Note that there is only the trivial solution when <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span>, so <span class="math">\(\mathcal{B}\)</span> is also linearly independent.<br />Therefore, <span class="math">\(\mathcal{B}\)</span> is a basis for <span class="math">\(\mb{R}^2\)</span>.</p>
</blockquote>
<p>A basis for <span class="math">\(\mb{R}^n\)</span> has exactly <span class="math">\(n\)</span> elements.</p>
<h2 id="shapes">Shapes</h2>
<p>Points are conceptually applicable in any dimension above 0.</p>
<p>Lines are conceptually applicable in any dimension above 1.</p>
<p>Planes are conceptually applicable in any dimension above 2.</p>
<p>A line in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + \vec{b}, c_1 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \vec{b} \in \mathbb{R}^n\)</span>. This is a line in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>.</p>
<p>A plane in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + c_2 \vec{v}_2 + \vec{b}, c_1, c_2 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \vec{v}_2, \vec{b} \in \mathbb{R}^n\)</span> and <span class="math">\(\set{\vec{v}_1, \vec{v}_2}\)</span> being a linearly independent set. This is a plane in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>.</p>
<p>A <span class="math">\(k\)</span>-plane in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k + \vec{b}, c_1, c_2 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k, \vec{b} \in \mathbb{R}^n\)</span> and <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> being a linearly independent set. This is a <span class="math">\(k\)</span>-plane in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>. A <span class="math">\(k\)</span>-plane is a <span class="math">\(k\)</span>-dimensional plane.</p>
<p>A hyperplane is a <span class="math">\(k\)</span>-plane, where <span class="math">\(k = n - 1\)</span>. It is an <span class="math">\(n - 1\)</span>-dimensional plane.</p>
<p>For example, <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ −2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 1 \\ −1 \end{bmatrix}}\)</span> defines a hyperplane in <span class="math">\(\mb{R}^n\)</span>, since the set is linearly independent.</p>
<h1 id="ok-so-its-like-week-3-and-im-behind-i-get-it">20/1/14 (OK, so it's like week 3 and I'm behind, I get it)</h1>
<h2 id="subspaces">Subspaces</h2>
<p>A <strong>subspace</strong> of <span class="math">\(\mb{R}^n\)</span> is a non-empty subset <span class="math">\(\mb{S}\)</span> of <span class="math">\(\mb{R}^n\)</span> such that it satisfies all ten properties defined in Theorem 1.1.1.</p>
<p>All subspaces are <strong>spaces</strong>, which are subsets of a vector space.</p>
<p>But since properties 2-5 and 7-10 follow from properties 1 and 6, all we need to do to prove all ten properties is to prove properties 1 and 6 hold.</p>
<h3 id="theorem-1.2.1-subspace-test">Theorem 1.2.1 (Subspace Test)</h3>
<p>Given a set <span class="math">\(\mb{S}\)</span>, <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span> if and only if:</p>
<ul>
<li><span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{R}^n\)</span>.</li>
<li><span class="math">\(\mb{S}\)</span> is non-empty.</li>
<li><span class="math">\(\forall \vec{x}, \vec{y} \in \mb{S}, \vec{x} + \vec{y} \in \mb{S}\)</span>.</li>
<li><span class="math">\(\forall \vec{x} \in \mb{S}, c \in \mb{R}, c\vec{x} \in \mb{S}\)</span></li>
</ul>
<p>To prove a set is a subspace, we need to prove all four properties.</p>
<p>Clearly, if conditions 2 and 4 hold, <span class="math">\(\vec{0} \in \mb{S}\)</span>. So if <span class="math">\(\vec{0} \notin \mb{S}\)</span>, then one or both of the conditions is not met and the set is not a subspace.</p>
<p>We can use this to check if a set is a subspace by seeing if <span class="math">\(\vec{0}\)</span> is in it.</p>
<p>Prove <span class="math">\(\mb{S} = \set{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} \middle| x + y = 0, x - z = 0}\)</span> is a subspace of <span class="math">\(\mb{R}^3\)</span> and write a basis for the subspace:</p>
<blockquote>
<p>Clearly, <span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{R}^3\)</span>.<br />Clearly, <span class="math">\(\vec{0} \in \mb{S}\)</span>, so the set is non-empty.<br />Let <span class="math">\(\vec{a}, \vec{b} \in \mb{S}\)</span>.<br />So <span class="math">\(a_1 + a_2 = a_1 - a_3 = 0\)</span> and <span class="math">\(b_1 + b_2 = b_1 - b_3 = 0\)</span>.<br />Clearly, <span class="math">\((a_1 + b_1) + (a_2 + b_2) = (a_1 + b_1) - (a_3 + b_3) = 0\)</span>.<br />So <span class="math">\(\vec{a} + \vec{b} \in \mb{S}\)</span> and <span class="math">\(\mb{S}\)</span> is closed under addition.<br />Clearly, <span class="math">\(c(a_1 + a_2) = c(a_1 - a_3) = c0\)</span>.<br />So <span class="math">\(c\vec{a} \in \mb{S}\)</span> and <span class="math">\(\mb{S}\)</span> is closed under scalar multiplication.<br />So <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^3\)</span>.<br />Since <span class="math">\(a_1 + a_2 = a_1 - a_3 = 0\)</span>, <span class="math">\(a_2 = -a_1\)</span> and <span class="math">\(a_3 = a_1\)</span>.<br />So the general vector is <span class="math">\(\vec{v}(a_1) = \begin{bmatrix} a_1 \\ -a_1 \\ a_1 \end{bmatrix} = a_1 \begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}\)</span>.<br />So a basis for <span class="math">\(\mb{S}\)</span> is <span class="math">\(\set{\begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}}\)</span>.</p>
</blockquote>
<p>We find a basis of a subspace by finding the general form of a vector in the subspace, using this to find a spanning set for <span class="math">\(S\)</span>, and then reducing it into a linearly independent set.</p>
<h3 id="theorem-1.2.2">Theorem 1.2.2</h3>
<p>If <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k \in \mb{R}^n\)</span>, then <span class="math">\(\mb{S} = \span \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, the set is a subset of <span class="math">\(\mb{R}^n\)</span>.<br />Clearly, <span class="math">\(\vec{0} \in \mb{S}\)</span>, since <span class="math">\(\vec{0} = 0\vec{v}_1 + \ldots + 0\vec{v}_k\)</span>.<br />Let <span class="math">\(\vec{a}, \vec{b} \in \mb{S}, w \in \mb{R}\)</span>.<br />Then <span class="math">\(\exists c_1, \ldots, c_k \in \mb{R}, \vec{a} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k\)</span> and <span class="math">\(\exists d_1, \ldots, d_k \in \mb{R}, \vec{b} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k\)</span>.<br />So <span class="math">\(\vec{a} + \vec{b} = (c_1 + d_1) \vec{v}_1 + \ldots + (c_k + d_k) \vec{v}_k\)</span>, and <span class="math">\(\vec{a} + \vec{b} \in \mb{S}\)</span>.<br />So <span class="math">\(w\vec{a} = wc_1 \vec{v}_1 + \ldots + wc_k \vec{v}_k\)</span> and <span class="math">\(w\vec{a} \in \mb{S}\)</span>.<br />So <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
</blockquote>
<h2 id="dot-product">Dot Product</h2>
The <strong>dot product</strong> of two vectors <span class="math">\(\vec{a}\)</span> and <span class="math">\(\vec{b}\)</span> is defined as $  =

<p>\begin{bmatrix} b_1</p>
<p>The dot product also has the geometric interpretation <span class="math">\(\vec{a} \cdot \vec{b} = \abs{a} \abs{b} \cos \theta\)</span>, where <span class="math">\(\theta\)</span> is the angle between the two vectors.</p>
<p>The dot product is also known as the <strong>scalar product</strong> or <strong>standard inner product</strong> of <span class="math">\(\mb{R}^n\)</span>.</p>
<h3 id="theorem-1.3.2">Theorem 1.3.2</h3>
<p>If <span class="math">\(\vec{x}, \vec{y}, \vec{z} \in \mb{R}^n, s, t, \in \mb{R}\)</span>, then:</p>
<ul>
<li><span class="math">\(\vec{x} \cdot \vec{x} \ge 0\)</span>, and <span class="math">\(\vec{x} \cdot \vec{x} = 0 \iff \vec{x} = \vec{0}\)</span>.</li>
<li><span class="math">\(\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}\)</span>.</li>
<li><span class="math">\(\vec{x} \cdot (s\vec{y} + t\vec{z}) = s (\vec{x} \cdot \vec{y}) + t (\vec{x} \cdot \vec{z})\)</span>.</li>
</ul>
<p>The <strong>length</strong> or <strong>norm</strong> of a vector <span class="math">\(\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_k \end{bmatrix}\)</span> is <span class="math">\(\magn{\vec{v}} = \sqrt{\sum_{i = 1}^k v_i^2}\)</span>. Note the similarity to scalars, where <span class="math">\(\abs{x} = \sqrt{x^2}\)</span>.</p>
<p>A vector of length 1 is a unit vector. A unit vector is therefore one such that <span class="math">\(\sum_{i = 1}^k v_i^2 = 1\)</span>.</p>
<p>Also, <span class="math">\(\vec{x} \cdot \vec{x} = \magn{x}^2\)</span>, which should be obvious since <span class="math">\(\theta = 0\)</span> and <span class="math">\(\cos \theta = 1\)</span>, so <span class="math">\(\vec{x} \cdot \vec{x} = \magn{x} \magn{x} = \magn{x}^2\)</span>.</p>
<h3 id="theorem-1.3.3">Theorem 1.3.3</h3>
<p>If <span class="math">\(\vec{x}, \vec{y} \in \mb{R}^n, c \in \vec{R}\)</span>, then:</p>
<ul>
<li><span class="math">\(\magn{\vec{x}} \ge 0\)</span>, and <span class="math">\(\magn{\vec{x}} = 0 \iff \vec{x} = \vec{0}\)</span></li>
<li><span class="math">\(\magn{cx} = \abs{c}\magn{\vec{x}}\)</span></li>
<li><span class="math">\(\abs{\vec{x} \cdot \vec{y}} \le \magn{\vec{x}}\magn{\vec{y}}\)</span> (Cauchy-Schwarz-Buniakowski Inequality)</li>
<li><span class="math">\(\magn{\vec{x} + \vec{y}} \le \magn{\vec{x}} + \magn{\vec{y}}\)</span> (Triangle Inequality)</li>
</ul>
<p>Proof of third consequence:</p>
<blockquote>
<p>Clearly, if <span class="math">\(\vec{x} = \vec{0}\)</span> then the inequality holds. Assume <span class="math">\(\vec{x} \ne \vec{0}\)</span>.<br />Clearly, <span class="math">\(\forall t \in \mb{R}, (t\vec{x} + \vec{y}) \cdot (t\vec{x} + \vec{y}) \ge 0\)</span> (from properties of dot product), so <span class="math">\(t^2(\vec{x} \cdot \vec{x}) + 2t(\vec{x} \cdot \vec{y}) + \vec{y} \cdot \vec{y} \ge 0\)</span>. Clearly, this is a quadratic polynomial where <span class="math">\(t\)</span> is the variable. The polynomial is greater or equal to 0 if and only if it has at most 1 root.<br />So the discriminant in the quadratic formula, <span class="math">\(b^2 - 4ac\)</span>, must be less or equal to 0.<br />So <span class="math">\(4(\vec{x} \cdot \vec{y})^2 - 4(\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y}) \le 0\)</span>.<br />So <span class="math">\((\vec{x} \cdot \vec{y})^2 \le (\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y})\)</span> and <span class="math">\((\vec{x} \cdot \vec{y})^2 \le \magn{x}^2\magn{y}^2\)</span>.<br />So <span class="math">\(\vec{x} \cdot \vec{y} \le \magn{x}\magn{y}\)</span>.</p>
</blockquote>
<h3 id="angle">Angle</h3>
<p>The <strong>angle</strong> between two vectors <span class="math">\(\vec{x}\)</span> and <span class="math">\(\vec{y}\)</span> is defined as the angle <span class="math">\(\theta\)</span> such that <span class="math">\(\vec{x} \cdot \vec{y} = \magn{x} \magn{y} \cos \theta\)</span>.</p>
<p>In other words, <span class="math">\(\theta = \arccos\left(\frac{\vec{x} \cdot \vec{y}}{\magn{x} \magn{y}}\right)\)</span>.</p>
<p>Two vectors are <strong>orthogonal</strong> if the angle between them is 90 degrees - if their dot product is 0.</p>
<p>One of the reasons the standard bases are easy to work with is specifically because they are orthogonal to each other and because they are unit vectors.</p>
<h3 id="planeshyperplanes">Planes/Hyperplanes</h3>
<p>We can use the dot product to derive a <strong>scalar equation</strong> form for a plane, that makes use of the fact that the normal of a plane is just a vector.</p>
<p>Let <span class="math">\(A(a_1, a_2, a_3)\)</span> be a fixed point on the plane. Let <span class="math">\(X(x_1, x_2, x_3)\)</span> be an arbitrary point on the plane.</p>
<p>Then <span class="math">\(\vec{X} - \vec{A}\)</span> is a vector that lies on the plane.</p>
<p>In other words, if <span class="math">\(\vec{x} = s\vec{u} + t\vec{v}, s, t \in \mb{R}\)</span>, then <span class="math">\(\vec{x} - \vec{b}\)</span> is clearly a vector that lies on the plane.</p>
<p>Let <span class="math">\(\vec{n}\)</span> be a vector normal to the plane. Clearly, <span class="math">\(\vec{n} \cdot (\vec{X} - \vec{A}) = 0\)</span>.</p>
<p>So <span class="math">\(n_1 (x_1 - a_1) + n_2 (x_2 - a_2) + n_3 (x_3 - a_3) = 0 = n_1 x_1 + n_2 x_2 + n_3 x_3 - n_1 a_1 - n_2 a_2 - n_3 a_3\)</span>.</p>
<p>So <span class="math">\(n_1 x_1 + n_2 x_2 + n_3 x_3 = n_1 a_1 + n_2 a_2 + n_3 a_3\)</span>. Since <span class="math">\(\vec{A}\)</span> is fixed, <span class="math">\(n_1 a_1 + n_2 a_2 + n_3 a_3\)</span> is constant and the equation is a function of <span class="math">\(x_1, x_2, x_3\)</span>.</p>
<p>Since <span class="math">\(\vec{X}\)</span> is arbitrary, this holds for every point on the plane, and so is an equation of the plane.</p>
<p>Using the above, we can find the scalar equation of any plane given the normal and a fixed point on the plane.</p>
<p>If we extend this to hyperplanes, ;wip</p>
<h3 id="cross-product">Cross Product</h3>
<p>However, we will typically be given the vector equation of the plane, like <span class="math">\(\vec{x} = c_1 \vec{u} + c_2 \vec{v} + \vec{w}\)</span>.</p>
<p>We can calculate the normal of the plane by finding two linearly independent vectors that lie on the plane, and finding a vector that is orthogonal to both of them.</p>
<p>Clearly, <span class="math">\(\vec{u}\)</span> and <span class="math">\(\vec{v}\)</span> lie on the plane, being the solutions when <span class="math">\(c_2 = 0\)</span> or <span class="math">\(c_1 = 0\)</span>, respectively.</p>
<p>Then for the normal <span class="math">\(\vec{n}\)</span> we know <span class="math">\(\vec{n} \cdot \vec{u} = \vec{n} \cdot \vec{v} = 0\)</span>. So <span class="math">\(n_1 u_1 + n_2 u_2 + n_3 u_3 = n_1 v_1 + n_2 v_2 + n_3 v_3 = 0\)</span>.</p>
<p>Solving two equations for three unknowns, we find that one possible solution is <span class="math">\(\vec{n} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}\)</span>.</p>
<p>This problem is so common that we gave its solution a name, the <strong>cross product</strong>. The cross product of two vectors <span class="math">\(\vec{u}\)</span> and <span class="math">\(\vec{v}\)</span> is <span class="math">\(\vec{u} \times \vec{v} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}\)</span>, and is always a vector that is orthogonal to both vectors.</p>
<p>Note that this operation is only defined in <span class="math">\(\mb{R}^3\)</span> and <span class="math">\(\mb{R}^7\)</span>.</p>
<h2 id="projections">Projections</h2>
<p>Given the vectors <span class="math">\(\vec{u}, \vec{v}, \vec{w} \in \mb{R}^n, \vec{v} \ne \vec{0}, \vec{v} \cdot \vec{w} = 0\)</span>, we want to write <span class="math">\(\vec{u}\)</span> as the sum of a scalar multiple of <span class="math">\(\vec{v}\)</span> and <span class="math">\(\vec{w}\)</span>, as the vector <span class="math">\(\vec{u} = c\vec{v} + \vec{w}, c \in \mb{R}\)</span>.</p>
<p>We first need to find out how much of <span class="math">\(\vec{u}\)</span> is in the direction of <span class="math">\(\vec{v}\)</span> - find <span class="math">\(c\)</span>. Clearly, <span class="math">\(\vec{u} \cdot \vec{v} = (c\vec{v} + \vec{w}) \cdot \vec{v} = c\magn{v}^2 + \vec{w} \cdot \vec{v} = c\magn{v}^2\)</span>.</p>
<p>So <span class="math">\(c = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}\)</span>.</p>
<p>The <strong>projection</strong> of <span class="math">\(\vec{u}\)</span> onto <span class="math">\(\vec{v}\)</span> is defined as <span class="math">\(\proj_{\vec{v}} \vec{u} = c\vec{v} = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}\vec{v}\)</span>, and is the vector along the same direction as <span class="math">\(\vec{v}\)</span> such that it has the same extent along <span class="math">\(\vec{v}\)</span> as <span class="math">\(\vec{u}\)</span>.</p>
<p>The <strong>perpendicular</strong> of <span class="math">\(\vec{u}\)</span> onto <span class="math">\(\vec{v}\)</span> is the vector that when added to the projection, results in <span class="math">\(\vec{u}\)</span>. Therefore, the perpendicular is <span class="math">\(\prp_{\vec{v}} \vec{u} = \vec{u} - \proj_{\vec{v}} \vec{u}\)</span>.</p>
<h3 id="planes">Planes</h3>
<p>How do we project a vector onto a plane? We can notice that the projection of a vector onto a plane is the perpendicular of the vector projected onto the normal of the plane.</p>
<p>Therefore, the projection of a vector <span class="math">\(\vec{v}\)</span> onto a plane with normal <span class="math">\(\vec{n}\)</span> is <span class="math">\(\prp_{\vec{n}} \vec{v}\)</span>.</p>
<h2 id="systems-of-linear-equations">Systems of Linear Equations</h2>
<p>A <strong>linear equation</strong> is an equation of the form <span class="math">\(a_1 x_1 + \ldots + a_n x_n = b, a_k, b \in \mb{C}, 1 \le k \le n, n \in \mb{N}\)</span>. This also includes equations that can be rewritten into this form.</p>
<p>A set of linear equations with the same variables <span class="math">\(x_1, \ldots, x_n\)</span> (including those where there are zero coefficients) is called a <strong>system of linear equations</strong>.</p>
<p>A system of linear equations can be written as <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{n, 1} x_1 + \ldots + a_{n, n} x_n = b_1 \\ \end{cases}\)</span>. Here, <span class="math">\(a_{i, j}\)</span> represents the <span class="math">\(j\)</span>th coefficient in the <span class="math">\(i\)</span>th equation.</p>
<p>A <strong>solution</strong> to a system of linear equation is a vector <span class="math">\(\vec{s} = \begin{bmatrix} s_1 \\ \vdots \\ s_n \end{bmatrix}, \vec{s} \in \mb{R}^n\)</span> such that if <span class="math">\((\forall 1 \le i \le n, x_i = s_i)\)</span>, all the equations in the system are satisfied.</p>
<p>A system of linear equations is <strong>consistent</strong> if it has at least one solution, and <strong>inconsistent</strong> otherwise.</p>
<p>Two systems of linear equations are <strong>equivalent</strong> if and only if they both have the same solution set.</p>
<p>Interpreted geometrically, a system of <span class="math">\(m\)</span> linear equations with <span class="math">\(n\)</span> variables is a set of <span class="math">\(m\)</span> hyperplanes in <span class="math">\(\mb{R}^n\)</span>. The solution to this system is represented by a vector that lies on all these hyperplanes.</p>
<h3 id="theorem-2.1.1">Theorem 2.1.1</h3>
<p>If a set of linear equations is consistent with more than 1 solution, then it has infinite solutions.</p>
<p>In other words, if <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{n, 1} x_1 + \ldots + a_{n, n} x_n = b_1 \\ \end{cases}\)</span> has solutions <span class="math">\(\vec{s}\)</span> and <span class="math">\(\vec{t}\)</span> such that <span class="math">\(\vec{s} \ne \vec{t}\)</span>, then <span class="math">\(\vec{s} + c(\vec{s} - \vec{t})\)</span> is a solution for all <span class="math">\(c \in \mb{R}\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{n, 1} x_1 + \ldots + a_{n, n} x_n = b_1 \\ \end{cases}\)</span> has solutions <span class="math">\(\vec{s}\)</span> and <span class="math">\(\vec{t}\)</span>.<br />Clearly, for all <span class="math">\(1 \le i \le n\)</span>, <span class="math">\(a_{i, 1} (s_1 + c(s_1 - t_1)) + \ldots + a_{i, n} (s_n + c(s_n - t_n)) = (a_{i, 1} s_1 + \ldots + a_{i, n} s_n) + c(a_{i, 1} s_1 + \ldots + a_{i, n} s_n) - c(a_{i, 1} t_1 + \ldots + a_{i, n} t_n) = b_i + cb_i - cb_i = b_i\)</span>.<br />So each equation is satisfied, and the system is consistent.</p>
</blockquote>
<p>The set of all solutions to a system of linear equations is known as a <strong>solution set</strong>.</p>
<h3 id="solving">Solving</h3>
<p>When we use the substitution and elimination techniques for solving systems of linear equations, we are forming new systems of lienar equations that have the same solution set as the original. We aim to obtain one that is easier to find the solution set for, and therefore solve the original system.</p>
<p>Note that when we solve a linear system, we don't really need the <span class="math">\(x\)</span> variables. Instead, we could write the system <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{n, 1} x_1 + \ldots + a_{n, n} x_n = b_1 \\ \end{cases}\)</span> more concisely as <span class="math">\(\left[\begin{array}{ccc|c} a_{1, 1} &amp; \ldots &amp; a_{1, n} &amp; b_1 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{n, 1} &amp; \ldots &amp; a_{n, n} &amp; b_1 \\ \end{array}\right]\)</span>.</p>
<p>This is called the <strong>augmented matrix</strong> of the system of linear equations.</p>
<p>The <strong>coefficient matrix</strong> of the system of linear equations is the same thing, but without the last column containing the constant value. In this case, it would be <span class="math">\(\left[\begin{array}{ccc} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{n, 1} &amp; \ldots &amp; a_{n, n} \\ \end{array}\right]\)</span>.</p>
<p>We can combine a coefficient matrix <span class="math">\(A\)</span> and a constant vector <span class="math">\(\vec{b}\)</span> into an augmented matrix by using the <span class="math">\(\left[A \middle| \vec{b}\right]\)</span>. This simply adds <span class="math">\(\vec{b}\)</span> as a column to the end of <span class="math">\(A\)</span>.</p>
<p>Solve <span class="math">\(\begin{cases} 2x_1 + 3x_2 &amp;= 11 \\ 3x_1 + 6x_2 &amp;= 7 \\ \end{cases}\)</span>:</p>
<blockquote>
<p>We multiply the first equation by <span class="math">\(-3\)</span> to obtain <span class="math">\(-6x_1 - 9x_2 = -33\)</span>.<br />We multiply the second equation by <span class="math">\(2\)</span> to obtain <span class="math">\(6x_1 + 12x_2 = 14\)</span>.<br />Now we add the first equation to the second equation to obtain <span class="math">\(0x_1 + 3x_2 = -19\)</span>.<br />Now we multiply the second equation by <span class="math">\(\frac{1}{3}\)</span> to obtain <span class="math">\(0x_1 + x_2 = -\frac{19}{3}\)</span>.<br />Now we add the second equation, multiplied by <span class="math">\(9\)</span>, to the first equation to obtain <span class="math">\(-6x_1 + 0x_2 = -90\)</span>.<br />Now we multiply the first equation by <span class="math">\(-6\)</span> to obtain <span class="math">\(x_1 + 0x_2 = 15\)</span>.<br />So <span class="math">\(x_1 = 15\)</span> and <span class="math">\(x_2 = -\frac{19}{3}\)</span>.</p>
</blockquote>
<p>Now solve using operations on the matrix form of the equations, <span class="math">\(\left[\begin{array}{cc|c} 2 &amp; 3 &amp; 11 \\ 3 &amp; 6 &amp; 7 \\ \end{array}\right]\)</span>:</p>
<blockquote>
<p>We multiply the first row by <span class="math">\(-3\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 3 &amp; 6 &amp; 7 \\ \end{array}\right]\)</span>.<br />We multiply the second row by <span class="math">\(2\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 6 &amp; 12 &amp; 14 \\ \end{array}\right]\)</span>.<br />We add the first to the seocnd to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 3 &amp; -19 \\ \end{array}\right]\)</span>.<br />We multiply the second equation by <span class="math">\(\frac{1}{3}\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />Now we add the second equation multiplied by 9 to the first equation to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; 0 &amp; -90 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />Now we multiply the first equation by <span class="math">\(-\frac{1}{6}\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} 1 &amp; 0 &amp; 15 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />So <span class="math">\(x_1 = 15\)</span> and <span class="math">\(x_2 = -\frac{19}{3}\)</span>.</p>
</blockquote>
<p>Note that at every step, we had a system of linear equations that had the same solution set as the original system. Eventually, we reduced the matrix down into a form that we could easily read the values off of.</p>
<p>Also note that there were only really two different operations that we used to solve the system. To make it easier for computers to work with, we define an additional swapping operation:</p>
<ul>
<li>Multiplying a row by a non-zero scalar (<span class="math">\(cR_i\)</span> multiplies the <span class="math">\(i\)</span>th row by <span class="math">\(c \in \mb{R}, c \ne 0\)</span>).</li>
<li>Adding a multiple of one row to another (<span class="math">\(R_i + cR_j\)</span> adds the <span class="math">\(j\)</span>th row multiplied by <span class="math">\(c \in \mb{R}, c \ne 0\)</span> to the <span class="math">\(i\)</span>th row).</li>
<li>Swapping two rows (<span class="math">\(R_i \leftrightarrow R_j\)</span>).</li>
</ul>
<p>These operations we call the <strong>elementary row operations</strong> (EROs). Note that they are also fully reversible - all EROs have an inverse that undoes the effect of the operation. This is trivial to prove.</p>
<p>Two matrices are <strong>row equivalent</strong> if one can be transformed into another by application of EROs. Since EROs are reversible, if a matrix <span class="math">\(A\)</span> is row equivalent to <span class="math">\(B\)</span>, then <span class="math">\(B\)</span> is also transformable into <span class="math">\(A\)</span> via the inverse of those EROs, and so <span class="math">\(B\)</span> is row equivalent to <span class="math">\(A\)</span>. In other words, row equivalence is commutative.</p>
<h3 id="row-reduced-echelon-form">Row Reduced Echelon Form</h3>
<p>A matrix is in <strong>reduced row echelon form/row canonical form</strong> (RREF) if and only if:</p>
<ol style="list-style-type: decimal">
<li>All rows with only zeroes are at the bottom. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 2 \end{bmatrix}\)</span>.</li>
<li>The first non-zero entry in each row is 1 (this is called the <strong>leading one</strong>). For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 4 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}\)</span>.</li>
<li>The leading one in each row, if it exists, appears to the right of the leading one of the row above it. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 1 &amp; 0 \end{bmatrix}\)</span>.</li>
<li>The leading one in each row must be the only non-zero entry in its column. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 1 &amp; 3 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}\)</span>.</li>
</ol>
<p>For augmented matrices, we check only the values in all but the last column when seeing if it is in RREF (the values on the left of the vertical line). In other words, we ignore the last column when checking for RREF form.</p>
<p>All matrices are representable in RREF. All matrices have one and only one representation in RREF.</p>
<h3 id="theorem-2.2.2">Theorem 2.2.2</h3>
<p>The RREF of a matrix is unique. In other words, each matrix has at most one possible matrix that it is both row equivalent to, and is in RREF.</p>
<p>Proof:</p>
<blockquote>
<p>;wip: something about proving this in chapter 4</p>
</blockquote>
<p>Row reduce <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \end{array}\)</span>:</p>
<blockquote>
<p><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\)</span> via <span class="math">\(R_2 + (-2)R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ \end{array}\)</span> via <span class="math">\(R_2 \leftrightarrow R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\((-1)R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 1 &amp; 16 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\(R_2 - R_1\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 1 &amp; 16 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\(R_2 - R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 11 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\(R_2 - R_3\)</span>.</p>
</blockquote>
<p>Note the general technique used in reducing the matrix. First, we want to obtain a triangle of 0 elements with leading ones diagonally rightwards, by going from top to bottom. Then, we make sure each leading one is the only non-zero element in its column by subtracting from rows below from bottom to top.</p>
<p>This technique is basically modifying the matrix until all the properties except for property 4. Then property 4 can be solved for relatively easily.</p>
<h3 id="solution-set">Solution Set</h3>
<p>Any row of the form <span class="math">\(\begin{array}{ccc|c} 0 &amp; \ldots &amp; 0 &amp; c \end{array}, c \ne 0\)</span> is inconsistent - no possible values can satisfy this equation, so there are no solutions.</p>
<p>Recall that if consistent, a system of linear equations has either one or infinitely many solutions.</p>
<p>Solve <span class="math">\(\begin{cases} x_1 + x_2 + x_3 &amp;= 4 \\ x_2 + x_3 &amp;= 3 \end{cases}\)</span>:</p>
<blockquote>
<p>The augmented matrix is <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 1 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 3 \end{array}\)</span>.<br />The matrix in RREF is ;wip</p>
</blockquote>
<hr>
<p>Copyright 2013 Anthony Zhang</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>.
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>