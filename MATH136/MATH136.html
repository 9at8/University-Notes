<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>MATH136 | Anthony Zhang</title>
  <style type="text/css">
  body {
    font-family: "Segoe UI", Verdana, Arial, Helvetica, sans-serif;
    background: #fffefe;
    padding: 5em;
  }
  
  pre {
    margin-left: 2em;
  }
  
  code {
    border: solid 1px black;
    background: #665555;
    color: white;
    padding: 0.1em;
    border-radius: 0.3em;
    display: inline-block;
  }
  
  pre code {
    padding: 1em;
    border-radius: 0.5em;
  }
  
  h1 {
    font-size: 4em;
  }
  
  table {
    margin: 0 auto;
  }
  
  td, th {
    padding: 0.5em;
    border: 1px solid grey;
  }
  
  tr {
    padding:: 0;
  }
  
  a.button {
    display: inline-block;
    padding: 1em;
    font-family: monospace;
    color: black;
    text-decoration: none;
    border: 0.2em solid black;
    border-radius: 0.5em;
    background: white;
  }
  
  a.button:hover, a.button:focus, a.button:active {
    background: black;
    color: white;
  }
  </style>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript">MathJax.Hub.Queue(["Typeset",MathJax.Hub]);</script>
</head>
<body>
<a class="button" href="..">&#8666; Return to University Notes index</a>
<h1 id="math136">MATH136</h1>
<p>Linear Algebra</p>
<pre><code>Instructor: Dan Wolczuk
Section 081 (online)
Email: dstwolcz@uwaterloo.ca</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\]</span></p>
<h1 id="im-going-to-stop-procrastinating-this-week-honest">13/1/14 (I'm going to stop procrastinating this week, honest!)</h1>
<h2 id="notation">Notation</h2>
<p>We represent vectors as column vectors (matrices of size <span class="math">\(n \times 1\)</span>) to distinguish them from points: <span class="math">\(\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}, v_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>However, sometimes we will also write them in <span class="math">\(n\)</span>-tuple form (for example, <span class="math">\((3, 7, 2)\)</span>)</p>
<p><span class="math">\(\mb{R}^n, n \in \mathbb{Z}, n &gt; 0\)</span> is the set of all elements of the form <span class="math">\((x_1, \ldots, x_n), x_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>We usually think of these elements as points, but now we will think of them as vectors, abstract objects that we can perform operations on, like adding and subtracting.</p>
<p>This is called an <span class="math">\(n\)</span>-dimensional Euclidean space, and is the set of all vectors that are of length (dimension) <span class="math">\(n\)</span>. For example, <span class="math">\(\mb{R}^3\)</span> (arr-three) is the set of all 3D vectors, and <span class="math">\(\vec{0} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}\)</span>.</p>
<p>So <span class="math">\(\mb{R}^3 = \set{\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \middle| x_1, x_2, x_3 \in \mathbb{R}}\)</span>.</p>
<p>Likewise, <span class="math">\(\mb{R}^n = \set{\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \middle| x_1, \ldots, x_n \in \mathbb{R}}\)</span>.</p>
<p>It might also occasionally be useful to think of these vectors as points, where the vectors are offsets from the origin. For example, a function <span class="math">\(f\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\right) = f(x_1, \ldots, x_n)\)</span>.</p>
<h2 id="operations">Operations</h2>
<p>Let <span class="math">\(\vec{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}, \vec{y} = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}, c \in \mb{R}\)</span>.</p>
<h3 id="equality">Equality</h3>
<p><span class="math">\(x = y\)</span> if and only if <span class="math">\(\forall 1 \le i \le n, x_i = y_i\)</span>.</p>
<p>In other words, two vectors are equal if and only if all their components are equal. Note that this is defined only for vectors with the same size (number of components).</p>
<h3 id="additionsubtraction">Addition/Subtraction</h3>
<p><span class="math">\[x + y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} + \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{bmatrix}\]</span></p>
<p>Also, <span class="math">\(\vec{x} - \vec{y} = \vec{x} + (-\vec{y})\)</span>. Therefore, <span class="math">\(x - y = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} - \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} x_1 - y_1 \\ \vdots \\ x_n - y_n \end{bmatrix}\)</span>.</p>
<h3 id="scalar-multiplication">Scalar Multiplication</h3>
<p><span class="math">\[cx = c\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} cx_1 \\ \vdots \\ cx_n \end{bmatrix}\]</span></p>
<p>If <span class="math">\(c = -1\)</span>, <span class="math">\(c\vec{x} = -x\)</span>.</p>
<h2 id="linear-combination">Linear Combination</h2>
<p>A <strong>linear combination</strong> of vectors <span class="math">\(\vec{v}_1, \ldots, \vec{v}_n\)</span> is the value <span class="math">\(L = c_1 v_1 + \ldots + c_n v_n, c_i \in \mathbb{R}, 1 \le i \le n\)</span>.</p>
<p>It is a linear expression with each term containing a coefficient and one vector value - a sum of scalar multiples.</p>
<h3 id="theorem-1.1.1">Theorem 1.1.1</h3>
<p>If <span class="math">\(\vec{x}, \vec{y}, \vec{w} \in \mathbb{R}^n, c, d \in \mathbb{R}\)</span>, then:</p>
<ul>
<li>Closure under addition: <span class="math">\(\vec{x} + \vec{y} \in \mathbb{R}^n\)</span>.</li>
<li>Associativity: <span class="math">\((\vec{x} + \vec{y}) + \vec{w} = \vec{x} + (\vec{y} + \vec{w})\)</span></li>
<li>Commutativity: <span class="math">\(\vec{x} + \vec{y} = \vec{y} + \vec{x}\)</span></li>
<li>Additive identity: <span class="math">\(\exists \vec{0} \in \mathbb{R}, \forall \vec{x} \in \mb{R}^n, \vec{x} + \vec{0} = \vec{x}\)</span> (<span class="math">\(\vec{0}\)</span> is called the zero vector).</li>
<li>Additive inverse: <span class="math">\(\forall \vec{x} \in \mathbb{R}^n, \exists -\vec{x} \in \mathbb{R}^n, \vec{x} + (-\vec{x}) = \vec{0}\)</span>.</li>
<li>Closure under scalar multiplication: <span class="math">\(c\vec{x} \in \mathbb{R}^n\)</span>.</li>
<li><span class="math">\(c(d\vec{x}) = (cd)\vec{x}\)</span>.</li>
<li>Scalar distributivity: <span class="math">\((c + d)\vec{x} = c\vec{x} + d\vec{x}\)</span>.</li>
<li>Vector distributivity: <span class="math">\(c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}\)</span>.</li>
<li>Multiplicative identity: <span class="math">\(1\vec{x} = \vec{x}\)</span>.</li>
</ul>
<p>Closure under addition means that when we add two elements in the set, the resulting sum is also in the set.</p>
<p>Closure under scalar multiplication means that when we multiply an element in the set by a scalar, the resulting product is also in the set.</p>
<p>Because these Euclidean spaces are closed under addition and scalar multiplication, they are closed under linear combinations - all linear combinations of vectors in <span class="math">\(\mb{R}^n\)</span> are also in <span class="math">\(\mb{R}^n\)</span>.</p>
<h2 id="span">Span</h2>
<p>The <strong>span</strong> of a set of vectors <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> is the set of all linear combinations of the vectors in the set.</p>
<p>In other words, <span class="math">\(\spn \mathcal{B} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k} = \set{c_1 v_1 + \ldots + c_n v_n \middle| c_1, \ldots, c_n \in \mathbb{R}}\)</span>.</p>
<p>A span is always a subset of the space it is in. In other words, <span class="math">\(\spn \mathcal{B} \subseteq \mb{R}^n\)</span>.</p>
<p>A span is just a set. Therefore, we can say that a set <span class="math">\(\mathcal{B}\)</span> <strong>spans</strong> another set <span class="math">\(\mathcal{C}\)</span> if and only if <span class="math">\(\mathcal{C}\)</span> is exactly the set of all linear combinations of the vectors in <span class="math">\(\mathcal{B}\)</span>.</p>
<p>The <strong>vector equation</strong> of a set of vectors is the generalized equation for the linear combinations of each element in the set. For example, <span class="math">\(\vec{x} = c_1 v_1 + \ldots + c_n v_n\)</span> is the vector equation for <span class="math">\(\mathcal{B}\)</span>.</p>
<h3 id="theorem-1.1.2">Theorem 1.1.2</h3>
<p>If <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>, then <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
<p>In other words, if a vector in a set of vectors can already be represented by a linear combination of other vectors in the set, it doesn't affect the span at all.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>.<br />So <span class="math">\(\exists c_1, \ldots, c_k \in \mathbb{R}, \vec{v}_{k + 1} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k\)</span>.<br />We want to show that <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.<br />Let <span class="math">\(\vec{x}\)</span> be an arbitrary element of <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}\)</span>.<br />So <span class="math">\(\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} \vec{v}_{k + 1}\)</span>.<br />So <span class="math">\(\exists d_1, \ldots, d_{k + 1} \in \mathbb{R}, \vec{x} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} (c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k) = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k + d_{k + 1} c_1 \vec{v}_1 + \ldots + d_{k + 1} c_k \vec{v}_k = (d_1 + d_{k + 1} c_1) \vec{v}_1 + \ldots + (d_k + d_{k + 1} c_k) \vec{v}_k\)</span>.<br />Clearly, <span class="math">\(x \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.<br />Clearly, <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k} \subseteq \spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}}\)</span>.<br />Therefore, <span class="math">\(\spn \set{\vec{v}_1, \ldots, \vec{v}_k, \vec{v}_{k + 1}} = \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
</blockquote>
<p>Consider <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}}\)</span>. Clearly, <span class="math">\(\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}\)</span>.</p>
<p>Clearly, <span class="math">\(\begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}}\)</span>, so <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}} = \spn \set{\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}}\)</span> represents a line in 3D.</p>
<p>The span of the zero vector is itself. The span of a linearly independent vector is a line. The span of two linearly independent vectors is a plane. The span of 3 linearly independent vectors is a 3D space. The span of 4 linearly independent vectors is a 4D space, and so on.</p>
<h3 id="linear-independence">Linear Independence</h3>
<p>When we say <span class="math">\(\vec{v}_{k + 1}\)</span> can be written as a linear combination of <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k\)</span>, we can also write <span class="math">\(\vec{v}_{k + 1} \in \spn \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span>.</p>
<p>Note that <span class="math">\(\vec{0}\)</span> can always be written as a linear combination of any vectors in the same space. The zero vector exists in all dimensions.</p>
<p>It is important to determine which vectors can be written as linear combinations of others.</p>
<p>A set of vectors is <strong>linearly independent</strong> if and only if the vectors in the set cannot be written as linear combinations of each other. Otherwise, the set is <strong>linearly dependent</strong>.</p>
<p>In other words, <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = c_k \vec{v}_k\)</span>. Rearranging, we get <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_{k - 1} \vec{v}_{k - 1} - c_k \vec{v}_k + c_{k + 1} \vec{v}_{k + 1} + \ldots + c\_n vec{v}_n = \vec{0}\)</span>.</p>
<h3 id="theorem-1.1.3">Theorem 1.1.3</h3>
<p>Therefore, a set of vectors <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> is linearly dependent if and only if <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \cdot \ldots \cdot c_n \ne 0 \implies c_1 \vec{v}_1 + \ldots + c_n vec{v}_n = \vec{0}\)</span>, and otherwise linearly independent.</p>
<p>The solution where <span class="math">\(c_i = 0, 1 \le i \le n\)</span> is called the trivial solution, because all the coefficients are 0. The trivial solution is always a solution to the above equation.</p>
<p>In other words, the set is linearly independent if and only if the zero vector cannot be written as a linear combination of the vectors in the set, except when all the coefficients are 0.</p>
<p>To prove that a set is linearly independent, we need to prove that the only solution to the above equation is the trivial solution.</p>
<p>To prove that a set is linearly independent, we need to prove that there is a solution to the above equation where at least one of the coefficients is non-aero.</p>
<p>When we find a solution to the equation, we can use this to find the vectors that are in the span of all the others - the vectors that can be written as a linear comnbination of the others. In the solution, <strong>all vectors with non-zero coefficients can be written as linear combinations of all the other vectors</strong>. If we rearrange the solved equation, we can find this linear combination.</p>
<h3 id="theorem-1.1.4">Theorem 1.1.4</h3>
<p>Note that any set of vectors that contains <span class="math">\(\vec{0}\)</span> is linearly dependent.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span> and <span class="math">\(\vec{v}_i = \vec{0}, 1 \le i \le n\)</span>.<br /><span class="math">\(\mathcal{B}\)</span> is linearly dependent if and only if <span class="math">\(\exists c_1, \ldots, c_n \in \mathbb{R}, c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0}, c_1 \cdot \ldots \cdot c_n = 0\)</span>.<br />Construct <span class="math">\(c_1, \ldots, c_{i - 1}, c_{i + 1}, \ldots, c_n = 0, c_i = 1\)</span>.<br />Then <span class="math">\(c_1 \cdot \ldots \cdot c_n = 0\)</span> and <span class="math">\(c_1 \vec{v}_1 + \ldots + c_n \vec{v}_n = \vec{0}\)</span>.<br />Therefore, <span class="math">\(\mathcal{B}\)</span> is linearly dependent.</p>
</blockquote>
<p>The vector equation of a linearly independent set is known as a simplified vector equation. A vector equation can be converted into a simplified one by removing terms where the vector can be written as a linear combination of the others.</p>
<p>Geometrically, two vectors span a plane if they are not parallel, and three vectors span a 3D space if they do not all lie on the same plane.</p>
<h2 id="bases">Bases</h2>
<p>A linearly independent set of vectors is always simpler than a linearly dependent one. A linearly dependent set of vectors can always be converted into a linearly dependent one with the same span. Therefore, the simplest set of vectors that spans a given set is always linearly independent.</p>
<p>Given <span class="math">\(\mathcal{B} = \set{\vec{v}_1, \ldots, \vec{v}_n}\)</span>, <span class="math">\(\mathcal{B}\)</span> is a <strong>basis</strong> for <span class="math">\(\spn \mathcal{B}\)</span> if <span class="math">\(\mathcal{B}\)</span> is linearly independent.</p>
<p>In other words, a basis for a set is a linearly independent set of vectors such that its span is exactly the set.</p>
<p>The basis for <span class="math">\(\set{\vec{0}}\)</span> is the empty set (<span class="math">\(\emptyset\)</span>).</p>
<p>The <strong>standard basis</strong> is a basis that is easy to write for any number of dimensions. The standard basis is of the form <span class="math">\(\set{\vec{e}_1, \ldots, \vec{e}_n}\)</span>, where <span class="math">\(\vec{e}_i\)</span> has all components set to 0, except for the <span class="math">\(i\)</span>-th component, for <span class="math">\(1 \le i \le n\)</span>.</p>
<p>For example, the standard basis of <span class="math">\(\mb{R}^4\)</span> is <span class="math">\(\set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}}\)</span>.</p>
<p>In order to prove that a set is the basis of another, we need to prove that it is linearly independent, and that it spans the set (by proving that an arbitrary vector in the second set can be written as a linear combination of the elements of the first).</p>
<p>Prove that <span class="math">\(\mathcal{B} = \set{\begin{bmatrix} 1 \\ 3 \end{bmatrix}, \begin{bmatrix} -1 \\ -1 \end{bmatrix}}\)</span> is a basis for <span class="math">\(\mb{R}^2\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2\)</span>.<br />We will find the coefficients we want by solving <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span> for <span class="math">\(c_1, c_2\)</span>.<br />Construct <span class="math">\(c_1 = \frac{x_2 - x_1}{2}, c_2 = \frac{x_2 - 3x_1}{2}\)</span>.<br />Clearly, <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \frac{x_2 - x_1}{2} \begin{bmatrix} 1 \\ 3 \end{bmatrix} + \frac{x_2 - 3x_1}{2} \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\exists c_1, c_2 \in \mathbb{R}, \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = c_1 \begin{bmatrix} 1 \\ 3 \end{bmatrix} + c_2 \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>.<br />So <span class="math">\(\mathcal{B}\)</span> spans <span class="math">\(\mb{R}^2\)</span>.<br />Note that there is only the trivial solution when <span class="math">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span>, so <span class="math">\(\mathcal{B}\)</span> is also linearly independent.<br />Therefore, <span class="math">\(\mathcal{B}\)</span> is a basis for <span class="math">\(\mb{R}^2\)</span>.</p>
</blockquote>
<p>A basis for <span class="math">\(\mb{R}^n\)</span> has exactly <span class="math">\(n\)</span> elements.</p>
<h2 id="shapes">Shapes</h2>
<p>Points are conceptually applicable in any dimension above 0.</p>
<p>Lines are conceptually applicable in any dimension above 1.</p>
<p>Planes are conceptually applicable in any dimension above 2.</p>
<p>A line in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + \vec{b}, c_1 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \vec{b} \in \mathbb{R}^n\)</span>. This is a line in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>.</p>
<p>A plane in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + c_2 \vec{v}_2 + \vec{b}, c_1, c_2 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \vec{v}_2, \vec{b} \in \mathbb{R}^n\)</span> and <span class="math">\(\set{\vec{v}_1, \vec{v}_2}\)</span> being a linearly independent set. This is a plane in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>.</p>
<p>A <span class="math">\(k\)</span>-plane in <span class="math">\(\mb{R}^n\)</span> takes the form of <span class="math">\(\vec{x} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k + \vec{b}, c_1, c_2 \in \mathbb{R}\)</span> given <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k, \vec{b} \in \mathbb{R}^n\)</span> and <span class="math">\(\set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> being a linearly independent set. This is a <span class="math">\(k\)</span>-plane in <span class="math">\(\mb{R}^n\)</span> that passes through <span class="math">\(\vec{b}\)</span>. A <span class="math">\(k\)</span>-plane is a <span class="math">\(k\)</span>-dimensional plane.</p>
<p>A hyperplane is a <span class="math">\(k\)</span>-plane, where <span class="math">\(k = n - 1\)</span>. It is an <span class="math">\(n - 1\)</span>-dimensional plane.</p>
<p>For example, <span class="math">\(\spn \set{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ −2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 1 \\ −1 \end{bmatrix}}\)</span> defines a hyperplane in <span class="math">\(\mb{R}^n\)</span>, since the set is linearly independent.</p>
<h1 id="ok-so-its-like-week-3-and-im-behind-i-get-it">20/1/14 (OK, so it's like week 3 and I'm behind, I get it)</h1>
<h2 id="subspaces">Subspaces</h2>
<p>A <strong>subspace</strong> of <span class="math">\(\mb{R}^n\)</span> is a non-empty subset <span class="math">\(\mb{S}\)</span> of <span class="math">\(\mb{R}^n\)</span> such that it satisfies all ten properties defined in Theorem 1.1.1.</p>
<p>All subspaces are <strong>spaces</strong>, which are subsets of a vector space.</p>
<p>But since properties 2-5 and 7-10 follow from properties 1 and 6, all we need to do to prove all ten properties is to prove properties 1 and 6 hold.</p>
<h3 id="theorem-1.2.1-subspace-test">Theorem 1.2.1 (Subspace Test)</h3>
<p>Given a set <span class="math">\(\mb{S}\)</span>, <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span> if and only if:</p>
<ul>
<li><span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{R}^n\)</span>.</li>
<li><span class="math">\(\mb{S}\)</span> is non-empty.</li>
<li><span class="math">\(\forall \vec{x}, \vec{y} \in \mb{S}, \vec{x} + \vec{y} \in \mb{S}\)</span>.</li>
<li><span class="math">\(\forall \vec{x} \in \mb{S}, c \in \mb{R}, c\vec{x} \in \mb{S}\)</span></li>
</ul>
<p>To prove a set is a subspace, we need to prove all four properties.</p>
<p>Clearly, if conditions 2 and 4 hold, <span class="math">\(\vec{0} \in \mb{S}\)</span>. So if <span class="math">\(\vec{0} \notin \mb{S}\)</span>, then one or both of the conditions is not met and the set is not a subspace.</p>
<p>We can use this to check if a set is a subspace by seeing if <span class="math">\(\vec{0}\)</span> is in it.</p>
<p>Prove <span class="math">\(\mb{S} = \set{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} \middle| x + y = 0, x - z = 0}\)</span> is a subspace of <span class="math">\(\mb{R}^3\)</span> and write a basis for the subspace:</p>
<blockquote>
<p>Clearly, <span class="math">\(\mb{S}\)</span> is a subset of <span class="math">\(\mb{R}^3\)</span>.<br />Clearly, <span class="math">\(\vec{0} \in \mb{S}\)</span>, so the set is non-empty.<br />Let <span class="math">\(\vec{a}, \vec{b} \in \mb{S}\)</span>.<br />So <span class="math">\(a_1 + a_2 = a_1 - a_3 = 0\)</span> and <span class="math">\(b_1 + b_2 = b_1 - b_3 = 0\)</span>.<br />Clearly, <span class="math">\((a_1 + b_1) + (a_2 + b_2) = (a_1 + b_1) - (a_3 + b_3) = 0\)</span>.<br />So <span class="math">\(\vec{a} + \vec{b} \in \mb{S}\)</span> and <span class="math">\(\mb{S}\)</span> is closed under addition.<br />Clearly, <span class="math">\(c(a_1 + a_2) = c(a_1 - a_3) = c0\)</span>.<br />So <span class="math">\(c\vec{a} \in \mb{S}\)</span> and <span class="math">\(\mb{S}\)</span> is closed under scalar multiplication.<br />So <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^3\)</span>.<br />Since <span class="math">\(a_1 + a_2 = a_1 - a_3 = 0\)</span>, <span class="math">\(a_2 = -a_1\)</span> and <span class="math">\(a_3 = a_1\)</span>.<br />So the general vector is <span class="math">\(\vec{v}(a_1) = \begin{bmatrix} a_1 \\ -a_1 \\ a_1 \end{bmatrix} = a_1 \begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}\)</span>.<br />So a basis for <span class="math">\(\mb{S}\)</span> is <span class="math">\(\set{\begin{bmatrix} 1 \\ -1 \\ 1 \end{bmatrix}}\)</span>.</p>
</blockquote>
<p>We find a basis of a subspace by finding the general form of a vector in the subspace, using this to find a spanning set for <span class="math">\(S\)</span>, and then reducing it into a linearly independent set.</p>
<h3 id="theorem-1.2.2">Theorem 1.2.2</h3>
<p>If <span class="math">\(\vec{v}_1, \ldots, \vec{v}_k \in \mb{R}^n\)</span>, then <span class="math">\(\mb{S} = \span \set{\vec{v}_1, \ldots, \vec{v}_k}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Clearly, the set is a subset of <span class="math">\(\mb{R}^n\)</span>.<br />Clearly, <span class="math">\(\vec{0} \in \mb{S}\)</span>, since <span class="math">\(\vec{0} = 0\vec{v}_1 + \ldots + 0\vec{v}_k\)</span>.<br />Let <span class="math">\(\vec{a}, \vec{b} \in \mb{S}, w \in \mb{R}\)</span>.<br />Then <span class="math">\(\exists c_1, \ldots, c_k \in \mb{R}, \vec{a} = c_1 \vec{v}_1 + \ldots + c_k \vec{v}_k\)</span> and <span class="math">\(\exists d_1, \ldots, d_k \in \mb{R}, \vec{b} = d_1 \vec{v}_1 + \ldots + d_k \vec{v}_k\)</span>.<br />So <span class="math">\(\vec{a} + \vec{b} = (c_1 + d_1) \vec{v}_1 + \ldots + (c_k + d_k) \vec{v}_k\)</span>, and <span class="math">\(\vec{a} + \vec{b} \in \mb{S}\)</span>.<br />So <span class="math">\(w\vec{a} = wc_1 \vec{v}_1 + \ldots + wc_k \vec{v}_k\)</span> and <span class="math">\(w\vec{a} \in \mb{S}\)</span>.<br />So <span class="math">\(\mb{S}\)</span> is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
</blockquote>
<h2 id="dot-product">Dot Product</h2>
The <strong>dot product</strong> of two vectors <span class="math">\(\vec{a}\)</span> and <span class="math">\(\vec{b}\)</span> is defined as $  =

<p>\begin{bmatrix} b_1</p>
<p>The dot product also has the geometric interpretation <span class="math">\(\vec{a} \cdot \vec{b} = \abs{a} \abs{b} \cos \theta\)</span>, where <span class="math">\(\theta\)</span> is the angle between the two vectors.</p>
<p>The dot product is also known as the <strong>scalar product</strong> or <strong>standard inner product</strong> of <span class="math">\(\mb{R}^n\)</span>.</p>
<h3 id="theorem-1.3.2">Theorem 1.3.2</h3>
<p>If <span class="math">\(\vec{x}, \vec{y}, \vec{z} \in \mb{R}^n, s, t, \in \mb{R}\)</span>, then:</p>
<ul>
<li><span class="math">\(\vec{x} \cdot \vec{x} \ge 0\)</span>, and <span class="math">\(\vec{x} \cdot \vec{x} = 0 \iff \vec{x} = \vec{0}\)</span>.</li>
<li><span class="math">\(\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}\)</span>.</li>
<li><span class="math">\(\vec{x} \cdot (s\vec{y} + t\vec{z}) = s (\vec{x} \cdot \vec{y}) + t (\vec{x} \cdot \vec{z})\)</span>.</li>
</ul>
<p>The <strong>length</strong> or <strong>norm</strong> of a vector <span class="math">\(\vec{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_k \end{bmatrix}\)</span> is <span class="math">\(\magn{\vec{v}} = \sqrt{\sum_{i = 1}^k v_i^2}\)</span>. Note the similarity to scalars, where <span class="math">\(\abs{x} = \sqrt{x^2}\)</span>.</p>
<p>A vector of length 1 is a unit vector. A unit vector is therefore one such that <span class="math">\(\sum_{i = 1}^k v_i^2 = 1\)</span>.</p>
<p>Also, <span class="math">\(\vec{x} \cdot \vec{x} = \magn{x}^2\)</span>, which should be obvious since <span class="math">\(\theta = 0\)</span> and <span class="math">\(\cos \theta = 1\)</span>, so <span class="math">\(\vec{x} \cdot \vec{x} = \magn{x} \magn{x} = \magn{x}^2\)</span>.</p>
<h3 id="theorem-1.3.3">Theorem 1.3.3</h3>
<p>If <span class="math">\(\vec{x}, \vec{y} \in \mb{R}^n, c \in \vec{R}\)</span>, then:</p>
<ul>
<li><span class="math">\(\magn{\vec{x}} \ge 0\)</span>, and <span class="math">\(\magn{\vec{x}} = 0 \iff \vec{x} = \vec{0}\)</span></li>
<li><span class="math">\(\magn{cx} = \abs{c}\magn{\vec{x}}\)</span></li>
<li><span class="math">\(\abs{\vec{x} \cdot \vec{y}} \le \magn{\vec{x}}\magn{\vec{y}}\)</span> (Cauchy-Schwarz-Buniakowski Inequality)</li>
<li><span class="math">\(\magn{\vec{x} + \vec{y}} \le \magn{\vec{x}} + \magn{\vec{y}}\)</span> (Triangle Inequality)</li>
</ul>
<p>Proof of third consequence:</p>
<blockquote>
<p>Clearly, if <span class="math">\(\vec{x} = \vec{0}\)</span> then the inequality holds. Assume <span class="math">\(\vec{x} \ne \vec{0}\)</span>.<br />Clearly, <span class="math">\(\forall t \in \mb{R}, (t\vec{x} + \vec{y}) \cdot (t\vec{x} + \vec{y}) \ge 0\)</span> (from properties of dot product), so <span class="math">\(t^2(\vec{x} \cdot \vec{x}) + 2t(\vec{x} \cdot \vec{y}) + \vec{y} \cdot \vec{y} \ge 0\)</span>. Clearly, this is a quadratic polynomial where <span class="math">\(t\)</span> is the variable. The polynomial is greater or equal to 0 if and only if it has at most 1 root.<br />So the discriminant in the quadratic formula, <span class="math">\(b^2 - 4ac\)</span>, must be less or equal to 0.<br />So <span class="math">\(4(\vec{x} \cdot \vec{y})^2 - 4(\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y}) \le 0\)</span>.<br />So <span class="math">\((\vec{x} \cdot \vec{y})^2 \le (\vec{x} \cdot \vec{x})(\vec{y} \cdot \vec{y})\)</span> and <span class="math">\((\vec{x} \cdot \vec{y})^2 \le \magn{x}^2\magn{y}^2\)</span>.<br />So <span class="math">\(\vec{x} \cdot \vec{y} \le \magn{x}\magn{y}\)</span>.</p>
</blockquote>
<h3 id="angle">Angle</h3>
<p>The <strong>angle</strong> between two vectors <span class="math">\(\vec{x}\)</span> and <span class="math">\(\vec{y}\)</span> is defined as the angle <span class="math">\(\theta\)</span> such that <span class="math">\(\vec{x} \cdot \vec{y} = \magn{x} \magn{y} \cos \theta\)</span>.</p>
<p>In other words, <span class="math">\(\theta = \arccos\left(\frac{\vec{x} \cdot \vec{y}}{\magn{x} \magn{y}}\right)\)</span>.</p>
<p>Two vectors are <strong>orthogonal</strong> if the angle between them is 90 degrees - if their dot product is 0.</p>
<p>One of the reasons the standard bases are easy to work with is specifically because they are orthogonal to each other and because they are unit vectors.</p>
<h3 id="planeshyperplanes">Planes/Hyperplanes</h3>
<p>We can use the dot product to derive a <strong>scalar equation</strong> form for a plane, that makes use of the fact that the normal of a plane is just a vector.</p>
<p>Let <span class="math">\(A(a_1, a_2, a_3)\)</span> be a fixed point on the plane. Let <span class="math">\(X(x_1, x_2, x_3)\)</span> be an arbitrary point on the plane.</p>
<p>Then <span class="math">\(\vec{X} - \vec{A}\)</span> is a vector that lies on the plane.</p>
<p>In other words, if <span class="math">\(\vec{x} = s\vec{u} + t\vec{v}, s, t \in \mb{R}\)</span>, then <span class="math">\(\vec{x} - \vec{b}\)</span> is clearly a vector that lies on the plane.</p>
<p>Let <span class="math">\(\vec{n}\)</span> be a vector normal to the plane. Clearly, <span class="math">\(\vec{n} \cdot (\vec{X} - \vec{A}) = 0\)</span>.</p>
<p>So <span class="math">\(n_1 (x_1 - a_1) + n_2 (x_2 - a_2) + n_3 (x_3 - a_3) = 0 = n_1 x_1 + n_2 x_2 + n_3 x_3 - n_1 a_1 - n_2 a_2 - n_3 a_3\)</span>.</p>
<p>So <span class="math">\(n_1 x_1 + n_2 x_2 + n_3 x_3 = n_1 a_1 + n_2 a_2 + n_3 a_3\)</span>. Since <span class="math">\(\vec{A}\)</span> is fixed, <span class="math">\(n_1 a_1 + n_2 a_2 + n_3 a_3\)</span> is constant and the equation is a function of <span class="math">\(x_1, x_2, x_3\)</span>.</p>
<p>Since <span class="math">\(\vec{X}\)</span> is arbitrary, this holds for every point on the plane, and so is an equation of the plane.</p>
<p>Using the above, we can find the scalar equation of any plane given the normal and a fixed point on the plane.</p>
<p>If we extend this to hyperplanes, ;wip</p>
<h3 id="cross-product">Cross Product</h3>
<p>However, we will typically be given the vector equation of the plane, like <span class="math">\(\vec{x} = c_1 \vec{u} + c_2 \vec{v} + \vec{w}\)</span>.</p>
<p>We can calculate the normal of the plane by finding two linearly independent vectors that lie on the plane, and finding a vector that is orthogonal to both of them.</p>
<p>Clearly, <span class="math">\(\vec{u}\)</span> and <span class="math">\(\vec{v}\)</span> lie on the plane, being the solutions when <span class="math">\(c_2 = 0\)</span> or <span class="math">\(c_1 = 0\)</span>, respectively.</p>
<p>Then for the normal <span class="math">\(\vec{n}\)</span> we know <span class="math">\(\vec{n} \cdot \vec{u} = \vec{n} \cdot \vec{v} = 0\)</span>. So <span class="math">\(n_1 u_1 + n_2 u_2 + n_3 u_3 = n_1 v_1 + n_2 v_2 + n_3 v_3 = 0\)</span>.</p>
<p>Solving two equations for three unknowns, we find that one possible solution is <span class="math">\(\vec{n} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}\)</span>.</p>
<p>This problem is so common that we gave its solution a name, the <strong>cross product</strong>. The cross product of two vectors <span class="math">\(\vec{u}\)</span> and <span class="math">\(\vec{v}\)</span> is <span class="math">\(\vec{u} \times \vec{v} = \begin{bmatrix} u_2 v_3 - u_3 v_2 \\ u_3 v_1 - u_1 v_3 \\ u_1 v_2 - u_2 v_1 \end{bmatrix}\)</span>, and is always a vector that is orthogonal to both vectors.</p>
<p>Note that this operation is only defined in <span class="math">\(\mb{R}^3\)</span> and <span class="math">\(\mb{R}^7\)</span>.</p>
<h2 id="projections">Projections</h2>
<p>Given the vectors <span class="math">\(\vec{u}, \vec{v}, \vec{w} \in \mb{R}^n, \vec{v} \ne \vec{0}, \vec{v} \cdot \vec{w} = 0\)</span>, we want to write <span class="math">\(\vec{u}\)</span> as the sum of a scalar multiple of <span class="math">\(\vec{v}\)</span> and <span class="math">\(\vec{w}\)</span>, as the vector <span class="math">\(\vec{u} = c\vec{v} + \vec{w}, c \in \mb{R}\)</span>.</p>
<p>We first need to find out how much of <span class="math">\(\vec{u}\)</span> is in the direction of <span class="math">\(\vec{v}\)</span> - find <span class="math">\(c\)</span>. Clearly, <span class="math">\(\vec{u} \cdot \vec{v} = (c\vec{v} + \vec{w}) \cdot \vec{v} = c\magn{v}^2 + \vec{w} \cdot \vec{v} = c\magn{v}^2\)</span>.</p>
<p>So <span class="math">\(c = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}\)</span>.</p>
<p>The <strong>projection</strong> of <span class="math">\(\vec{u}\)</span> onto <span class="math">\(\vec{v}\)</span> is defined as <span class="math">\(\proj_{\vec{v}} \vec{u} = c\vec{v} = \frac{\vec{u} \cdot \vec{v}}{\magn{v}^2}\vec{v}\)</span>, and is the vector along the same direction as <span class="math">\(\vec{v}\)</span> such that it has the same extent along <span class="math">\(\vec{v}\)</span> as <span class="math">\(\vec{u}\)</span>.</p>
<p>The <strong>perpendicular</strong> of <span class="math">\(\vec{u}\)</span> onto <span class="math">\(\vec{v}\)</span> is the vector that when added to the projection, results in <span class="math">\(\vec{u}\)</span>. Therefore, the perpendicular is <span class="math">\(\prp_{\vec{v}} \vec{u} = \vec{u} - \proj_{\vec{v}} \vec{u}\)</span>.</p>
<h3 id="planes">Planes</h3>
<p>How do we project a vector onto a plane? We can notice that the projection of a vector onto a plane is the perpendicular of the vector projected onto the normal of the plane.</p>
<p>Therefore, the projection of a vector <span class="math">\(\vec{v}\)</span> onto a plane with normal <span class="math">\(\vec{n}\)</span> is <span class="math">\(\prp_{\vec{n}} \vec{v}\)</span>.</p>
<h2 id="systems-of-linear-equations">Systems of Linear Equations</h2>
<p>A <strong>linear equation</strong> is an equation of the form <span class="math">\(a_1 x_1 + \ldots + a_n x_n = b, a_k, b \in \mb{C}, 1 \le k \le n, n \in \mb{N}\)</span>. This also includes equations that can be rewritten into this form.</p>
<p>A set of linear equations with the same variables <span class="math">\(x_1, \ldots, x_n\)</span> (including those where there are zero coefficients) is called a <strong>system of linear equations</strong>.</p>
<p>A system of linear equations can be written as <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span>. Here, <span class="math">\(a_{i, j}\)</span> represents the <span class="math">\(j\)</span>th coefficient in the <span class="math">\(i\)</span>th equation.</p>
<p>A <strong>solution</strong> to a system of linear equation is a vector <span class="math">\(\vec{s} = \begin{bmatrix} s_1 \\ \vdots \\ s_n \end{bmatrix}, \vec{s} \in \mb{R}^n\)</span> such that if <span class="math">\((\forall 1 \le i \le n, x_i = s_i)\)</span>, all the equations in the system are satisfied.</p>
<p>A system of linear equations is <strong>consistent</strong> if it has at least one solution, and <strong>inconsistent</strong> otherwise.</p>
<p>Two systems of linear equations are <strong>equivalent</strong> if and only if they both have the same solution set.</p>
<p>Interpreted geometrically, a system of <span class="math">\(m\)</span> linear equations with <span class="math">\(n\)</span> variables is a set of <span class="math">\(m\)</span> hyperplanes in <span class="math">\(\mb{R}^n\)</span>. The solution to this system is represented by a vector that lies on all these hyperplanes.</p>
<h3 id="theorem-2.1.1">Theorem 2.1.1</h3>
<p>If a set of linear equations is consistent with more than 1 solution, then it has infinite solutions.</p>
<p>In other words, if <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> has solutions <span class="math">\(\vec{s}\)</span> and <span class="math">\(\vec{t}\)</span> such that <span class="math">\(\vec{s} \ne \vec{t}\)</span>, then <span class="math">\(\vec{s} + c(\vec{s} - \vec{t})\)</span> is a solution for all <span class="math">\(c \in \mb{R}\)</span>.</p>
<p>Proof:</p>
<blockquote>
<p>Assume <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> has solutions <span class="math">\(\vec{s}\)</span> and <span class="math">\(\vec{t}\)</span>.<br />Clearly, for all <span class="math">\(1 \le i \le n\)</span>, <span class="math">\(a_{i, 1} (s_1 + c(s_1 - t_1)) + \ldots + a_{i, n} (s_n + c(s_n - t_n)) = (a_{i, 1} s_1 + \ldots + a_{i, n} s_n) + c(a_{i, 1} s_1 + \ldots + a_{i, n} s_n) - c(a_{i, 1} t_1 + \ldots + a_{i, n} t_n) = b_i + cb_i - cb_i = b_i\)</span>.<br />So each equation is satisfied, and the system is consistent.</p>
</blockquote>
<p>The set of all solutions to a system of linear equations is known as a <strong>solution set</strong>.</p>
<h3 id="solving">Solving</h3>
<p>When we use the substitution and elimination techniques for solving systems of linear equations, we are forming new systems of lienar equations that have the same solution set as the original. We aim to obtain one that is easier to find the solution set for, and therefore solve the original system.</p>
<p>Note that when we solve a linear system, we don't really need the <span class="math">\(x\)</span> variables. Instead, we could write the system <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> more concisely as <span class="math">\(\left[\begin{array}{ccc|c} a_{1, 1} &amp; \ldots &amp; a_{1, n} &amp; b_1 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} &amp; b_1 \\ \end{array}\right]\)</span>.</p>
<p>This is called the <strong>augmented matrix</strong> of the system of linear equations.</p>
<p>The <strong>coefficient matrix</strong> of the system of linear equations is the same thing, but without the last column containing the constant value. In this case, it would be <span class="math">\(\left[\begin{array}{ccc} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \\ \end{array}\right]\)</span>.</p>
<p>We can combine a coefficient matrix <span class="math">\(A\)</span> and a constant vector <span class="math">\(\vec{b}\)</span> into an augmented matrix by using the <span class="math">\(\left[A \middle| \vec{b}\right]\)</span>. This simply adds <span class="math">\(\vec{b}\)</span> as a column to the end of <span class="math">\(A\)</span>.</p>
<p>Solve <span class="math">\(\begin{cases} 2x_1 + 3x_2 &amp;= 11 \\ 3x_1 + 6x_2 &amp;= 7 \\ \end{cases}\)</span>:</p>
<blockquote>
<p>We multiply the first equation by <span class="math">\(-3\)</span> to obtain <span class="math">\(-6x_1 - 9x_2 = -33\)</span>.<br />We multiply the second equation by <span class="math">\(2\)</span> to obtain <span class="math">\(6x_1 + 12x_2 = 14\)</span>.<br />Now we add the first equation to the second equation to obtain <span class="math">\(0x_1 + 3x_2 = -19\)</span>.<br />Now we multiply the second equation by <span class="math">\(\frac{1}{3}\)</span> to obtain <span class="math">\(0x_1 + x_2 = -\frac{19}{3}\)</span>.<br />Now we add the second equation, multiplied by <span class="math">\(9\)</span>, to the first equation to obtain <span class="math">\(-6x_1 + 0x_2 = -90\)</span>.<br />Now we multiply the first equation by <span class="math">\(-6\)</span> to obtain <span class="math">\(x_1 + 0x_2 = 15\)</span>.<br />So <span class="math">\(x_1 = 15\)</span> and <span class="math">\(x_2 = -\frac{19}{3}\)</span>.</p>
</blockquote>
<p>Now solve using operations on the matrix form of the equations, <span class="math">\(\left[\begin{array}{cc|c} 2 &amp; 3 &amp; 11 \\ 3 &amp; 6 &amp; 7 \\ \end{array}\right]\)</span>:</p>
<blockquote>
<p>We multiply the first row by <span class="math">\(-3\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 3 &amp; 6 &amp; 7 \\ \end{array}\right]\)</span>.<br />We multiply the second row by <span class="math">\(2\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 6 &amp; 12 &amp; 14 \\ \end{array}\right]\)</span>.<br />We add the first to the seocnd to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 3 &amp; -19 \\ \end{array}\right]\)</span>.<br />We multiply the second equation by <span class="math">\(\frac{1}{3}\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; -9 &amp; -33 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />Now we add the second equation multiplied by 9 to the first equation to obtain <span class="math">\(\left[\begin{array}{cc|c} -6 &amp; 0 &amp; -90 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />Now we multiply the first equation by <span class="math">\(-\frac{1}{6}\)</span> to obtain <span class="math">\(\left[\begin{array}{cc|c} 1 &amp; 0 &amp; 15 \\ 0 &amp; 1 &amp; -\frac{19}{3} \\ \end{array}\right]\)</span>.<br />So <span class="math">\(x_1 = 15\)</span> and <span class="math">\(x_2 = -\frac{19}{3}\)</span>.</p>
</blockquote>
<p>Note that at every step, we had a system of linear equations that had the same solution set as the original system. Eventually, we reduced the matrix down into a form that we could easily read the values off of.</p>
<p>Also note that there were only really two different operations that we used to solve the system. To make it easier for computers to work with, we define an additional swapping operation:</p>
<ul>
<li>Multiplying a row by a non-zero scalar (<span class="math">\(cR_i\)</span> multiplies the <span class="math">\(i\)</span>th row by <span class="math">\(c \in \mb{R}, c \ne 0\)</span>).</li>
<li>Adding a multiple of one row to another (<span class="math">\(R_i + cR_j\)</span> adds the <span class="math">\(j\)</span>th row multiplied by <span class="math">\(c \in \mb{R}, c \ne 0\)</span> to the <span class="math">\(i\)</span>th row).</li>
<li>Swapping two rows (<span class="math">\(R_i \leftrightarrow R_j\)</span>).</li>
</ul>
<p>These operations we call the <strong>elementary row operations</strong> (EROs). Note that they are also fully reversible - all EROs have an inverse that undoes the effect of the operation. This is trivial to prove.</p>
<p>Two matrices are <strong>row equivalent</strong> if one can be transformed into another by application of EROs. Since EROs are reversible, if a matrix <span class="math">\(A\)</span> is row equivalent to <span class="math">\(B\)</span>, then <span class="math">\(B\)</span> is also transformable into <span class="math">\(A\)</span> via the inverse of those EROs, and so <span class="math">\(B\)</span> is row equivalent to <span class="math">\(A\)</span>. In other words, row equivalence is commutative.</p>
<p>A matrix of size <span class="math">\(m \times n\)</span> (&quot;<span class="math">\(m\)</span> by <span class="math">\(n\)</span>&quot;) has <span class="math">\(m\)</span> rows and <span class="math">\(n\)</span> columns. With <span class="math">\(x_{i, j}\)</span>, rows are indexed by <span class="math">\(i\)</span>, and columns by <span class="math">\(j\)</span>. This is called row-major ordering.</p>
<h3 id="row-reduced-echelon-form">Row Reduced Echelon Form</h3>
<p>A matrix is in <strong>reduced row echelon form/row canonical form</strong> (RREF) if and only if:</p>
<ol style="list-style-type: decimal">
<li>All rows with only zeroes are at the bottom. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 2 \end{bmatrix}\)</span>.</li>
<li>The first non-zero entry in each row is 1 (this is called the <strong>leading one</strong>). For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 4 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 3 \end{bmatrix}\)</span>.</li>
<li>The leading one in each row, if it exists, appears to the right of the leading one of the row above it. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 1 &amp; 0 \\ 1 &amp; 0 \end{bmatrix}\)</span>.</li>
<li>The leading one in each row must be the only non-zero entry in its column. For example, <span class="math">\(\begin{bmatrix} 1 &amp; 0 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}\)</span>, but not <span class="math">\(\begin{bmatrix} 1 &amp; 3 &amp; 5 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}\)</span>.</li>
</ol>
<p>For augmented matrices, we check only the values in all but the last column when seeing if it is in RREF (the values on the left of the vertical line). In other words, we ignore the last column when checking for RREF form.</p>
<p>All matrices are representable in RREF. All matrices have one and only one representation in RREF.</p>
<h3 id="theorem-2.2.2">Theorem 2.2.2</h3>
<p>The RREF of a matrix is unique. In other words, each matrix has at most one possible matrix that it is both row equivalent to, and is in RREF.</p>
<p>Proof:</p>
<blockquote>
<p>;wip: something about proving this in chapter 4</p>
</blockquote>
<p>Row reduce <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \end{array}\)</span>:</p>
<blockquote>
<p><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 2 &amp; 4 &amp; 1 &amp; -16 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\)</span> via <span class="math">\(R_2 + (-2)R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ \end{array}\)</span> via <span class="math">\(R_2 \leftrightarrow R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; -1 &amp; -34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\((-1)R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 1 &amp; 2 &amp; 1 &amp; 9 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 1 &amp; 16 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\(R_2 - R_1\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 1 &amp; 16 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\(R_2 - R_3\)</span>.<br /><span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 0 &amp; -7 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> is row equivalent to <span class="math">\(\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 11 \\ 0 &amp; 1 &amp; 0 &amp; -18 \\ 0 &amp; 0 &amp; 1 &amp; 34 \\ \end{array}\)</span> via <span class="math">\(R_2 - R_3\)</span>.</p>
</blockquote>
<p>Note the general technique used in reducing the matrix. First, we want to obtain a triangle of 0 elements with leading ones diagonally rightwards, by going from top to bottom. Then, we make sure each leading one is the only non-zero element in its column by subtracting from rows below from bottom to top.</p>
<p>This technique is basically modifying the matrix until all the properties except for property 4. Then property 4 can be solved for relatively easily.</p>
<h3 id="solution-set">Solution Set</h3>
<p>A matrix is inconsistent if and only if its RREF contains a row of the form <span class="math">\(\begin{array}{ccc|c} 0 &amp; \ldots &amp; 0 &amp; 1 \end{array}\)</span> - no possible values can satisfy this equation (<span class="math">\(0 = 1\)</span>), so there are no solutions.</p>
<p>Recall that if consistent, a system of linear equations has either one or infinitely many solutions.</p>
<p>Solve <span class="math">\(\begin{cases} x_1 + x_2 + x_3 &amp;= 4 \\ x_2 + x_3 &amp;= 3 \end{cases}\)</span>:</p>
<blockquote>
<p>The augmented matrix is <span class="math">\(\begin{array}{ccc|c} 1 &amp; 1 &amp; 1 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 3 \end{array}\)</span>.<br />The matrix in RREF is <span class="math">\(\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 &amp; 3 \end{array}\)</span>.<br />Note that this corresponds to <span class="math">\(x_1 = 1\)</span> and <span class="math">\(x_2 + x_3 = 3\)</span>, and that the second equation has infinitely many solutions.<br />We can find the general solution by writing everything in terms of the fewest variables possible.<br />Clearly, <span class="math">\(x_3 = 3 - x_2\)</span>, so the solution is <span class="math">\(\vec{x} = \begin{bmatrix} 1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 1 \\ x_2 \\ 3 - x_2 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 3 \end{bmatrix} + x_2\begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}\)</span>.<br />We can denote this as <span class="math">\(\vec{x} = \begin{bmatrix} 1 \\ 0 \\ 3 \end{bmatrix} + s\begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix}, s \in \mb{R}\)</span>, by assigning <span class="math">\(x_2\)</span> to the parameter <span class="math">\(s\)</span>.<br />This represents a line at the intersection of two planes in <span class="math">\(\mb{R}^3\)</span>.</p>
</blockquote>
<p>The solution set is, geometrically, either a point, a line, a plane, a hyperplane, or empty (no solutions). If there is only one solution, the solution set is a point.</p>
<p>If a <strong>column</strong> in a coefficient matrix has <strong>no leading ones</strong>, the corresponding variable is called a <strong>free variable</strong>. A free variable is a variable that can be any value. Free variables exists if and only if there are an infinite number of solutions.</p>
<h3 id="solving-linear-systems">Solving Linear Systems</h3>
<p>We can take this technique and generalize it a bit to solve any linear system, or show it to not have any solutions:</p>
<ol style="list-style-type: decimal">
<li>Write the augmented matrix for the system.</li>
<li>Row reduce the matrix.</li>
<li>Write the linear system for the new, reduced matrix.</li>
<li>Check for equations of the form <span class="math">\(0 = b, b \ne 0\)</span>, and stop and flag as inconsistent if found.</li>
<li>Write all variables in terms of the free variables.</li>
<li>Assign a parameter to each free variable.<br /></li>
<li>Write the components that are not already fully solved in terms of the free variables.</li>
<li>Write the solution as a linear combination of vectors using vector operations.</li>
</ol>
<p>The standard technique for row reducing matrices is Guass-Jordan elimination. However, this is not always the most efficient way to do it, and there are a lot of tricks that can be used to speed it up in some special cases.</p>
<p>;wip: rewrite my linear solver lib to multiply only, and divide only at the end, this is precision-preserving through bignum and gmp</p>
<p>However, we don't always need to solve a system. Sometimes, we just need to figure out if there are zero, one, or infinite solutions. We will now look at some tools that can be used to analyze this.</p>
<p>The <strong>rank</strong> of a matrix is the number of leading ones in the RREF of the matrix. This is equivalent to the number of variables minus the number of free variables.</p>
<p>The rank of a matrix <span class="math">\(A\)</span> is denoted <span class="math">\(\rank A\)</span>.</p>
<h3 id="theorem-2.2.3">Theorem 2.2.3</h3>
<p>Given an <span class="math">\(m \times n\)</span> coefficient matrix <span class="math">\(A\)</span> for a linear system:</p>
<ul>
<li>If <span class="math">\(\rank A &lt; \rank \sys{A}{\vec{b}}\)</span>, then the system is inconsistent, where <span class="math">\(\vec{b}\)</span> is the right hand side vector for the linear system. In other words, if the rank of the coefficent matrix is less than that of the augmented matrix, then the matrix is inconsistent.</li>
<li>If <span class="math">\(\sys{A}{\vec{b}}\)</span> is consistent, then there are <span class="math">\(n - \rank A\)</span> free variables. If there are 0 free variables, then the system has only one solution.</li>
<li><span class="math">\(\rank A = m\)</span> if and only if the system <span class="math">\(\sys{A}{\vec{b}}\)</span> is consistent for every <span class="math">\(\vec{b} \in \mb{R}^m\)</span>. In other words, there are no free variables if and only if the the system is consistent for every possible vector of right hand side values.</li>
</ul>
<p>Proof of first:</p>
<p>(point 1)</p>
<blockquote>
<p>Assume <span class="math">\(\rank A &lt; \rank \sys{A}{\vec{b}}\)</span>.<br />Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />This corresponds to <span class="math">\(0 = 1\)</span>, which makes the system inconsistent.</p>
</blockquote>
<p>Proof:</p>
<p>(point 2)</p>
<blockquote>
<p>Assume <span class="math">\(\sys{A}{\vec{b}}\)</span> is consistent.<br />The number of free variables is the number of variables minus the number of those that are not free.<br />Those that are not free have leading ones, so <span class="math">\(\rank A\)</span> is the number of non-free variables.<br />So the number of free variables is <span class="math">\(n - \rank A\)</span>.</p>
</blockquote>
<p>Proof:</p>
<p>(point 3)</p>
<blockquote>
<p>First, we will prove that if the system <span class="math">\(\sys{A}{\vec{b}}\)</span> is not consistent for every <span class="math">\(\vec{b} \in \mb{R}^m\)</span>, then <span class="math">\(\rank A \ne m\)</span>.<br />Suppose <span class="math">\(\sys{A}{\vec{b}}\)</span> is inconsistent for some <span class="math">\(\vec{b} \in \mb{R}^m\)</span>.<br />Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />This row does not contain a leading one, so <span class="math">\(\rank A &lt; m\)</span>.<br />Now we will prove that if <span class="math">\(\rank A \ne m\)</span>, then the system <span class="math">\(\sys{A}{\vec{b}}\)</span> is not consistent for every <span class="math">\(\vec{b} \in \mb{R}^m\)</span>.<br />Suppose <span class="math">\(\rank A \ne m\)</span>. Then <span class="math">\(\rank A &lt; m\)</span>. Then there is a row of all zeroes in the RREF of A.<br />Then there must be a row in the RREF of the augmented matrix with all zeros except the last element, which is 1.<br />Then the system is inconsistent.</p>
</blockquote>
<h3 id="homogenous-systems">Homogenous Systems</h3>
<p>A system is <strong>homogenous</strong> if and only if the right-hand side contains only zeros. In other words, it has the form <span class="math">\(\sys{A}{\vec{0}}\)</span>.</p>
<p>A homogenous matrix is one where the last column is all 0 - a matrix corresponding to a homogenous system.</p>
<p>Note that since the right side is always 0, the RREF of a homogenous matrix also has the right side always 0.</p>
<p>Also note that it is impossible to get a row where every element is 0 except the right hand side. Therefore, <strong>homogenous systems are always consistent</strong>.</p>
<p>For example, <span class="math">\(\vec{0}\)</span> is always a solution to a homogenous system. This is called the <strong>trivial solution</strong>.</p>
<p>When we row reduce a homogenous matrix, we can save space by just row reducing the coefficient matrix instead. The last column will all be 0 anyways.</p>
<h3 id="theorem-2.2.4">Theorem 2.2.4</h3>
<p>Given an <span class="math">\(m \times n\)</span> matrix <span class="math">\(A\)</span>, <span class="math">\(\rank A \le \min{m, n}\)</span>.</p>
<h3 id="theorem-2.2.5">Theorem 2.2.5</h3>
<p>The solution set of a homogenous system with <span class="math">\(n\)</span> variables is a subspace of <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Note that the solution set of a homogenous system written as a vector equation always has <span class="math">\(\vec{0}\)</span> as the constant term. That means that all solutions are linear combinations of a few vectors.</p>
<p>As a result, we can always write the solution set of a homogenous system as the span of a set of vectors.</p>
<p>Because all spans are subspaces (by Theorem 1.2.2), the solution set is a subspace.</p>
<p>Because the solution set is a subspace, we often call it the <strong>solution space</strong> of a aystem.</p>
<h1 id="section">30/1/14</h1>
<h2 id="matrices">Matrices</h2>
<p>Matrices are abstract objects like vectors. We usually denote matrix variables with capital letters.</p>
<p>An <span class="math">\(m \times n\)</span> matrix <span class="math">\(A\)</span> is a rectangular array with <span class="math">\(m\)</span> rows and <span class="math">\(n\)</span> columns. We represent the <span class="math">\(i\)</span>th row and <span class="math">\(j\)</span>th column as <span class="math">\(a_{i, j}\)</span>, or <span class="math">\((A)_{i, j}\)</span>.</p>
<p>In other words, <span class="math">\(A = \begin{bmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; \vdots \\ a_{m, 1} &amp; \ldots &amp; a_{m, n} \end{bmatrix}\)</span>.</p>
<p>All <span class="math">\(m \times n\)</span> matrices belong to the set <span class="math">\(M_{m \times n}\)</span>, similar to how all <span class="math">\(n\)</span>-dimensional vectors belong to the set <span class="math">\(\mb{R}^n\)</span>.</p>
<h3 id="operations-1">Operations</h3>
<p>Let <span class="math">\(A, B, C\)</span> be matrices.</p>
<p>Equality: <span class="math">\(A = B \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (A)_{i, j} = (B)_{i, j}\)</span>.</p>
<p>In other words, two matrices are equal if and only if they are both the same size and contain the same entries.</p>
<p>Addition/subtraction: <span class="math">\(A \pm B = C \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (C)_{i, j} = (A)_{i, j} \pm (B)_{i, j}\)</span>.</p>
<p>In other words, when we add matrices (only defined for matrices of the same size), we simply add their corresponding entries.</p>
<p>Scalar multiplication: <span class="math">\(t \in \mb{R}; tA = C \iff \forall 1 \le i \le m, \forall 1 \le j \le n, (C)_{i, j} = t(A)_{i, j}\)</span>.</p>
<p>In other words, when we multiply a matrix by a scalar or a scalar by a matrix, we simply multiply each entry in the matrix by the scalar.</p>
<p>With these operations, we can take linear combinations of matrices.</p>
<h3 id="theorem-3.1.1">Theorem 3.1.1</h3>
<p>For all <span class="math">\(A, B, C \in M_{m \times n}, s, t, \in \mb{R}\)</span>:</p>
<ul>
<li>Closure under addition: <span class="math">\(A + B \in M_{m \times n}\)</span>.</li>
<li>Associativity: <span class="math">\((A + B) + C = A + (B + C)\)</span></li>
<li>Commutativity: <span class="math">\(A + B = B + A\)</span></li>
<li>Additive identity: <span class="math">\(\exists 0_{m, n} \in \mathbb{R}, \forall A \in M_{m, n}, A + 0_{m, n} = A\)</span> (<span class="math">\(0_{m, n}\)</span> is called the zero matrix).</li>
<li>Additive inverse: <span class="math">\(\forall A \in M_{m, n}, \exists -A \in M_{m, n}, A + (-A) = 0_{m, n}\)</span>.</li>
<li>Closure under scalar multiplication: <span class="math">\(cA \in M_{m, n}\)</span>.</li>
<li><span class="math">\(c(dA) = (cd)A\)</span></li>
<li>Scalar distributivity: <span class="math">\((c + d)A = cA + dA\)</span>.</li>
<li>Vector distributivity: <span class="math">\(c(A + B) = cA + cB\)</span>.</li>
<li>Multiplicative identity: <span class="math">\(1A = A\)</span>.</li>
</ul>
<h3 id="transpose">Transpose</h3>
<p>Vectors and matrices are closely related. We have been writing vectors as columns for a while now. In fact, we can use vectors to represent the columns of a matrix.</p>
<p>For example, if <span class="math">\(\vec{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \vec{v} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}\)</span>, then <span class="math">\(\begin{bmatrix} \vec{u} &amp; \vec{v} \end{bmatrix} = \begin{bmatrix} 1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6 \end{bmatrix}\)</span>.</p>
<p>The <strong>transpose</strong> of a matrix <span class="math">\(A \in M_{m, n}\)</span> is <span class="math">\(A^T \in M_{n, m}\)</span> (&quot;A transposed&quot;), a matrix such that <span class="math">\((A)_{i, j} = (A^T)_{j, i}\)</span>.</p>
<p>Note that the dimensions were swapped around. Basically, the rows became the columns, and the columns became the rows - we reflected the entries around a diagonal line starting from the top left.</p>
<ul>
<li><span class="math">\(A = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}; A^T = \begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix}\)</span>.</li>
<li><span class="math">\(B = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{bmatrix}; B^T = \begin{bmatrix} 1 &amp; 3 &amp; 5 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}\)</span>.</li>
<li><span class="math">\(C = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}; C^T = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{bmatrix}\)</span>.</li>
</ul>
<p>Some matrices satisfy the property <span class="math">\(A = A^T\)</span>. These matrices are called <strong>symmetric</strong>, because they are symmetric about the diagonal line. Note that only square matrices can be symmetric.</p>
<p>Transposes are useful for representing the rows of a matrix with vectors, rather than columns.</p>
<p>For example, if <span class="math">\(\vec{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \vec{v} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}\)</span>, then <span class="math">\(\begin{bmatrix} \vec{u}^T \\ \vec{v}^T \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}\)</span>.</p>
<h3 id="theorem-3.1.2">Theorem 3.1.2</h3>
<p>For all <span class="math">\(A, B \in M_{m, n}, c \in \mb{R}\)</span>:</p>
<ul>
<li><span class="math">\((A^T)^T = A\)</span></li>
<li><span class="math">\((A + B)^T = A^T + B^T\)</span></li>
<li><span class="math">\((cA)^T = c(A^T)\)</span></li>
</ul>
<p>Proof:</p>
<p>(proof of first)</p>
<blockquote>
<p>Clearly, <span class="math">\(\forall 1 \le i \le m, \forall 1 \le j \le n, ((A^T)^T)_{i, j} = (A^T)_{j, i} = (A)_{i, j}\)</span>.<br />So <span class="math">\((A^T)^T = A\)</span>.</p>
</blockquote>
<p>(proof of second)</p>
<blockquote>
<p>Clearly, <span class="math">\(\forall 1 \le i \le m, \forall 1 \le j \le n, ((A + B)^T)_{i, j} = (A + B)_{j, i} = (A)_{j, i} + (B)_{j, i} = (A^T)_{i, j} + (B^T)_{i, j}\)</span>.<br />So <span class="math">\((A + B)^T = A^T + B^T\)</span>.</p>
</blockquote>
<p>(proof of third)</p>
<blockquote>
<p>Clearly, <span class="math">\(\forall 1 \le i \le m, \forall 1 \le j \le n, ((cA)^T)_{i, j} = (cA)_{j, i} = c((A)_{j, i}) = c((A^T)_{i, j})\)</span>.<br />So <span class="math">\((cA)^T = c(A^T)\)</span>.</p>
</blockquote>
<h1 id="section-1">1/2/14</h1>
<h2 id="matrix-multiplication">Matrix Multiplication</h2>
<h3 id="matrix-vector-multiplication">Matrix-Vector Multiplication</h3>
<p>Consider the linear system <span class="math">\(\sys{A}{\vec{B}}\)</span> with the solution <span class="math">\(\vec{x}\)</span>.</p>
<p>When we multiply a matrix by a vector, we want to have it so that <span class="math">\(A\vec{x} = \vec{b}\)</span>. This is because it would be very useful to help solve systems of linear equations.</p>
<p>Matrix-vector multiplication can be defined in terms of matrices with represented with row vectors or column vectors.</p>
<p>We will first define it in terms of row vectors.</p>
<p>Let <span class="math">\(A = \begin{bmatrix} \vec{a}_1^T \\ \vdots \\ \vec{a}_m^T \end{bmatrix}\)</span> and <span class="math">\(A \in M_{m, n}\)</span>.</p>
<p>Note that <span class="math">\(\vec{a}_i^T\)</span> represents the coefficients of the <span class="math">\(i\)</span>th equation of the system. So if and only if <span class="math">\(\vec{x}\)</span> is a solution, <span class="math">\(\vec{x} \cdot \vec{a}_i^T = b_i\)</span>.</p>
<p>Now we can define matrix multiplication: <span class="math">\(A\vec{x} = \begin{bmatrix} \vec{a}_1^T \cdot \vec{x} \\ \vdots \\ \vec{a}_m^T \cdot \vec{x} \end{bmatrix} = \vec{b}\)</span>.</p>
<p>Basically, we take the dot product of each row and assemble it together into a vector. The product of a matrix and a vector is therefore a vector.</p>
<p>Note that in order for matrix-vector multiplication to be defined, the number of columns in <span class="math">\(A\)</span> must equal the number of rows/components in <span class="math">\(\vec{x}\)</span> in order for the dot product to work. An <span class="math">\(m \times n\)</span> matrix can only be multiplied by vectors in <span class="math">\(\mb{R}^n\)</span>.</p>
<p>Now we can also define it in terms of column vectors.</p>
<p>Note that the system <span class="math">\(\begin{cases} a_{1, 1} x_1 + \ldots + a_{1, n} x_n = b_1 \\ \vdots a_{m, 1} x_1 + \ldots + a_{m, n} x_n = b_m \\ \end{cases}\)</span> has the coefficient matrix <span class="math">\(A = \begin{bmatrix} \begin{bmatrix} a_{1, 1} \\ \vdots \\ a_{m, 1} \end{bmatrix} \cdots \begin{bmatrix} a_{1, n} \\ \vdots \\ a_{m, n} \end{bmatrix} \end{bmatrix} = \begin{bmatrix} \vec{c}_1 \ldots \vec{c}_n \end{bmatrix}\)</span>, where <span class="math">\(\vec{c}_i\)</span> represents the <span class="math">\(i\)</span>th column of the matrix.</p>
<p>Note that the system can be written as <span class="math">\(x_1\begin{bmatrix} a_{1, 1} \\ \vdots \\ a_{m, 1} \end{bmatrix} + \ldots + x_n\begin{bmatrix} a_{1, n} \\ \vdots \\ a_{m, n} \end{bmatrix} = \vec{b}\)</span>.</p>
<p>So we can write the system as <span class="math">\(x_1 \vec{c}_1 + \ldots + x_n \vec{c}_n = \vec{b}\)</span>.</p>
<p>Now we define matrix-vector multiplication in terms of columns as <span class="math">\(A\vec{x} = x_1 \vec{c}_1 + \ldots + x_n \vec{c}_n\)</span>, where <span class="math">\(\vec{c}_i\)</span> represents the <span class="math">\(i\)</span>th column of <span class="math">\(A\)</span>.</p>
<p>We define it this way because now we can represent the system with <span class="math">\(A\vec{x} = \vec{b}\)</span>.</p>
<p>Note that we can use this to select one particular column of the matrix using a standard basis vector (a vector of all zeros except a lone one).</p>
<p>For example, multiplying an <span class="math">\(m \times n\)</span> matrix by the <span class="math">\(i\)</span>th standard basis vector in <span class="math">\(\mb{R}^n\)</span> (the vector with all components set to 0 except for the <span class="math">\(ith\)</span> component, which is set to 1) will result in the <span class="math">\(i\)</span>th column of the matrix.</p>
<h3 id="matrix-matrix-multiplication">Matrix-Matrix Multiplication</h3>
<p>Now we can define the product of a matrix and a matrix.</p>
<p>Given an <span class="math">\(m \times n\)</span> matrix <span class="math">\(A = \begin{bmatrix} \vec{a}_1^T \\ \vdots \\ \vec{a}_n^T \end{bmatrix}\)</span> and an <span class="math">\(n \times p\)</span> matrix <span class="math">\(B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_p \end{bmatrix}\)</span>, <span class="math">\(AB = A\begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_p \end{bmatrix} = \begin{bmatrix} A\vec{b}_1 &amp; \ldots &amp; A\vec{b}_p \end{bmatrix}\)</span> is an <span class="math">\(m \times p\)</span> matrix.</p>
<p>Basically, when we multiply a matrix by a matrix, we multiply the first matrix by each of the column vectors in the second matrix.</p>
<p>Note that this requires that the number of columns in <span class="math">\(A\)</span> must be the same as the number of rows in <span class="math">\(B\)</span>, since the dot product must be defined.</p>
<p>Note that <span class="math">\((AB)_{i, j} = \vec{a}_i^T \vec{b}_j = \vec{a}_i \cdot \vec{b}_j = \sum_{k = 1}^n (a_i^T)_k (\vec{a}_k)_j = \sum_{k = 1}^n (A)_{i, k} (B)_{k, j}\)</span>.</p>
<h3 id="theorem-3.1.3-properties-of-matrix-multiplication">Theorem 3.1.3 (Properties of Matrix Multiplication)</h3>
<p>If <span class="math">\(A, B, C\)</span> are matrices of the required dimensions, <span class="math">\(t \in \mb{R}\)</span>:</p>
<ul>
<li>Distributivity: <span class="math">\(A(B + C) = AB + AC\)</span>.</li>
<li>Scalar multiplication: <span class="math">\(t(AB) = (tA)B = A(tB)\)</span>.</li>
<li>Associativity: <span class="math">\((AB)C = A(BC)\)</span>.</li>
<li>Transposition distributivity: <span class="math">\((AB)^T = A^T B^T\)</span>.</li>
</ul>
<p>Note that matrix multiplication is not commutative over matrices: <span class="math">\(AB \ne BA\)</span> for some matrices.</p>
<h3 id="theorem-3.1.4">Theorem 3.1.4</h3>
<p>Note that <span class="math">\(A = B \implies AC = BC \wedge CA = BA\)</span>. However, <span class="math">\(AC = BC \vee CA = BA\)</span> <strong>does not imply</strong> <span class="math">\(A = B\)</span> - there is no cancellation law for matrix multiplication.</p>
<p>That said, given <span class="math">\(A, B \in M_{m, n}\)</span>, <span class="math">\((\forall \vec{x} \in \mb{R}^n, A\vec{x} = B\vec{x}) \implies A = B\)</span>.</p>
<p>In other words, if each product of a matrix with an arbitrary vector are equal to each other, then the matrices are themselves equal to each other.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(A = \begin{bmatrix} \vec{a}_1 &amp; \ldots &amp; \vec{a}_n \end{bmatrix}, B = \begin{bmatrix} \vec{b}_1 &amp; \ldots &amp; \vec{b}_n \end{bmatrix}\)</span>.<br />Let <span class="math">\(i \in \mb{N}, i \le n\)</span>.<br />Let <span class="math">\(\vec{e}_i\)</span> be the <span class="math">\(i\)</span>th standard basis vector in <span class="math">\($\mb{R}^n (the vector with all zeroes except the \)</span>i<span class="math">\(th element, which is 1).   Let \)</span> ^n<span class="math">\(. Construct \)</span> = <em>i<span class="math">\(.   Assume \)</span>A = B<span class="math">\(. Then \)</span>A</em>i = B<em>i = </em>i = _i<span class="math">\(.   Since \)</span>i$ is arbitrary, <span class="math">\(forall 1 \le i \le n, \vec{a}_i = \vec{b}_i\)</span>.<br />So <span class="math">\(A = B\)</span>.</p>
</blockquote>
<h1 id="section-2">3/2/14</h1>
<h2 id="identity-matrix">Identity Matrix</h2>
<p>The identity matrix is the matrix such that, when multiplied with any matrix, or when any matrix is multiplied by it, results in that same matrix.</p>
<p>In other words, we want to find <span class="math">\(I\)</span> such that <span class="math">\(\forall A \in M_{m, n}, AI = IA = A\)</span>.</p>
<hr>
<p>Copyright 2013 Anthony Zhang</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>.
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>