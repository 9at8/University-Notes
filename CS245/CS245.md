CS 245
======

Logic and computation.

    Instructor: Daniela Maftuleac
    Email: daniela.maftuleac@uwaterloo.ca
    Office Hours: Tuesday 2-3pm, Wednesday 12-1pm Math Tutorial Center
    Website: https://www.student.cs.uwaterloo.ca/~cs245/

$$
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
$$

Assignments are due at 1pm at the dropboxes on MC 4th floor, due on Thursdays, posted on Tuesdays, returned in tutorials. Remark requests must be done within a week after the marks are posted.

Logic
-----

Logic is the science of correct reasoning.  It is concerned with determining whether things are true or false.

In **symbolic logic**, we use symbols to represent truth values and manipulate logical statements using certain logical rules.

This course deals with propositional logic, predicate logic, and program verification.

Prove that $3 \mid (4^n + 5)$ for all $n \in \mb{N}$:

> Clearly, this is true for $n = 0$, since $3 \mid (1 + 5)$.  
> Assume for some $k \ge 0$ that $3 \mid (4^k + 5)$.  
> So $\exists p \in \mb{Z}, 3p = 4^k + 5$ and $4(3p) = 4(4^k + 5)$, so $12p - 15 = 3(4p - 5) = 4^{k + 1} + 5$.  
> Let $q = 4p - 5$. Since $q \in \mb{Z}$, $\exists q \in \mb{Z}, 3q = 4^{k + 1} + 5$.  
> So $3 \mid 4^{k + 1} + 5$, and by induction, $3 \mid 4^n + 5$ for all $n \in \mb{N}$.  

The idea is that with strong induction, we can use any of the previous cases to prove the inductive hypothesis.

# 7/5/14

Propositional Logic
-------------------

The language of propositions is propositional logic.

A proposition is a declarative statement that is either true or false. It may depend on the context, like how the proposition $x \ge 5$ depends on the contextual variable $x$.

We always assume that for any proposition with its context (the situation the proposition applies in), either the proposition is true, or it is false, and it cannot be both. If a statement is false, then its negation is true.

Propositions must be declarative. They must make sense when we ask, "is it true that PROPOSITION?". It must be assignable to either true or false.

**Simple/atomic propositions** are the building blocks of **compound propositions**. For example, "It is currently raining in Waterloo" and "It is currently 12 degrees Celsius in Waterloo" are simple propositions, and "It is either raining or 12 degrees in Waterloo" is a compound proposition.

Atomic ropositions are the smallest possible statements that are still propositions. They cannot be divided into smaller propositions.

### Expression Syntax

The propositional language is known as $\mathcal{L}_P$ (Language of Propositions). This language contains **atoms**, which are lowercase Latin characters such as $p$, $q$, and $r$, optionally with subscripts. These all belong to the set $\operatorname{Atom}(\mathcal{L}_P)$, so $p \in \operatorname{Atom}(\mathcal{L}_P)$.

There are also **punctuation** symbols, the round parentheses "(" and ")", which change the precendence of subexpressions.

There are also **connectives**, which are $\neg, \wedge, \vee, \rightarrow, \leftrightarrow$.

An **expression** is a finite sequence of symbols. The length of an expression is the number of symbols it contains. Expressions may not necessarily be valid, like $( \vee \neg \vee ($.

The empty expression (containing no symbols) is denoted $\emptyset$.

### Expression Operations

Let $U, V, W$ be expressions. Then $U$ and $V$ are equal ($U = V$) if and only if they contain the same sequence of symbols.

Then $UV$ is the concatenation of $U$ and $V$ - the expression containing the entire sequence in $U$, followed by the entire sequence in $V$.

If $U = W_1 V W_2$, then $V$ is a **segment** of $U$ - it is a sequence of symbols that $U$ also contains. All sequences contain the empty expression.

If $V$ is a segment of $U$ and $V \ne U$, then $V$ is a **proper segment** of $U$.

If $U = VW$, then $V$ is an **initial segment** and $W$ is a **terminal segment**.

### Expression Validity

The binary (two-parameter) connectives are $\wedge, \vee, \rightarrow, \leftrightarrow$. In the following, $\odot$ will represent any arbitrary one of these binary connectives. The only connective that is not binary is the unary $\neg$ connective.

The set of **formulas** is the set of all valid/well-formed expressions, and is denoted $\formlp$. Given $A, B \in \formlp$, we can define $\formlp$ as follows:

* $\operatorname{Atom}(\mathcal{L}_P) \subseteq \formlp$ - all atoms are formulas.
* $\neg A \in \formlp$ - the negation of any formula is also a formula (the set of formulas are closed under negation).
* $A \odot B \in \formlp$ - binary connectives applied to two formulas are also formulas (the set of formulas is closed under all the binary connectives).

Formulas are often represented using Roman capital letters.

Theorem 2.2.3 states the obvious consequences of the definition: all formulas of $\mathcal{L}_P$ are atoms, or of the form $\neg A, A \wedge B, A \vee B, A \rightarrow B, A \leftrightarrow B$.

Let $R(n)$ be a property (true if the property holds for $n$, false otherwise). Theorem 2.2.4 states that if $\forall p \in \operatorname{Atom}(\mathcal{L}_P), R(p)$ and $\forall A \in \formlp, R(A) \implies R(\neg A)$ and $\forall A, B \in \formlp, R(A) \wedge R(B) \implies R(A \odot B)$, then $\forall A \in \formlp, R(A)$.

In other words, if a property holds for all the forms of formulas, then it holds for all formulas.

The **type** of a formula is determined by its top-level operator. For example, $\neg A$ is a negation, $A \wedge B$ is conjunction, $A \vee b$ is a disjunction, $A \rightarrow B$ is an implication, $A \leftrightarrow B$ is an equivalence. According to theorem 2.3.3, all formulas must be one of these forms.

We want to be able to look at any expression and determine whether it is well formed. For example, $\neg()$ is not a well formed formula (WFF) because it is not obtainable through the rules that define an element in the set of formulas.

Is $(\neg a) \wedge (b \vee c)$ well formed?

> We can first notice that this is of the form $P \wedge Q$, where $P = \neg a$ and $Q = b \vee c$. By the rules, we know that the whole thing is well formed if and only if $P$ and $Q$ are both well formed.  
> Now we check if $P$ is well formed. Clearly, it is of the form $\neg R$, where $R = a$, and $a$ is an atom (which is, by definition, well formed), so by the rules, $\neg a$ is well formed.  
> Now we check if $Q$ is well formed. Clearly, it is of the form $S \vee T$, where $S = b$ and $T = c$. By the rules, $Q$ is well formed if and only if $S$ and $T$ are. Since they are atoms, they are both well formed by definition.  
> Since $P$ and $Q$ are well formed, $P \wedge Q$ is well formed and so is $(\neg a) \wedge (b \vee c)$.  

# 12/5/14

Parse Trees
-----------

The other way of testing for well-formedness is by constructing a parse tree out of the symbols in the formula. The top-level operator or atom is the root node of the tree, and the operands are its children. This is repeated until all the symbols have been added to the tree. Parentheses simply change the arrangement of the nodes and are not included in the parse tree.

For example, the expression $(((\neg p) \wedge q) \rightarrow (p \wedge (q \vee (\neg r))))$ has the following parse tree:

          \implies
           /   \
          /     \
      \wedge   \wedge
       /  \     /  \
    \neg   q   p  \vee
      |           /  \
      p          q  \neg
                      |
                      r

Every parse tree is unique. The rules for a valid formula can be applied to a parse tree to test for well-formedness:

* Tree nodes must be either atoms or connectives.
* If a node is an atom, it cannot have any children - if a node has children, it must be a connective.
* If a node is the unary connective ($\neg$), then it must have exactly one child.
* If the node is a binary connective, then it must have exactly two children.
* If a tree is empty, then it is not a well formed formula.

An algorithm for testing if an expression $U$ is a well formed formula is given below

1. If $U$ is empty, produce False.
2. If $U \in \operatorname{Atom}(\mathcal{L}_P)$, then produce True.
3. If the expression does not begin with a $($ symbol, produce False.
4. If the second symbol is $\neg$, then let $V$ be an expression such that $U = (\neg V)$. Apply this algorithm again with $V$ as the expression, and if it is false, produce False.
5. Otherwise (the second symbol is not $\neg$), apply this algorithm again with all the symbols starting from and including the second symbol, except doing nothing in step 7. If it results in False, return False. Otherwise, let $V$ be the well formed formula from the second symbol to wherever the algorithm run ended. If the next symbol after this is not a binary connective, produce False. Do the formula thing again to get a well formed formula $W$, or produce False.
6. If the symbol after the place $W$ ended is not a $)$ symbol, produce False.
7. If the expression does not end after the $)$ symbol, produce False.
8. Produce True.

This algorithm gets applied a finite number of times, proportional to the depth of the tree.

The **height** of a parse tree is the length of the longest path from the root to a leaf.

### Precedence

We now introduce a system of precedence. The logical connectives listed by priority, from highest to lowest, are $\neg$, $\wedge$, $\vee$, $\rightarrow$, and $\leftrightarrow$.

Precedence is the idea that some connectives get higher priority than others. For example, $p \vee q \wedge r$ could potentially mean either $((p \vee q) \wedge r)$, or $(p \vee (q \wedge r))$. But since $\wedge$ has a higher precedence than $\vee$, we define $p \vee q \wedge r$ to mean $(p \vee (q \wedge r))$.

When there are multiple ambiguities, we start with the ones with highest precedence. For example, $\neg p \wedge q \vee r$ is resolved to $(neg p) \wedge q \vee r$, then $(((neg p) \wedge q) \vee r)$.

With this in place, we can now eliminate many of the superfluous parentheses to clean up our formulas.

**Course of values induction** is a type of strong induction where the inductive hypothesis used to prove the inductive conclusion is the conjunction of all the previous cases.

In other words, we do course of values induction by proving a property holds for $M(1)$, and then by proving that $M(1) \wedge \ldots \wedge M(k) \implies M(k + 1)$ for any $k \ge 1$. By the principal of strong induction, $M(n)$ therefore holds for all $n \ge 1$.

Sometimes we want to use induction on the height of the parse tree. This is often useful for proving things about propositional formulas, due to the recursive nature of formulas.

If a formula $C$ contains the segment $(\neg A)$, where $A$ is another formula, then $A$ is the **scope** of the $\neg$ within $C$. The scope is the area where the $\neg$ applies.

For binary connectives, if $C$ contains $(A \odot B)$ where $\odot$ is a binary connective, then $A$ and $B$ are the left and right scopes, respectively, of the binary connective within $C$.

A scope is unique to the operator. Two operators cannot have the exact same scope, though the scopes themselves can be equal.

# 14/5/14

Semantics
---------

The semantics of a language describe how we interpret should valid formulas in that language. Semantics assigns meanings to formulas.

Let $A$ and $B$ be formulas, representing the propositions $\mathcal{A}$ and $\mathcal{B}$, respectively. Then $\neg A$ represents "Not $\mathcal{A}$", $A \wedge B$ represents "$\mathcal{A}$ and $\mathcal{B}$".

Semantics is formally given as functions that map each formula to a value in $\set{0, 1}$ (false and true). This function can be represented using a **truth table**.

The value of a formula $A$ given a truth valuation $t$ is represented $A^t$, with $A^t \in \set{0, 1}$. If we used conventional function notation, we might write $t(A) \in \set{0, 1}$.

**Truth valuations** are functions that are used to assign truth values to propositional variables. The domain is the set of propositional variables in a formula, and the range is $\set{0, 1}$. If $t$ is a truth valuation, then $A^t \in \set{0, 1}$, where $A$ is a propositional variable.

We can completely represent the truth valuation for a formula by listing out all possible values of the domain and the resulting value in the range. For $n$ variables, there are $2^n$ possible values.

We can define the truth valuation function $t$ in terms of formulas $A$ and $B$ recursively:

1. $A \in \operatorname{Atom}(\mathcal{L}_P) \implies A^t \in \set{0, 1}$.
2. $(\neg A) = \begin{cases} 1 &\text{if } A^t = 0 \\ 0 &\text{otherwise} \end{cases}$.
3. $(A \wedge B) = \begin{cases} 1 &\text{if } A^t = B^t = 1 \\ 0 &\text{otherwise} \end{cases}$.
4. $(A \vee B) = \begin{cases} 1 &\text{if } A^t = 1 \text{ or } B^t = 1 \\ 0 &\text{otherwise} \end{cases}$.
5. $(A \rightarrow B) = \begin{cases} 1 &\text{if } A^t = 0 \text{ or } B^t = 1 \\ 0 &\text{otherwise} \end{cases}$.
6. $(A \leftrightarrow B) = \begin{cases} 1 &\text{if } A^t = B^t \\ 0 &\text{otherwise} \end{cases}$.

Given a set of formulas $\Sigma$, $\Sigma^t = \begin{cases} 1 &\text{if } \forall A \in \Sigma, A^t = 1 \\ 0 &\text{otherwise} \end{cases}$. In other words, a set of formulas is true in a given truth valuation if and only if all of the formulas in the set are true.

$\Sigma$ is **satisfiable** if and only if there exists a truth valuation $t$ such that $\Sigma^t = 1$. It this $t$ exists, then $t$ **satisfies** $\Sigma$. Therefore, $\emptyset$ satisfiable by any truth valuation.

A truth valuation can also be called a **model**.

A **tautology** is a formula which is always true, so for any truth valuation $t$, $A^t = 1$.

A **contradiction** is a formula which is always false, so for any truth valuation $t$, $A^t = 0$.

A **semantically consistent** formula is one that is not a contradiction, so there exists a truth valuation $t$ such that $A^t = 1$.

Satisfiability is a property of a set of formulas where there exists a truth valuation that makes every formula in the set true.

The negation of a tautology is always a contradiction, and only the negation of a tautology is a contradiction. Likewise, the negation of a contradiction is a tautology, and only the negation of a contradiction is a tautology.

Rather than writing out the entire truth table, we can simplify formulas using the following identities:

* $\neg 0 = 1, \neg 1 = 0$
* $A \wedge 1 = 1 \wedge A = A$
* $A \wedge 0 = 0 \wedge A = 0$
* $A \vee 1 = 1 \vee A = 1$
* $A \vee 0 = 0 \vee A = A$
* $A \rightarrow 1 = 1$
* $0 \rightarrow A = 1$
* $1 \rightarrow A = A$
* $A \rightarrow 0 = \neg A$

Show that $A = ((p \wedge q \rightarrow r) \wedge (p \rightarrow q)) \rightarrow (p \rightarrow r)$ is a tautology:

> Assume $p^t = 0$. Then $A^t = (1 \rightarrow (p \rightarrow r))^t = 1$.  
> Assume $p^t = 1$. Then $A^t = (((q \rightarrow r) \wedge q) \rightarrow r)^t$.  
> Assume $q^t = 0$. Then $A^t = 1$. Assume $q^t = 1$. Then $A^t = (r \rightarrow r)^t = 1$.  
> So $A$ is a tautology.  

Let $\mathcal{A}$ and $\mathcal{A}_1, \ldots, \mathcal{A}_n$ be propositions.

Deductive logic studies whether $\mathcal{A}$ can be deduced from $\mathcal{A}_1, \ldots, \mathcal{A}_n$.

Let $\Sigma \subseteq \formlp, A \in \formlp$.

$A$ is a **tautological consequence/semantic entailment** of $\Sigma$ (written $\Sigma \models A$) if and only if for all truth valuation $t$, $\Sigma^t = 1 \implies A^t = 1$. Also, $\neg (\Sigma \models A) = \Sigma \not\models A$. We can also write out the elements of $\Sigma$ directly, like $P_1, \ldots, P_n \models A$, which is the same as $\set{P_1, \ldots, P_n} \models A$.

In other words, if $\Sigma \models A$, then the truth of $\Sigma$ implies the truth of $A$.

As a result, if $\emptyset \models A$, then $A$ is a tautology. This is because $\emptyset^t = 1$ for any truth valuation $t$, so $A^t = 1$ as well.

As a result of our semantic definitions of our operations, conjunction and disjunction is commutative and associative. So $A \wedge B = B \wedge A, A \vee B = B \vee A, (A \wedge B) \wedge C = A \wedge (B \wedge C), (A \vee B) \vee C = A \vee (B \vee C)$.

Also, $A_1, \ldots, A_n \models A \iff \emptyset \models A_1 \wedge \ldots \wedge A_n \rightarrow A \iff \emptyset \models A_1 \implies (A_2 \implies (\ldots (A_n \implies A) \ldots))$.

# 21/5/14

Note that $\Sigma \implies A$ is not a formula, because it cannot be created using the formula rules. It is an operator that accepts formulas as both operands.

It is true that $\Sigma \not\models A$ if there exists a truth valuation $t$ such that $\Sigma^t = 1$ and $A^t = 0$.

If $A \models B$ and $B \models A$, then $A \equiv B$. Also, if $A \equiv B$ and $C \equiv D$, then $\neg A \equiv \neg B$ and $A \odot C \equiv B \odot D$ where $\odot$ is a binary connective.

Also, we define $\emptyset^t = 1$ for any truth valuation $t$.

If $B \equiv C$, then $A \equiv A'$ where $A'$ is $A$ with any number of occurrences of $B$ replaced by $C$. This is known as **replaceability**.

The **dual** of $A$, which is a formula that uses only the connectives $\neg, \wedge, \vee$ is $A'$, which is $A$ with every $\wedge$ replaced by $\vee$, every $\vee$ with $\wedge$, and every atom $A$ with $\neg A$. It is always true that $A = \neg A'$. This is basically application of De Morgan's laws to formulas.

We know that $A \rightarrow B \equiv \neg A \vee B$. Because of this, $\rightarrow$ is **definable** in terms of $\neg, \vee$, or **reducible to** $\neg, \vee$.

$\neg$ is a unary connective - it accepts one operand. $\vee$ is a binary connective - it accepts two operands. It is also possible to have ternary connectives and even connectives of arity 4 and above (arity is the number of operands).

Let $f$ be an $n$-ary connective - a connective accepting $n$ operands. Then $f A_1 \ldots A_n$ is a formula where $A_1, \ldots, A_n$ are connected by the connective $f$.

An $n$-ary connective has $2^n$ possible input cases to consider. For $k$ possible inputs, there are $2^k$ possible output cases. So there are $2^{2^n}$ possible $n$-ary connectives.

A set of connectives is **adequate** if and only if any $n$-ary connective can be defined in terms of these connectives.

For example, $\set{\wedge, \vee, \neg}$ is an adequate set because the truth table for any $n$-ary operator can be represented by a formula using just these connectives. This can be proven:

> Let $f$ be an $n$-ary connective. Clearly, there are $2^n$ possible truth valuations $t_1, \ldots, t_{2^n}$ for $A_1, \ldots, A_n$. Let $t$ be one of these truth valuations.  
> Let $T(t) = T_1(t) \wedge \ldots \wedge T_n$(t) where $T_i(t) = \begin{cases} A_i &\text{if } A_i^t = 1 \\ \neg A_i &\text{if } A_i^t = 0 \end{cases}$. For example, if $A_1^t = 1, A_2^t = 0, A_3^t = 1$, then $T = A_1 (\neg A_2) A_3$.  
> Clearly, $T(t_i)^{t_j}$ is true if and only if $t_i = t_j$. In other words, $T(t)$ results in a formula that is true in only the truth valuation $t$ and no others.
> Let $V = T(t_1) \vee \ldots \vee T(t_{2^n})$, where there is a $T(i)$ term if and only if $(f A_1, \ldots, A_n)^t = 1$.  
> Clearly, this is a formula that is true when $f A_1, \ldots, A_n$ is true, and false otherwise. Therefore, $f A_1, \ldots, A_n = V$.  
> So any $f$ can be defined in terms of $\set{\wedge, \vee, \neg}$, which makes it adequate.  

We usually prove sets are adequate by defining the connectives in a set that is already known to be adequate in terms of the connectives of this set. If a connective $f$ is definable in terms of a set of connectives $A$, and every connective in that set is definable in terms of another set $B$, then $f$ is definable in terms of $B$.

This is because we could define $f$ first in terms of $A$, and then define that in terms of $B$.

For example, $\set{\wedge, \neg}$ is adequate because $\wedge$ and $\neg$ are already in the set, and $A \vee B \equiv \neg((\neg A) \wedge (\neg B))$, so $\vee$ is definable in terms of $\set{\wedge, \neg}$.

# 26/5/14

The Schroder connective is defined as $A \downarrow B$ where $A \downarrow B \equiv \neg (A \wedge B)$ - the NOR operation.

$\set{\neg, \rightarrow}$ is an adequate set.

Proof Calculus
--------------

We want a calculus for reasoning about propositional logic. This will allow us to simplify proofs of validity.

In this course we will be looking at the Hilbert and natural deduction systems for formal proof calculus.

Let $\Sigma = \set{\alpha_1, \ldots, \alpha_n}$. This is a set of premises.

$\Sigma \models d$ is the same as $\forall t, \Sigma^t = 1 \implies d^t = 1$.

$\Sigma \vdash \alpha$ means that $\alpha$ is provable or deducible from $\Sigma$.

Hilbert System
--------------

The **Hilbert System** is a deductive system over the set of propositional logic formulas.

$\Sigma \vdash_H \alpha$ means that $\alpha$ is provable or deducible in the Hilbert system. $\vdash_H A$ means $\emptyset \vdash_H A$.

$\Sigma$ is a set of formulas known as premises, and from them we can deduce $\delta$ using axioms and inference rules.

Axioms are basically just tautologies. In this course we will only be using axioms over $\set{\neg, \rightarrow}$ since this set is small and results in fewer axioms:

1. $\phi \rightarrow (\psi \rightarrow \phi)$ - basically, this is just a special case of 
2. $(\phi \rightarrow (\psi \rightarrow \xi)) \rightarrow ((\phi \rightarrow \psi) \rightarrow (\phi \rightarrow \xi))$ - basically, $\rightarrow$ is distributive
3. $(\neg \phi \rightarrow \neg \psi) \rightarrow (\psi \rightarrow \phi)$ - basically, the contrapositive of a formula implies the formula

Here, $\phi, \psi, \xi$ are any formulas.

**Inference rules** are manipulations we can perform on formulas to obtain new formulas. In this system we have only Modus Ponens (MP): $\set{\phi, \phi \rightarrow \psi} \rightarrow_H \psi$.

A **proof** is a set of formulas that includes axioms, premises, and conclusions. We start with the axioms that we know are true, so we use inference rules over the axioms and previously obtained formulas, listing out each new formula obtained, until we get the desired result.

For example, prove that $\vdash_H A \rightarrow A$:

1. $A \rightarrow ((A \rightarrow A) \rightarrow A)$ (Axiom 1 where $\psi = A \rightarrow A$)
2. $(A \rightarrow ((A \rightarrow A) \rightarrow A)) \rightarrow ((A \rightarrow (A \rightarrow A)) \rightarrow (A \rightarrow A))$ (Axiom 2 where $\psi = A \rightarrow A$)
3. $(A \rightarrow (A \rightarrow A)) \rightarrow (A \rightarrow A)$ (Modus Ponens on 1 and 2)
4. $A \rightarrow A$ (Modus Ponens on 1 and 3)

Prove that $\set{A \rightarrow B, B \rightarrow C} \vdash_H A \rightarrow C$:

1. $A \rightarrow B$ (Premise 1)
2. $B \rightarrow C$ (Premise 2)
3. $((B \rightarrow C) \rightarrow (A \rightarrow (B \rightarrow C)))$ (Axiom 1 where $\phi = B \rightarrow C$)
4. $A \rightarrow (B \rightarrow C)$ (Modus Ponens on 1 and 3)
5. $(A \rightarrow (B \rightarrow C)) \rightarrow ((A \rightarrow B) \rightarrow (A \rightarrow C))$ (Axiom 2)
6. $(A \rightarrow B) \rightarrow (A \rightarrow C)$ (Modus Ponens on 4 and 5)
7. $A \rightarrow C$ (Modus Ponens on 1 and 6)

The **deduction theorem** says that $\Sigma \vdash_H A \rightarrow B$ if and only if $\Sigma \cup A \vdash_H B$. So $\set{\alpha_1, \ldots, \alpha_n} \vdash_H \alpha$ is the same as $\emptyset \vdash_H (\alpha_1 \rightarrow (\ldots (alpha_n \rightarrow \alpha) \ldots))$.

In other words, to prove an implication, we just have to assume the antecedent and use it to prove the consequent.

# 28/5/14

Prove that $\neg \neg A \vdash_H A$:

1. $\neg \neg A$ (Premise 1)
2. $\neg \neg A \rightarrow (\neg \neg \neg \neg A \rightarrow \neg \neg A)$ (Axiom 1 where $\psi = \neg \neg \neg \neg A$)
3. $\neg \neg \neg \neg A \rightarrow \neg \neg A$ (Modus Ponens on 1 and 2)
4. $(\neg (\neg \neg \neg A) \rightarrow \neg A) \rightarrow (\neg A \rightarrow \phi)$ (Axiom 3 where $\phi = \neg \neg \neg A$)
5. $\neg A \rightarrow \neg \neg \neg A$ (Modus Ponens on 3 and 4)
6. $(\neg A \rightarrow \neg \neg \neg A) \rightarrow (\neg \neg A \rightarrow A)$ (Axiom 3 where $\psi = \neg \neg A$)
7. $\neg \neg A \rightarrow A$ (Modus Ponens on 5 and 6)
8. $A$ (Modus Ponens on 1 and 7)

$\Sigma \models \alpha$ is a semantic construct. This is associated with soundness.

$\Sigma \vdash \alpha$ is a syntactical construct. This is associated with completeness.

Formulas must be semantically entailed before it can be syntactically entailed. We need it to be semantically sound before proving it is complete.

Here are some rules of thumb to more easily construct proofs.

* Axiom 3 is often helpful in adding or remove negations, by putting the formula with the negation on one of the sides and the formula without on the other.
* Axiom 1 is useful for getting the converse and setting up modus ponens to simplify.
* Axiom 2 is just... just use it as a last resort.
* General pattern is premise/axiom, modus ponens, premise/axiom, etc.
* Remember the proofs for some more sane manipulation rules like $\neg \neg A = A$ and use those instead of these crazy axioms.

Natural Deduction
-----------------

In natural deduction, we infer a **conclusion** from the **premises** using various **proof rules**.

Natural deduction was born out of people noticing that the Hilbert system is terribly complicated even for the simplest of proofs.

Let $\Sigma = \set{\alpha_1, \ldots, \alpha_n}$ be the set of all the premises.

For convenience, we define that $\alpha_1, \ldots, \alpha_n$ is equivalent to $\set{\alpha_1, \ldots, \alpha_n}$.

We also define that $\Sigma, \alpha$ is equivalent to $\Sigma \cup \set{\alpha}$, and given a set $S$, then $\Sigma, S$ is equivalent to $\Sigma, S$.

$\Sigma \vdash \alpha$ is not a formula, but is a proposition.

> If the train arrives late and there are no taxis at the station, then John is late for his meeting.  
> John is not late for his meeting.  
> The train did arrive late.  
> Therefore, there were taxis at the station.  

The above can be written as follows:

> Let $l$ represent the train being late. Let $m$ represent being late to the meeting. Let $t$ represent there being taxis.  
> Then $l \wedge \neg t \rightarrow m$, $\neg m$, $l$, so $t$.  

The rules of deduction in ND are:

| Name                             | Symbol                                                                                         | Rule                                                                                                                   | Description                                                                            |
|:---------------------------------|:-----------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|
| Reflexivity                      | $\operatorname{Ref}$                                                                           | $A \vdash A$                                                                                                           | Anything can be deduced from itself.                                                   |
| Addition of Premises             | $+$ with $\Sigma_1 \vdash \alpha$                                                              | $\Sigma_1 \vdash \alpha$ implies $\Sigma_1 \cup \Sigma_2 \vdash \alpha$                                                | Premises can be freely added to valid arguments.                                       |
| Reflexive premises               | $\epsilon$                                                                                     | $\alpha \in \Sigma$ implies $\Sigma \vdash \alpha$                                                                     | Anything can be deduced if it is itself deducible.                                     |
| Negation-introduction            | $\neg+$ with $\Sigma, \alpha \vdash \psi$ and $\Sigma, \alpha \vdash \neg \psi$                | $\Sigma, \alpha \vdash \psi$ and $\Sigma, \alpha \vdash \neg \psi$ implies $\Sigma \vdash \neg \alpha$                 | If $\alpha$ results in a contradiction, then $\neg \alpha$ must be true.               |
| Negation-elimination             | $\neg-$ with $\Sigma, \neg \alpha \vdash \psi$ and $\Sigma, \neg \alpha \vdash \neg \psi$      | $\Sigma, \neg \alpha \vdash \psi$ and $\Sigma, \neg \alpha \vdash \neg \psi$ implies $\Sigma \vdash \alpha$            | If $\neg \alpha$ results in a contradiction, then $\alpha$ must be true.               |
| Conjunction-introduction         | $\wedge+$ with $\Sigma \vdash \alpha$ and $\Sigma \vdash \psi$                                 | $\Sigma \vdash \alpha$ and $\Sigma \vdash \psi$ implies $\Sigma \vdash \alpha \wedge \psi$                             | If something implies two things individually, then it implies both together.           |
| Conjunction-elimination          | $\wedge-$ with $\Sigma \vdash \alpha \wedge \psi$                                              | $\Sigma \vdash \alpha \wedge \psi$ implies $\Sigma \vdash \alpha$ and $\Sigma \vdash \psi$                             | If two things both follow from the premises, each one follows individually.            |
| Implication-introduction         | $\rightarrow+$ with $\Sigma, \alpha \vdash \psi$                                               | $\Sigma, \alpha \vdash \psi$ implies $\Sigma \vdash \alpha \rightarrow \psi$                                           | If $\psi$ can be deduced from $\alpha$, then $\alpha \rightarrow \psi$.                |
| Implication-elimination          | $\rightarrow-$ with $\Sigma \vdash \alpha \rightarrow \psi$ and $\Sigma \vdash \alpha$         | $\Sigma \vdash \alpha \rightarrow \psi$ and $\Sigma \vdash \alpha$ implies $\Sigma \vdash \psi$                        | If something implies something else and that thing is true, then so is the other.      |
| Disjunction-introduction         | $\vee+$ with $\Sigma \vdash \alpha$                                                            | $\Sigma \vdash \alpha$ implies $\Sigma \vdash \alpha \vee \psi$ and $\Sigma \vdash \psi \vee \alpha$                   | Something deducible implies that it or something else is deducible.                    |
| Disjunction-elimination          | $\vee-$ with $\Sigma, \alpha_1 \vdash \psi$ and $\Sigma, \alpha_2 \vdash \psi$                 | $\Sigma, \alpha_1 \vdash \psi$ and $\Sigma, \alpha_2 \vdash \psi$ implies $\Sigma, \alpha_1 \vee \alpha_2 \vdash \psi$ | If two things individually imply another, then either implies it.                      |
| Double-implication-introduction  | $\leftrightarrow+$ with $\Sigma, \alpha \vdash \psi$ and $\Sigma, \psi \vdash \alpha$          | $\Sigma, \alpha \vdash \psi$ and $\Sigma, \psi \vdash \alpha$ implies $\Sigma \vdash \alpha \leftrightarrow \psi$      | If one thing implies another and the other implies it, they are equivalent.            |
| Double-implication-elimination 1 | $\leftrightarrow-$ with $\Sigma \vdash \alpha \leftrightarrow \alpha$ and $\Sigma \vdash \psi$ | $\Sigma \vdash \alpha \leftrightarrow \psi$ and $\Sigma \vdash \alpha$ implies $\Sigma \vdash \psi$                    | If two things are equivalent and the first is deducible, so is the second.             |
| Double-implication-elimination 2 | $\leftrightarrow-$ with $\Sigma \vdash \alpha \leftrightarrow \psi$ and $\Sigma \vdash \alpha$ | $\Sigma \vdash \alpha \leftrightarrow \psi$ and $\Sigma \vdash \psi$ implies $\Sigma \vdash \alpha$                    | If two things are equivalent and the second is deducible, so is the first.             |
| Transitivity of deducibility     | $\operatorname{Tr}$ with $\Sigma_1 \vdash \Sigma_2$ and $\Sigma_2 \vdash \Sigma_3$             | $\Sigma_1 \vdash \Sigma_2$ and $\Sigma_2 \vdash \Sigma_3$ implies $\Sigma_1 \vdash \Sigma_3$                           | If one thing implies a second, which implies a third, then the first implies the last. |

Prove the $\epsilon$ rule from the basic rules $\operatorname{Ref}, +$:

> Let $A = \Sigma \setminus \set{\alpha}$.  
> Clearly, $\alpha \implies \alpha$ by $\operatorname{Ref}$ (step 1).  
> So $\set{\alpha} \cup A \vdash \alpha$ by $+$ with step 1 (step 2).  
> Since $\set{\alpha} \cup A = \Sigma$, $\Sigma \vdash \alpha$ (step 3, conclusion).  

Prove that $\neg \neg p \vdash p$:

1. $\neg \neg p, \neg p \vdash \neg \neg p$ ($\epsilon$).
2. $\neg \neg p, \neg p \vdash \neg p$ ($\epsilon$).
3. $\neg \neg p \vdash p$ ($\neg-$).

Note that implication-introduction and implication-elimination is very similar to the deduction theorem.

Prove that $A \rightarrow B, B \rightarrow C \vdash A \rightarrow C$:

1. $A \rightarrow B, B \rightarrow C, A \vdash A \rightarrow B$ ($\epsilon$)
2. $A \rightarrow B, B \rightarrow C, A \vdash A$ ($\epsilon$)
3. $A \rightarrow B, B \rightarrow C, A \vdash B$ ($\rightarrow-$ with step 1 and 2)
4. $A \rightarrow B, B \rightarrow C, A \vdash B \rightarrow C$ ($\epsilon$)
5. $A \rightarrow B, B \rightarrow C, A \vdash C$ ($\rightarrow-$ with step 3 and 4)
6. $A \rightarrow B, B \rightarrow C \vdash A \rightarrow C$ ($\rightarrow+$ with step 5)

# 2/6/14

If a system is sound and complete, then arguments can be proved if and only if they are valid.

We can write $A, B \vdash C$ as $\frac{A \qquad B}{C}$. This is known as **inference notation**.

### Formal Proofs

A **formal proof** for $\Sigma_k \vdash \alpha_k$ is simply a sequence of the natural deduction rules - $\Sigma_1 \vdash \alpha_1, \ldots, \Sigma_k \vdash \alpha_k$. If this is the case, then $\alpha_n$ is **formally deducible** from $\Sigma_n$ - it can be proven from those premises.

Note that $\Sigma \vdash \alpha$ is very different from $\Sigma \models \alpha$ - the former deals purely with syntax, while the latter deals with semantics - what the symbols actually mean.

Negation-introduction ($\neg+$) is also called **contradiction** or **reductio ad absurdum**. It is usually the rule we use when we want to prove by contradiction, and is very similar to negation-elimination ($\neg-$).

Prove that $p \vdash \neg \neg p$:

1. $p, \neg p \vdash p$ ($\epsilon$)
2. $p, \neg p \vdash \neg p$ ($\epsilon$)
3. $p \vdash \neg \neg p$ (reductio ad absurdum with step 1 and 2)

In formal proofs, we usually start off by stating premises (also called assumptions), and then adding rules and modifying results until we arrive at the conclusion of the proof. The conclusion of the proof is the thing we are trying to prove.

The proof format we have been using so far is in something known as **line/sequent/turnstile notation**. The $\vdash$ symbol is a sequent or turnstile, with all premises before it and all conclusions after.

In **box notation**, we write assumptions, and then we draw a box that contains it, the box acts like a "scope" in which the assumptions hold and we can directly write the deduced thing. Inside the box we can directly write the resulting formulas without writing the $\alpha_1, \ldots, \alpha_n \vdash$ before it. Scopes can nest, and this saves a lot of rewriting of premises.

Alternatively, we might indent the values instead of drawing a box around them. For example, the previous proof that $A \rightarrow B, B \rightarrow C \vdash A \rightarrow C$:

1. $A \rightarrow B, B \rightarrow C$ (assumption)
2. The following is indented or drawn in a box:
    1. $A \rightarrow B, B \rightarrow C, A$ (assumption)
    2. $A \rightarrow B$ ($\epsilon$)
    3. $A$ ($\epsilon$)
    4. $B$ ($\rightarrow-$ with step 2.2 and 2.3)
    5. $B \rightarrow C$ ($\epsilon$)
    6. $C$ ($\rightarrow-$ with step 2.4 and 2.5)
3. $A \rightarrow C$ ($\rightarrow+$ with step 2.6):

When we begin a box or indentation, we insert an assumption. When we close a box, we remove the assumption. This ability to insert and remove asumptions keeps our proofs clean and organized.

Boxes or indentations can nest, and have lexical scope - when we have a box inside a box, we can use any statements above it, even if it is in a parent box. Outside of a box, we cannot use any of the resulting formulas directly - they are always attached to their assumptions. For example, in the above example we could not state $C$ in step 3, but we could use $A \rightarrow B, B \rightarrow C, A \vdash C$ in the $\rightarrow+$ rule.

A box is often closed when we arrive at a conclusion using a rule such as $\neg-$, $\rightarrow+$, or $\vee-$. Usages of $\neg-$ and $\rightarrow+$ are usually found right after a box closes. $\vee-$ is usually found after two boxes.

All boxes must be closed by the end of the argument. Otherwise, there are extra premises and the proof is not yet complete.

Elimination rules turn two statements into one. Introduction rules introduce new statements, or turn one statement into two.

# 4/6/14

Soundness/Completeness
----------------------

**Soundness** is a property of a deductive system where it is impossible to prove any invalid arguments.

**Completeness** is a property of a deductive system where it is always possible to prove valid arguments.

The **soundness theorem** states that if something is provable, then it is valid - $\Sigma \vdash \alpha$ implies that $\Sigma \models \alpha$.

In practice, we prove soundness by using the contrapositive: by proving that $\Sigma \not\models \alpha$ implies $\Sigma \not\vdash \alpha$.

If $A \vdash B$ and $B \vdash C$, then $A \vdash C$.

In other words, if $\Sigma' = \set{A_1, \ldots, A_n}$ and $\Sigma \vdash A_1, \ldots, \Sigma \vdash A_n$ and $\Sigma' \vdash \alpha$, then $\Sigma \vdash \alpha$

Proof:

1. $A_1, \ldots, A_n \vdash \alpha$ (Premise)
2. $A_1, \ldots, A_{n - 1} \vdash A_n \rightarrow \alpha$ ($\rightarrow$+)
3. $\vdash A_1 \rightarrow (\ldots (A_n \rightarrow \alpha) \ldots)$
4. $\Sigma \vdash A_1 \rightarrow (\ldots (A_n \rightarrow \alpha) \ldots)$
5. $\Sigma \vdash A_1$ (premise)
6. $\Sigma \vdash A_2 \rightarrow (\ldots (A_n \rightarrow \alpha) \ldots)$
7. $\Sigma \vdash \alpha$

The proof rules of Natural Deduction are purely syntactic - they only shuffle symbols around without considering what they mean. Now we connect it with the actual semantics.

Soundness of natural deduction means that formal proofs demonstrate tautological consequences - $\Sigma \vdash \alpha$ implies that $\Sigma \models \alpha$. In other words, every proof should be a valid argument.

Completeness of natural deduction means that tautological consequences are provable using natural deduction - $\Sigma \models \alpha$ implies that $\Sigma \vdash \alpha$. In other words, every valid argument should have a proof.

Basically, anything provable by truth tables can also be proven using the ND rules.

Since ND is sound and complete, $\Sigma \models \alpha$ if and only if $\Sigma \vdash \alpha$.

Also, $\rightarrow$ does not have an associative property, so $(A \rightarrow B) \rightarrow C$ is not the same as $A \rightarrow (B \rightarrow C)$.

# 9/6/14

### Natural Deduction

Soundness and completeness is rather intuitive. For example, it is obvious that if $p$ and $q$ are true, then $p$ is also true, so $p \wedge q \models p$. In ND, this is represented using the $\wedge-$ rule, which states $p \wedge q \vdash p$.

When we want to prove that two formulas are tautologically equivalent, we can either use a truth table, or write a formal proof.

A **sound** formula is one where its proof, $\Sigma \vdash \alpha$, implies $\Sigma \models \alpha$.

We can prove the soundness theorem for ND using strong induction over the length of the proof:

> Let $n$ be the length of the proof in steps.  
> Clearly, if $n = 1$, then the proof must have the form $\Sigma, \alpha \vdash \alpha$ ($\epsilon$), since this is the only rule that can prove something in one step.  
> Clearly, $\Sigma, \phi \models \phi$ because anything is a tautological consequence of itself.  
> Assume for some $k \ge 1$ that all proofs of length $k$ or less are sound - the inductive hypothesis.  
> Then a proof of length $k + 1$ has the steps $\alpha_1, \ldots, \alpha_k, \phi$, with rules associated with each step.  
> Clearly, $\alpha_1, \ldots, \alpha_k$ is sound, by the inductive hypothesis.  
> We want to prove that $\alpha_1, \ldots, \alpha_k, \phi$ is also sound.  
> Clearly, $\phi$ is the result of applying the ND rules. So each rule is a possible case and we need to check each rule to verify that the proof of length $k + 1$ is sound.  
> Assume the last rule is $\wedge+$. Then $\phi$ is of the form $\alpha_i \wedge \alpha_j$ where $i < k, j < k$.  
> Then there must be sound proofs of $\alpha_i$ and $\alpha_j$, by the inductive hypothesis.  
> So $\Sigma_i \models \alpha_i$ and $\Sigma_j \models \alpha_j$.  
> By a truth table, $\Sigma \models \alpha_i \wedge \alpha_j$, so $\phi$ is sound.  
> We can use this technique to verify this for every ND rule.  
> Therefore, $\Sigma \vdash \phi$ is always sound - it always implies $\Sigma \models \phi$.  

**Syntactical consistency** is a property of sets of formulas $\Sigma \subseteq \formlp$ where there does not exist $\alpha \in \Sigma$ such that $\Sigma \vdash \alpha$ and $\Sigma \vdash \neg \alpha$ at the same time - that it not possible to derive a contradiction from the premises.

This is not to be confused with semantic consistency, which applies only to formulas.

A set of formulas $\Sigma \subseteq \formlp$ is **maximally consistent** if and only if $\Sigma$ is consistent and any $\alpha \in \formlp \setminus \Sigma$, $\Sigma \cup \set{\alpha}$ is inconsistent. In other words, it is the largest possible set of consistent formulas.

There are multiple possible maximally consistent sets.

If $\Sigma$ is maximally consistent:

* $p \in \Sigma$ if and only if $\Sigma \vdash \alpha$.
* $\alpha \wedge \phi \in \Sigma$ if and only if $\Sigma \vdash \alpha$ and $\Sigma \vdash \phi$.
* $\alpha \vee \phi \in \Sigma$ if and only if $\Sigma \vdash \alpha$ or $\Sigma \vdash \phi$.
* $\alpha \rightarrow \phi \in \Sigma$ if and only if $\Sigma \vdash \alpha \implies \Sigma \vdash \phi$.
* $\alpha \leftrightarrow \phi \in \Sigma$ if and only if $\Sigma \vdash \alpha \iff \Sigma \vdash \phi$.

We can prove the completeness theorem for ND by proving the contrapositive:

> We want to prove $\Sigma \not\vdash \phi \implies \Sigma \not\models \phi$.  
> Assume $\Sigma \not\vdash \phi$.  
> Then $\Sigma \cup \set{\neg \alpha}$ is syntactically consistent, because there is no way that $\Sigma \vdash \alpha$ or $\Sigma \vdash \neg \phi$ for any $\phi \in \Sigma$.  
> So there exists a truth valuation $t$ such that $\Sigma^t = 1 \wedge (\neg \phi)^t = 1$.  
> So $\Sigma \not\models \phi$.  

The **Lindenbaum Lemma** says that any consistent set of formulas can be extended into a maximally consistent set.

# 11/6/14

Soundness theorem: $\Sigma \vdash \alpha \implies \Sigma \models \alpha$.

Completeness theorem: $\Sigma \models \alpha \implies \Sigma \vdash \alpha$, and its contrapositive $\Sigma \not\vdash \alpha \implies \Sigma \not\models \alpha$.

Soundness: $\Sigma$ is satisfiable implies $\Sigma$ is sound.

Completeness: $\Sigma$ is sound implies $\Sigma$ is satisfiable.

Logical connectives:

* $\neg p$ - not $p$, $p$ does not hold, it is not the case that $p$, $p$ is false
* $p \wedge q$ - $p$ and $q$, $p$ but $q$, not only $p$ but $q$, $p$ while $q$, $p$ despite $q$, $p$ yet $q$, $p$ although $q$
* $p \vee q$ - $p$ or $q$, $p$ or $q$ or both, $p$ and/or q, $p$ unless $q$
* $p \rightarrow q$ - if $p$ then $q$, $q$ if $p$, $p$ only if $q$, $q$ when $p$, $p$ is sufficient for $q$, $q$ is necessary for $p$, $p$ implies $q$
* $p \leftrightarrow q$ - $p$ if and only if $q$ ($p$ iff $q$), $p$ is necessary and sufficient for $q$, $p$ exactly if $q$, $p$ is equivalent to $q$

Tips and tricks for proofs:

* Natural deduction:
    * If you want to prove an implication $A \rightarrow B$, then assume $A$ and show B, and use $\rightarrow+$.
    * If you have something to prove with a different connective, then look at the corresponding introduction rule for that connective and try to show those premises.
    * As a rule of thumb, when you have something you want to show and can't see how to get it, use proof by contradiction: assume the negation of what you want to show, derive a contradiction, and then use $\neg-$ to remove the negation on what you wanted to show.
    * The turnstile and box notations both have their own advantages.
* Hilbert deduction:
    * Think about the problem in high-level steps. For example, if you have proven $\neg \neg p$ then a high-level step would be to conclude that $p$, and figure out how to express this high level step in lower level terms later.
    * The deduction theorem is one of the most powerful high-level steps for rewriting something you want to show in an easier form.

Prove $\neg A \vee \neg B \vdash \neg (A \wedge B)$:

1. $\neg A, \neg \neg (A \wedge B), \neg (A \wedge B) \vdash \neg \neg (A \wedge B)$ ($\epsilon$)
2. $\neg A, \neg \neg (A \wedge B), \neg (A \wedge B) \vdash \neg (A \wedge B)$ ($\epsilon$)
3. $\neg A, \neg \neg (A \wedge B) \vdash A \wedge B$ ($\neg-$ with 1, 2)
4. $\neg A, \neg \neg (A \wedge B) \vdash A$ ($\wedge-$ with 3)
5. $\neg A, \neg \neg (A \wedge B) \vdash \neg A$ ($\epsilon$)
6. $\neg A \vdash \neg (A \wedge B)$ ($\neg-$ 4, 5)
7. $\neg B, \neg \neg (A \wedge B), \neg (A \wedge B) \vdash \neg \neg (A \wedge B)$ ($\epsilon$)
8. $\neg B, \neg \neg (A \wedge B), \neg (A \wedge B) \vdash \neg (A \wedge B)$ ($\epsilon$)
9. $\neg B, \neg \neg (A \wedge B) \vdash A \wedge B$ ($\neg-$ with 7, 8)
10. $\neg B, \neg \neg (A \wedge B) \vdash B$ ($\wedge-$ with 9)
11. $\neg B, \neg \neg (A \wedge B) \vdash \neg B$ ($\epsilon$)
12. $\neg B \vdash \neg (A \wedge B)$ ($\neg-$ with 10, 11)
13. $\neg A \vee \neg B \vdash \neg (A \wedge B)$ ($\vee+$ with 6, 12)

Prove $\neg (A \wedge B) \vdash \neg A \vee \neg B$:

1. $\neg (A \wedge B), \neg \neg A, \neg A \vdash \neg \neg A$ ($\epsilon$)
2. $\neg (A \wedge B), \neg \neg A, \neg A \vdash \neg A$ ($\epsilon$)
3. $\neg (A \wedge B), \neg \neg A \vdash A$ ($\neg-$ with 1, 2)
4. $\neg (A \wedge B), \neg \neg B, \neg B \vdash \neg \neg B$ ($\epsilon$)
5. $\neg (A \wedge B), \neg \neg B, \neg B \vdash \neg B$ ($\epsilon$)
6. $\neg (A \wedge B), \neg \neg B \vdash B$ ($\neg-$ with 4, 5)
;wip

Prove $A \rightarrow B \vdash \neg B \rightarrow \neg A$:

1. $A \rightarrow B, \neg B, \neg \neg A, \neg A \vdash \neg \neg A$ ($\epsilon$)
2. $A \rightarrow B, \neg B, \neg \neg A, \neg A \vdash \neg A$ ($\epsilon$)
3. $A \rightarrow B, \neg B, \neg \neg A \vdash A$ ($\neg-$ with 1, 2)
4. $A \rightarrow B, \neg B, \neg \neg A \vdash A \rightarrow B$ ($\epsilon$)
5. $A \rightarrow B, \neg B, \neg \neg A \vdash B$ ($\rightarrow-$ with 3, 4)
6. $A \rightarrow B, \neg B, \neg \neg A \vdash \neg B$ ($\epsilon$)
7. $A \rightarrow B, \neg B \vdash \neg A$ ($\neg-$)
8. $A \rightarrow B \vdash \neg B \rightarrow \neg A$ ($\rightarrow+$ with 7)

# 16/6/14

Predicate Logic
---------------

Predicate logic is also known as **first order logic**.

Predicate logic is like propositional logic, but extended with quantifiers, variables, predicates, functions, constants, and more.

An ordered pair is an object that contains two objects $a$ and $b$, represented by $\tup{a, b}$, which, unlike sets, preserves the order of $a$ and $b$ (which one goes first). Two ordered pairs $\tup{a, b}$ and $\tup{c, d}$ are equal if and only if $a = c$ and $b = d$.

This can be extended into the **tuple** object - an $n$-tuple is an object that stores objects $a_1, \ldots, a_n$, preserving order, and is represented $\tup{a_1, \ldots, a_n}$. Again, $\tup{a_1, \ldots, a_n} = \tup{b_1, \ldots, b_n}$ if and only if $a_i = b_i, 1 \le i \le n$.

### Relations

An $n$-ary relation of $S$ is $R = \set{\tup{x_1, \ldots, x_n} \middle| x_1, \ldots, x_n \in S, f(x_1, \ldots, x_n)}$ for some boolean function $f(x_1, \ldots, x_n)$.

For example, $\le$ is a binary relation over $\mb{N}$ where $R = \set{\tup{a, b} \middle| a, b \in \mb{N}, a \le n}$. It is the set of all pairs of values where the first value is less than or equal to the second.

A ternary relation over $\mb{N}$ might be $R = \set{\tup{x, y, z} \middle| x + y < z, x, y, z \in \mb{N}}$.

It is always true that any $n$-ary relation $R$ over $S$ is a subset of $S^n$.

Equality is a special binary relation $R = \set{\tup{x, y} \middle| x, y \in S, x = y} = \set{\tup{x, x} \middel| x \in S}$.

The Boolean value $\tup{x, y} \in R$ for a binary relation can also be written $x R y$.

$R$ is **reflexive** over $S$ if and only if $\forall x \in S, x R x$.

$R$ is **symmetric** over $S$ if and only if $\forall x, y \in S, x R y \implies y R x$.

$R$ is **transitive** over $S$ if and only if $\forall x, y, z \in S, x R y \wedge y R z \implies x R z$.

If $R$ is all three, then $R$ is an equivalence relation. Examples of equivalence relations include $=$ and $\equiv \pmod{36}$.

The $R$-equivalence class of $x$ is $\overline x = \set{y \in S \middle| x R y}$. It is the set of all elements of $S$ that satisfy $x R y$ and is therefore a subset of $S$.

### Functions

A **function/mapping** $f$ is a set of ordered pairs where $\tup{x, y} \in f \wedge \tup{x, z} \implies y = z$. In other words, it is a set of tuples with no duplicates.

The **domain** of $f$ is $\operatorname{dom}(f) = \set{x \middle| \tup{x, y} \in f}$.

The **range** of $f$ is $\operatorname{ran}(f) = \set{y \middle| \tup{x, y} \in f}$.

If $\exists y, \tup{x, y} \in f$, then $f(x) = y$. If $\operatorname{dom}(f) = S$ and $\operatorname{ran}(f) \subseteq T$ for some sets $S$ and $T$, then $f: S \to T$ ($f$ is a function from $S$ to $T$). Similarly, we can define $n$-ary functions like $g: S_1, S_2, S_3 \to T$.

If $R$ is an $n$-ary relation and $S' \subseteq S$, then the **restriction** of $R$ to $S'$ is $R \cap S'^n$. Basically, it is a restriction of the possible input and output values that are allowed. If $R$ is a function, this is written $R \mid S': S' \to T$ - the restriction restricts the allowed x-values.

$f$ is **onto/surjective** if $\operatorname{ran}(f) = T$ - if the image is the same as the codomain. In other words, the output of the function is all the possible output values in $T$, or every possible output value has a corresponding input value.

Basically, every unique $y \in T$ has a (not necessarily unique) $x \in S$ such that $f(x) = y$.

$f$ is **one-to-one/injective** if $f(x) = f(y) \implies x = y$ - if every output value has a unique input value. In other words, every possible input value has a unique output value.

Basically, if $x_1, x_2 \in S$ and $x_1 \ne x_2$, then $f(x_1) \ne f(x_2)$.

If $f$ is both, then $f$ is a bijection and is **bijective** - every output value has one and only one input value, and every input value has one and only one output value. A bijection maps every element of $S$ to a unique element of $T$ and vice versa.

# 18/6/14

Two sets $S, T$ are **equipotent** if and only if there exists a bijection from $S$ to $T$. This is denoted $S \sim T$.

If and only if $S \sim T$, then $\abs{S} = \abs{T}$. A set $S$ is **countably infinite** if $\abs{S} = \abs{\mb{N}}$ - if we can construct a bijection between it and the set of all natural numbers. $S$ is **countable** if it is finite, or countably infinite - if $\abs{S} \le \abs{\mb{N}}$.

We can prove a set is countably infinite by showing that there is a bijection between it and $\mb{N}$, and we prove a set is countable by either proving it is finite or showing the bijection exists.

We can prove a set $S$ is not countable by proving that the bijection cannot exist.

Prove that the set of integers is countable:

> Clearly, there are more integers than natural numbers. However, they are both infinite, and infinite sets work in funny ways.  
> Even though there is a bijection between a subset of $\mb{Z}$ and $\mb{N}$, it is actually still possible to define a bijective $f: \mb{Z} \to \mb{N}$.  
> Let $f(z) = \begin{cases} 1 - 2z &\text{if } x \le 0 \\ 2z &\text{if }x > 0 \end{cases}$.  
> This is a bijection because we can find the inverse, $f^{-1}(n) = \begin{cases} \frac{1 - n}{2} &\text{if } 2 \mid n \\  \frac n 2 &\text{if } 2 \nmid n \end{cases}$.  
> So the set of integers is countable.  

A countable set is one where we can enumerate or count the elements - we can assign a natural number to each element in the set.

Any subset of a countable set is countable. The union of countably many countable sets is also countable:

> Let $S_1 \cup \ldots \cup S_n \cup \ldots$ be a union of countably many countable sets.  
> Let $S_i = \set{a_{i, 1}, \ldots, a_{i, m}, \ldots}$.  
> Then $S_1 \cup \ldots \cup S_n \cup \ldots = \set{a_{1, 1}, a_{1, 2}, a_{2, 1}, a_{1, 3}, a_{2, 2}, a_{3, 1}, a_{1, 4}, a_{2, 3}, a_{3, 2}, a_{4, 1}, \ldots}$ - ordered increasing by $n + m$ and then by $n$.  
> Let $f: S_1 \cup \ldots \cup S_n \cup \ldots \to \mb{N}$ be defined as $f(a_{n, m}) = \frac 1 2 (n + m - 1)(n + m - 2) + n = \frac 1 2 (n + m - 1)(n + m - 2) + n = n^2 + m^2 + 2nm - 3n - 3m + 2$.  
> This is called the diagonalization argument. It can also be used to prove that rational numbers are countable (by making $n, m$ the numerator and denominator) and similar things.  

The Cartesian product of a finite amount of countable sets is also a countable set. Also, the set of finite sets all containing only elements in a particular countable set is countable.

Predicate/first order logic is an extension of propositional logic that adds constructs to help deal with individuals/objects in a set. While propositional logic makes us list out all the objects we are using, predicate logic gives additional tools to deal things like some objects, all objects, or more.

For example, "Every student is younger than some instructor" could be represented in propositional logic by writing $(s_1 < i_1 \vee \ldots \vee s_1 < i_n) \wedge \ldots \wedge (s_m < i_1 \vee \ldots \vee s_m < i_n)$ where $s_1, \ldots, s_m$ are the student ages and $i_1, \ldots, i_n$ are the instructor ages.

Predicate logic makes it much easier to express this: $\forall s \in S, \exists i \in I, s < i$, where $S$ is the set of all students and $I$ is the set of all instructors.

### Syntax

The language of predicate logic is called $\mathfrak{L}$.

Formulas in predicate logic is made up of several components:

* Domain of objects - sets containing any number of objects.
* Variables (VS) - particular objects, but not necessarily a specific one.
    * Lowercase to distinguish them from predicates.
* Designated objects/constants (CS) - specific objects such as a particular number or a matrix.
* Functions such as $+$, $-$, $f$, and $g$.
* Relations such as $=$ and $>$.
* Quantifiers such as $\forall$ and $\exists$.
    * They have the form $\forall x. P(\ldots)$ or $\exists x. P(\ldots)$.
* Propositional connectives.

# 23/6/14

Write "Every child is younger than its mother" using predicate logic:

> Let $C(x)$ represent $x$ being a child. Let $M(x)$ represent the mother of $x$. Let $Y(x, y)$ represent $x$ being younger than $y$.  
> Then $\forall x. C(x) \rightarrow Y(x, M(x))$.  

$\forall x. \forall y. \operatorname{plus}(x, y) = \operatorname{plus}(y, x)$ - addition is commutative.

$\forall x. \neg \operatorname{equal}(0, \operatorname{succ}(x))$ - 0 is not the successor of any non-negative integer.

Well formed formulas in $\mathfrak{L}$ can be defined as follows:

* $\operatorname{Term}(\mathfrak{L})$ is the set of all constants and variables (terms), and functions over terms.
    * For example, $f(g(a, b), c) \in \operatorname{Term}(\mathfrak{L})$.
    * We basically need to pass the right number of parameters to every function.
* $\operatorname{Atom}(\mathfrak{L})$ is the set of all predicates over terms - the set of all $P(t_1, \ldots, t_n), t_1, \ldots, t_n \in \operatorname{Term}(\mathfrak{L})$ where $P$ is an $n$-ary predicate.
    * For example, $\operatorname{equal}(\operatorname{plus}(x, 2), \operatorname(\operatorname{plus}(x, 1), 1))$.
* $\operatorname{Form}(\mathfrak{L}) = \operatorname{Atom}(\mathfrak{L}) \cup \set{(\neg \phi), (\phi \wedge \psi), (\phi \vee \psi), (\phi \rightarrow \psi), (\phi \leftrightarrow \psi)} \cup \set{\forall x. \phi, \exists x, \phi}$ where $\phi, \psi \in \operatorname{Form}(\mathfrak{L})$ - the set of all atoms combined with the set of operations over other formulas.
    * For example, $\neg \operatorname{equal}(1, 2) \in \operatorname{Form}(\mathfrak{L})$.
    * For example, $\forall x, \operatorname{equal}(x, x) \in \operatorname{Form}(\mathfrak{L})$.

We can also draw parse trees for formulas. Here, the leaves are terms and the other nodes are either connectives or predicates or quantifiers.