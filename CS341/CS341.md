CS341
=====

Algorithms.

    Doug Stinson
    Section 001
    https://www.student.cs.uwaterloo.ca/~cs341/

# 14/9/15

Most materials will be on LEARN. Questions and answers can be posted on Piazza.

Study of the design and the analsis of algorithms - in particular, the correctness (proved via formal proofs) and efficiency (proved using time complexity analysis).

Useful metrics for algorithms are asymptotic complexity (best case/average case/worst case time/space complexity), number of specific computations used (like comparisons in sorting algorithms), whether an algorithm is the most efficient for a particular problem, etc.

Interesting things to know are lower bounds on possible algorithms to solve a particular problems, problems that cannot be solved by any algorithm (undecidability), and problems that cannot be solved efficiently by any algorithm, but can be solved (NP-hardness).

Useful design strategies we will go over are divide and conquer, greedy algorithms, dynamic programming, breadth-first and depth-first search, local search, and linear programming.

An example of an algorithm design is the maximum problem: given an array of integers $A$, find the maximum integer in $A$. A simple solution to this is as follows:

    def find_max(array):
        current = array[0]
        for elem in array[1:]:
            if elem > current: current = elem
        return elem

This seems to be obviously correct, but we can also use program verification to prove it correct. In this case, we will use induction to prove the loop invariant - at the end of each iteration of the loop, `current` is equal to the largest element encountered so far in the array. We'd check the base case for arrays of length 1, then prove the inductive hypothesis to formally verify the program.


Clearly, this algorithm is $\Theta(n)$, since it loops over the entire array once. Formal time complexity analysis can also be done by noting that the loop body takes $\Theta(1)$ time, and the loop runs $\Theta(n)$ times.

We also know that this algorithm is asymptotically optimal, at least in terms of the number of comparisons done. Since a correct solution to the Maximum problem must look at every element in the array, it must therefore do at least $n - 1$ comparisons. Since each comparison takes $\Theta(1)$ time, the best possible algorithm must take $\Omega(n)$ time. Let's formally prove this:

> Suppose there was an algorithm that could determine the maximum element of an array using fewer than $n - 1$ comparisons.  
> Let $G$ be a graph such that each vertex in $G$ corresponds to an element in $A$, and each comparison done by a run of the algorithm between any two elements results in an edge between those two elements.  
> Clearly, there are $n - 2$ edges or less, and $n$ vertices.  
> Therefore, there are at least 2 components in $G$, since the graph cannot be connected.  
> Clearly, the solution could be in any of the components, and can only be found by comparing them.  
> Therefore, the algorithm cannot exist.  

An algorithm to find the minimum and the maximum element of an array can also trivially be designed, using $2n - 2$ comparisons and $\Theta(n)$ time. However, it is possible to design another algorithm that does fewer than $2n - 2$ comparisons (though the time complexity might be worse).

One way to do this is to consider elements two at a time - we compare elements one pair at a time, the larger of the pair with the maximum, and the smaller of which with the minimum. That means we need 3 comparisons per pair, or $\ceil{\frac 3 2 n} - 2$ in total, a ~25% improvement:

    def find_min_max(array):
        if array[0] < array[1]: min, max = array[0], array[1]
        else: max, min = array[0], array[1
        for i in range(2, len(array), 2):
            if array[i] < array[i + 1]: small, big = array[i], array[i + 1]
            else: big, small = array[i], array[i + 1]
            if big > max: max = big
            if small < min: min = small
        if len(array) % 2:
            if array[-1] > max: max = array[-1]
            if array[-1] < min: min = array[-1]

It can actually be proven, using graph theory, that the number of comparisons for this algorithm is optimal. However, it's too lengthy and complicated to cover here.

Suppose we want to determine whether and which three elements of an array of integers sum to 0. This is pretty easy to do in $O(n^3)$ (specifically, $n \choose 3$ runs of the inner loop) simply by checking all possible triples in the array. Basically, we pick two elements $A[i]$ and $A[j]$, then try to search for a $A[k] = -(A[i] + A[j])$ using linear search. However, it is actually possible to do so in $O(n^2 \log n)$ by sorting the array first, and searching for $k$ using binary search instead.

In fact, it's possible to do even better by sorting the array, taking each $A[i]$, and searching from both ends of the array inward for $j$ and $k$ such that $A[j] + A[k] = -A[i]$. After each step in the search, we either move inward from the left if $A[j] + A[k] < -A[i]$, or inward from the right if $A[j] + A[k] > -A[i]$

# 16/9/15

The pseudocode for the above algorithm looks like the following:

    def better_3sum(array):
        array = sorted(array)
        result = []
        for i in range(n - 2):
            j, k = i + 1, n
            while j < k:
                triple_sum = array[i] + array[j] + array[k]
                if triple_sum < 0: j += 1
                elif triple_sum > 0: k -= 1
                else:
                    result.append((i, j, k))
                    j += 1
                    k -= 1

Basically, this goes through each value of $i$, and does an $O(n)$ search for two values that would make it possible to have all three sum to 0. Proving this correct is left as an exercise to the reader - prove that $k - j$ is monotonically decreasing, and the pairs cover all possible pairs that can possibly sum up to $A[i]$. It's somewhat reminiscent of the array merge in merge sort.

This algorithm is $O(n^2)$, since the search is $O(n^2)$ and the sort is $O(n \log n)$. There are actually even better algorithms, and the best currently known ones are $O\left(n^2 \left(\frac{\log \log n}{\log n}\right)^2\right)$.

A **problem** is a computational task. A **problem instance** is the input for the computational task. The **problem solution** is the output. The **size of a problem instance** is a positive integer that is a measure of the size of the instance, and depends on the problem itself. The size is usually fairly intuitive, such as the size of an array for array sum, but sometimes it gets a bit more tricky.

An **algorithm** is a high level description of a computation. An algorithm **solves** a problem if it finds a valid solution for any instance in finite time. A **program** is an implementation of an algorithm using a computer language.

Time Complexity Analysis
------------------------

The running time of a program $M$ for a problem instance $I$ is $T_M(I)$. The worst case running time for problem instances of size $n$ for the program is $T_M(n)$. The average case running time is $T_M^{avg}(n)$.

The **worst case time complexity** of an algorithm $A$ is $f(n)$ such that a program $M$ exists implementing $A$ such that $T_M(n) \in \Theta(f(n))$.

Running time can only be found by running the program on a computer. Complexity is independent, but we lose information such as the constant factors.

To get the time complexity of an algorithm, we can either use $\Theta(f(n))$ throughout our analysis, or prove $O(f(n))$ and then $\Omega(f(n))$, which is often easier due to being able to make more assumptions for each separate proof.

The complexity of a loop in code is sum of the complexity of its body over each iteration of the loop. The complexity of consecutive operations is the sum of the complexities of the operations they are composed of.

Prove that $(\ln n)^a \in o(n^b)$ for any $a$ and $b$:

> Let $L = \lim_{n \to \infty} \frac{(\ln n)^a}{n^b} \lH \lim_{n \to \infty} \frac{a (\ln n)^{a - 1}}{b n^b} \lH \ldots \lH \lim_{n \to \infty} \frac{a!}{b^a n^b} = 0$.  
> By the limit order rule, $(\ln n)^a \in o(n^b)$.  

# 21/9/15

Useful to know:

* $\sum_{i = 0}^{n - 1} (a + di) = na + \frac{dn(n - 1)} 2$ - arithmetic sequence
* $\sum_{i = 0}^{n - 1} ar^i = \begin{cases}
a\frac{r^n - 1}{r - 1} &\text{if } r > 1 \\
na &\text{if } r = 1 \\
a\frac{1 - r^n}{1 - r} &\text{if } r < 1 \\
\end{cases}$ - geometric sequence
* $\sum_{i = 0}^{n - 1} (a + di)r^i = \frac a {1 - r} - \frac{(a + (n - 1)d)r^n}{1 - r} + \frac{dr(1 - r^{n - 1})}{(1 - r)^2}$ -arithmetic-geometric sequence
* $\sum_{i = 1}^n \frac 1 {i^2} = \frac{\pi^2} 6 = \Theta(1)$
* $_n = \sum_{i = 1}^n \frac 1 i \in \Theta(\log n)$ - harmonic sequence

The order of $n!$ is $\Theta(n^2 \sqrt n e^{-n})$. This is similar to Striling's approximation, $\sqrt{2 \pi n} n^n e^{-n}$ (this gets more and more accurate for larger values).

For order notation, the base of logarithmic terms don't matter because $\log n = \frac 1 {\log_b 10} \log_b n$, and $\frac 1 {\log_b 10}$ is a constant factor.

**Polynomial growth rates** are those in $O(n^k), k \in \mb{R}$. For example, the best known algorithm for graph isomorphism is $n^{\sqrt n \log_2 n}$, so it is not polynomial.

# 23/9/15

Consider the following program:

```python
def useless(n):
    for i in range(n):
        while j >= 1:
            j /= 2
```

Clearly, each iteration of the inner loop will run $\Theta(\log n)$ times, so the overall time complexity is $\sum_{i = 1}^n \Theta(\log i) = \Theta(\sum_{i = 1}^n \log i) = \Theta(\log(\prod_{i = 1}^n i)) = \Theta(\log(n!)) = \Theta(n \log n)$.

A **recurrence relation** specifies $a_n$ in the infinite sequence of real numbers $a_1, a_2, a_3, \ldots$ in terms of the previous terms $a_1, \ldots, a_{n - 1}$ ($a_1$ must therefore be a constant - the **initial value**). A **solution** to a recurrence relation is a closed form formula for $a_n$ - one that does not depend on the previous values of $a_1, \ldots, a_{n - 1}$. These are generally solved using techniques like guess and check or recursion trees, but there aren't any methods that are guaranteed to solve all recurrences, and in fact, reurrences may not necessarily even have solutions.

Recurrence relations can also be written using function notation, like $T(1) = 2, T(n) = T(n - 1) + 1$.

The **guess and check** method involves computing enough $a_n$ instances to guess the form of the solution (without any constants). Then, we solve for the constants using the computed values, and check if our solution is correct - if it is, we must prove it is correct using induction, and if not, we start over with a different guess.

Solve $a_0 = 4, a_n = a_{n - 1} + 6n - 5$:

> Guess and check: the first few elements of the sequence are $a_0 = 4, a_1 = 5, a_2 = 12, a_3 = 25, a_4 = 44$.  
> This seems to be quadratic growth, so we guess $an^2 + bn + c, a, b, c \in \mb{R}$ as the solution.  
> Solving for $a, b, c$, $c = 4, a + b + c = 5, 4a + 2b + c = 12$, we get $a = 3, b = -2, c = 4$.  
> So we guess that $a_n = 3n^2 - 2n + 4$. We can verify this for $0 \le n \le 4$ as our inductive base case.  
> Assume for some $k \in \mb{N}$ that $a_k = 3k^2 - 2k + 4$.  
> Clearly, $a_{k + 1} = 3k^2 - 2k + 4 + 6(k + 1) - 5 = 3(k + 1)^2 - 2(k + 1) + 4$.  
> So by induction, $a_n = 3n^2 - 2n + 4$.  

The **recurrence tree** method is often used for divide-and-conquer algorithms. This technique involves building a call tree for the recurrence. then summing up the results of the function on each level of the tree.

To construct a recurrence tree, we start with the timing function $T(n)$, then add children for each recursive call. The recursive calls are then removed from the parent. This is repeated for the children, all the way until we reah the base case. Note that after each step of growing the tree, the sum of all the nodes are always equivalent to $T(n)$.

Then, we sum the nodes at each level in the tree. The sum of all these sums is equal to $T(n)$.

Use the recurrence tree method to find the running time of merge sort:

> Clearly, $T(n) = \begin{cases}
2T\left(\frac n 2\right) + cn &\text{if } n > 1 \wedge n \text{ is a power of 2} \\
d &\text{if } n = 1 \\
\end{cases}$ where $c, d$ are constants.  
> Construct a recursion tree: start with the root node $N$ with value $T(n)$, then add 2 children, $N_1, N_2$, both $T\left(\frac n 2\right)$, and replace $N$'s value with $cn$.  
> What we get is a binary tree with every internal node having 2 children, and value $c2^i$ (where $i$ is the level in the tree, where 0 is the bottommost level, the leaf nodes), and each leaf node has value $d$.  
> Let $n = 2^j$. Then at the top level each node has value $c2^j$, at the second level $c2^{j - 1}$, at the third level $c2^{j - 2}$, and so on. All leaf nodes must have value $d$, since it is the only base case.  
> Clearly, the height of the tree must then be $j + 1$, with $j$ interior levels. Clearly, at the bottom level there are $2^j$ leaf nodes, since each level $i$ has $2^{j - i}$ nodes. The sum of the bottom level is therefore $2^j d = dn$.  
> Clearly, at each internal level $i$ each node has value $c 2^i$. Since there are $2^{j - i}$ nodes, the sum of each interior level is $c2^{j - i}2^i = c2^j = cn$.  
> So $T(n) = dn + jcn = dn + cn \log_2 n = \Theta(n \log n)$.  

The **master theorem** is a generalized formula for solving certain forms of recurrences. One thing it says is that given $T(n) = aT(\frac n b) + \Theta(n^y)$ where $a \ge 1, b > 1$, and $n$ is a power of $b$, then $T(n) \in \begin{cases}
\Theta(n^x) &\text{if } y < x \\
\Theta(n^x \log n) &\text{if } y = x \\
\Theta(n^y) &\text{if } y > x \\
\end{cases}$ where $x = \log_b a$.

Also, the exact value is $T(n) = da^j + cn^y \sum_{i = 0}^{j - 1} \left(\frac a {b^y}\right)^i$, or more simply $T(n) = dn^x + cn^y \sum_{i = 0}^{j - 1} r^i$ where $x = \log_b a$ and $r = \frac a {b^y} = b^{x - y}$.

This can be proved by constructing tree, figuring out the geometric sequence for the tree sums, and then solving for its value for each of the three cases.

A more general version, which will not be proved here, is that given $T(n) = aT(\frac n b) + f(n)$ where $a \ge 1, b > 1$, and $n$ is a power of $b$, then $T(n) \in \begin{cases}
\Theta(n^x) &\text{if } f(n) \in O(n^{x - \epsilon}) \text{ for some } \epsilon > 0 \\
\Theta(n^x \log n) &\text{if } f(n) \in \Theta(n^x) \\
\Theta(f(n)) &\text{if } \frac{f(n)}{n^{x + \epsilon}} \text{ is an increasing function of } n \text{ for some } \epsilon > 0 \text{ after } n \text{ is greater than some } n_0 \\
\end{cases}$ where $x = \log_b a$.

# 28/9/15

Solve $T(1) = 1, T(n) = 3T(\frac n 4) + n \log n$:

> Let $a = 3, b = 4, f(n) = n \log n$, so $x = \log_4 3$ and $f(n) / n^{x + \epsilon}$  is an increasing function of $n$ for $\epsilon = 1 - \log_4 3$ since $f(n) / n^{x + \epsilon} = \log n$.  
> So $T(n) \in \Theta(n \log n)$.  

The main task when using the master theorem to prove time complexities is choosing the right $\epsilon$ to make one of the cases valid. For example, the master theorem cannot be used for something like $T(1) = 1, T(n) = 2T(\frac n 2) + n \log n$, since none of the cases apply. Instead, we can use the recurrence tree method to solve the recurrence directly:

> Each level $i$ (from the bottom) of the tree has $2^i$ nodes, each with value $2^{j - i}(j - i)$ where $n = 2^j$.  
> So at each level, the sum is $2^i 2^{j - i} (j - i) = (j - i) 2^j$.  
> So the sum of the tree is $\sum_{i = 0}^j (j - i) 2^j = j^2 2^j - 2^j \sum_{i = 0}^j i = 2^j\left(j^2 - \frac{j(j + 1)}{2}\right) = \Theta(n \log^2 n)$.  

Divide and Conquer
------------------

Divide and conquer is a design strategy for algorithms where we divide a problem into smaller subproblems, solve those individually, then combine them back together to get a good result. Formally:

* Divide the problem instance $I$ into smaller subproblems $I_1, \ldots, I_n$.
* Solve $I_1, \ldots, I_n$ recursively to get sollutions $S_1, \ldots, S_n$.
* Use $S_1, \ldots, S_n$ to compute $S$ - combine the results. This is usually the hardest step.

For example, mergesort splits an array into two subarrays, and then recursively sorts those. Then, the merge is performed to combine the two sorted subarrays.

The time complexity of this is $T(n) = \begin{cases}
T(\ceil{\frac n 2}) + T(\floor{\frac n 2}) + cn &\text{if } n > 1 \\
d &\text{if } n = 1 \\
\end{cases}$ for some constants $c, d$. This is called the **exact recurrence**. If we remove the ceilings and floors, we get a **sloppy recurrence** $T(n) = \begin{cases}
2T(\frac n 2) + cn &\text{if } n > 1 \\
d &\text{if } n = 1 \\
\end{cases}$, which only works the same when $n$ is a power of 2, but is a lot simpler to work with.

The master theorem can prove that mergesort is $\Theta(n \log n)$ for the sloppy recurrence, and therefore only for when $n$ is a pwoer of 2. However, if we want to get the time complexity for all $n$, we need to use induction on the exact recurrence.

# 30/9/15

### Non-dominated points

Suppose we have a set of 2D coordinates/points. One point **dominates** another if it has a greater or equal X and Y coordinate than the other. We want to find those points that are non-dominated - the points that are not dominated by any other point. The non-dominated points form a sort of boundary below which all the dominated points lie.

Clearly, this can easily be solved simply by checking every pair of points to see which dominate which, but this would take $\Theta(n^2)$ time.

A better solution is to sort the points by their X coordinate, and then apply divide and conquer.

Dividing larger sets of points $S$ is also easy - we can just split the sorted points into two sets $S_1, S_2$, and then recursively solve the problem. It is the combining step that is the difficult step.

Given a point and the highest point on its right, it is easy to determine whether it is dominated. This is our base case

Clearly, no point in $S_1$ can dominate a point in $S_2$, since the X coordinates of points in $S_1$ is strictly less than points in $S_2$. So we find the point in $S_2$ with the highest Y coordinate, and all points in $S_1$ with Y coordinate at or below this value are guaranteed to be dominated by at least one point in $S_2$. In this way, we can eliminate all the points in $S_1$ that are dominated by points in $S_2$ in $O(n)$ time - this is the combining step.

In code, this looks like:

```python
def non_dominated(points):
    points = sorted(points, key=lambda p: p.x)
    if len(points) == 1: return {points[0]}
    pivot = floor(len(points) / 2)
    left = non_dominated(points[:pivot])
    right = non_dominated(points[pivot + 1:])
    while i < len(left) and left[i].y > right[1].y:
        i += 1
```

Clearly, the time complexity of this algorithm is $T(n) = T\left(\floor{\frac n 2}\right) + T\left(\ceil{\frac n 2}\right) = O(n \log n)$ (the sloppy version of this can be proven using the master theorem).

Without divide and conquer, we could also just sort the list, and then iterate though it from right to left, keeping track of the largest Y axis coordinate seen so far and eliminating points at or below this Y coordinate:

```python
def non_dominated(points):
    largest = float("-inf")
    for point in reversed(sorted(points, key=lambda p: p.x)):
        if point.y <= largest:
            yield point
        else:
            largest = point.y
```

### Closest pair

Suppose we have a set $Q$ of $n$ distinct 2D points in Euclidean space. We want to find the two points that are closest together - the points $u, v$ for which $\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}$ is minimised.

Clearly, minimising $\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}$ is the same as minimising $(u_x - v_x)^2 + (u_y - v_y)^2$. We will again sort the points in $Q$ by X coordinate, taking $\Theta(n \log n)$ time.

The dividing step is again trivial to get two partitions $Q_1, Q_2$, but the combining step takes some work.

If we are given the distance $\delta$ between the closest pair of points seen so far in the left and right partitions, and a point $p$, we know that if this point is in the closest pair of points, the other point must have X coordinate within $\delta$ of this point's X coordinate. Therefore, the other point must be in the **critical strip** - a section of the plane from $p_x - \delta$ to $p_x + \delta$ and extending infinitely both up and down.

We now want to find the closest pair between the left and right partitions, if it is less than the .

There is also a lemma that says that if a critical strip $R$ of $Q$ is sorted by Y coordinate, and $R[j]$ and $R[k]$ have distance less than $\delta$ where $j < k$, then $k \le j + 7$. Here, $\delta$ is the smaller of the minimum distance between points in $Q_1$, and the minimum distance between points in $Q_2$. This can be proven by dividing the strip around the point as squares of side length $\frac \delta 2$, and then showing that no two points can be in the same square. In other words, within a critical strip and given $\delta$, the point closest to a given point can be determined in 7 comparisons or less.

```python
def closest_pair(points, start, end):
    if start == end: return float("inf")
    pivot = floor((left + right) / 2)
    closest_left = closest_pair(points, left, pivot)
    closest_right = closest_pair(points, pivot + 1, right)
    closest = min(closest_left, closest_right)
    candidates = select_candidates(left, right, closest, points[pivot].x) # find points in the critical strip
    candidates = sorted(candidates, key=lambda p: p.y)
    closest = check_candidates(candidates, closest)
    return closest
```

The time complexity of this is $O(n \log n \log n)$. We can improve this by eliminating the Y coordinate sort of the candidates. One of them is to sort all the points $Q$ by Y coordinate to begin with, and then use that list when checking candidates, and then adding the necessary checks that make it work. Alternatively, we could replace the Y coordinate sort with a merge, and maintaining the candidates in sorted order.

```python
def closest_pair(points, start, end):
    if start == end: return float("inf")
    pivot = floor((left + right) / 2)
    closest_left = closest_pair(points, left, pivot)
    closest_right = closest_pair(points, pivot + 1, right)
    closest = min(closest_left, closest_right)
    merge_in_place(points, start, pivot, end)
    candidates = select_candidates(left, right, closest, points[pivot].x) # find points in the critical strip
    closest = check_candidates(candidates, closest)
    return closest
```

# 5/10/15

The **size** of an integer is the number of bits that we use to represent it, rather than the value of the integer itself - this is basically just $\ceil{\log_2 n}$. The **bit complexity** of algorithms operating on integers is therefore the complexity as a function of the sizes of those integers.

Multiplication
--------------

### Integer multiplication

Suppose we have 2 positive binary integers, $X = [X_{k - 1}, \ldots, X_0]$ and $Y = [Y_{k - 1}, \ldots, Y_0]$. We want to compute the $2k$-bit product $Z = [Z_{2k - 1}, \ldots, Z_0]$. Naively, we can use the simple binary multiplication algorithm, to shift $Y$ by each position $0 \le i < k$ and add those shifted values for which $X_i = 1$, in $O(k^2)$. Note that the fastest way to add two integers is $\Theta(k)$.

We can do better than this by using divide and conquer, by using $XY = (2^{\frac k 2} X_L + X_R)(2^{\frac k 2} Y_L + Y_R) = 2^k X_L Y_L + 2^{\frac k 2}(X_L Y_R + X_R Y_L) + X_R Y_R$, then solving for those four multiplications between $X_L, X_R, Y_L, Y_R$ (multiplying the powers of 2 are simply shifts, so they're basically free):

```python
def multiply(x, y):
    if len(x) == 1: return x[0] * y[0]
    x_left, x_right = x[:len(x) // 2], x[len(x) // 2:]
    y_left, y_right = y[:len(y) // 2], y[len(y) // 2:]
    z_top = multiply(x_left, y_left)
    z_middle = add(multiply(x_left, x_right), multiply(x_right, y_left))
    z_bottom = multiply(x_right, y_right)
    z = add(
        shift_left(z_top, len(x)),
        shift_left(z_middle, len(x) // 2),
        z_bottom
    )
    return z
```

Using the master theorewm, we can show that this is takes $\Theta(n^2)$ time - no improvement over the naive algorithm. However, this is a good base for an improved version.

Consider $(X_L + X_R)(Y_L + Y_R) = X_L Y_L + X_L Y_R + X_R Y_L + X_R Y_R$. So $X_L Y_R + X_R Y_L = (X_L + X_R)(Y_L + Y_R) - X_L Y_L - X_R Y_R$. Since we need to calculate $X_L Y_L$ and $X_R Y_R$ anyways, we can compute $X_L Y_R + X_R Y_L$ with just one multiplication by computing $(X_L + X_R)(Y_L + Y_R)$.

So using this:

```python
def multiply(x, y):
    if len(x) == 1: return x[0] * y[0]
    x_left, x_right = x[:len(x) // 2], x[len(x) // 2:]
    y_left, y_right = y[:len(y) // 2], y[len(y) // 2:]
    z_top = multiply(x_left, y_left)
    z_middle = multiply(add(x_left, x_right), add(y_left, y_right))
    z_bottom = multiply(x_right, y_right)
    z = add(
        shift_left(z_top, len(x)),
        shift_left(z_middle, len(x) // 2) - z_top - z_bottom,
        z_bottom
    )
    return z
```

Now, we only solve 3 subproblems instead of 4. This gives us a time complexity of around $\Theta(k^{\log_2 3})$, or about $\Theta(k^{1.59})$.

Even better algorithms exist. One is Toom-Cook, which splits $X$ and $Y$ into 3 pieces each, and solves 5 subproblems, all in $O(k^{\log_3 5})$. The Stronhage-Strassen method takes $O(k \log \log \log k)$ time, using Fourier transforms. The Furer method is the most advanced as of the present, using $O(k \log k 2^{3 \log^* k})$ time where $\log^* k$ is the inverse Ackermann function.

### Matrix multiplication

We will now assume that integer multiplication can be done in constant time, to avoid having to consider bit complexity.

Suppose we have two $n$ by $n$ matrices $A, B$. Clearly, the naive algorithm for multiplying them is $\Theta(n^3)$, since we just take the dot product of the corresponding row of $A$ and the corresponding column of $B$ - the dot product takes $\Theta(n)$, and there are $\Theta(n^2)$ cells in the result.

We can use the same technique as for integer multiplcation - we break each $n$ by $n$ matrix into 4 $\frac n 2$ by $\frac n 2$ matrices, multiply those, then combine those together, then again try to remove subproblem calculations where possible.

If we split $A, B$ into 4 blocks $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}, B = \begin{bmatrix} e & f \\ g & h \end{bmatrix}$, then $AB = \begin{bmatrix} ae + bg & af + bh \\ ce + dg & cf + dh \end{bmatrix}$. If we use divide and conquer to solve these 8 multiplications in the block version, it still takes $\Theta(n^3)$. However, the problem is now in a form that is useful for optimizing, by reducing the number of sub-problems.

Strassen's method notes that if $P_1 = a(f - h), P_2 = (a + b)h, P_3 = (c + d)e, P_4 = d(g - e), P_5 = (a + d)(e + h), P_6 = (b - d)(g + h), P_7 = (a - c)(e - f)$, then $ae + bg = P_5 + P_4 - P_2 + P_6, af + bh = P_1 + P_2, ce + dg = P_3 + P_4, cf + dh = P_5 + P_1 - P_3 - P_7$.

So $AB = \begin{bmatrix} P_5 + P_4 - P_2 + P_6 & P_1 + P_2 \\ P_3 + P_4 & P_5 + P_1 - P_3 - P_7 \end{bmatrix}$, and since there are only 7 multiplications needed to calculate $P_1, \ldots, P_7$, we only have 7 sub-problems to solve. As a result, the algorithm is $\Theta(n^{\log_2 7})$ or around $\Theta(n^{2.81})$, since the recurrence is $T(n) = 7T\left(\frac n 2\right) + \Theta(n^2)$ (solve this using the master theorem).

Research into this problem has resulted into better algorithms. The current state of the art is the Coppersmith-Winograd algorithm, taking around $O(n^{2.373})$ time.

### Selection

Suppose we have an array $A$ of size $n$, and we want to find the $k$th smallest integer, where the smallest element is found when $k = 1$. The median algorithm is a special case of this where $k = \ceil{\frac n 2}$.

Naively, we could just sort the array and pick the $k$th element, all in $\Theta(n \log n)$ time. Another naive approach is to use a modified selection sort (repeatedly finding the smallest element in the remaining part of the array, and putting it at the end of the sorted array), where we stop after $k$ moves, all in $\Theta(kn)$ time - this is linear if $k$ is constant, but quadratic if it is not. Combining the two, we can use a modified heapsort, which can solve the problem in $\Theta(n + k \log n)$ time.

# 7/10/15

There's a make-up lecture on November 21st or so.

Using divide and conquer, we might do something similar to quicksort, called quickselect - we pick a pivot, partition the elements, and then recursively quickselect within the partition that contains the desired element:

```python
def quickselect(array, k):
    pivot = array[0]
    partition1, partition2 = [x for x in array if x < pivot], [x for x in array if x > pivot]
    if k - 1 == len(partition1):
        return pivot
    elif k - 1 < len(partition1):
        return quickselect(partition1, k)
    else:
        return quickselect(partition2, k - len(partition1) - 1)
```

Note that unlike quicksort, there's only one recursive call in each invocation. On average, the pivot will be good about 50% of the time, where a good pivot is one that is within the 25th and 75th percentile inclusive. So on average, we would expect that every two calls, we would get a good pivot. So on average, every other call would have partitions that are of size at most 75% of the array's size. Therefore, on average the size of the subproblem would be reduced by 25% on every other call, which makes this function $\Theta(n)$ average case.

Note that in the worst case, the pivot would be at one of the extremes of the percentiles, and so the subproblem would only be reduced by a constant amount, and there are $n$ calls. Therefore, the function is $\Theta(n^2)$ worst case.

However, if we could always choose a good pivot, then we can ensure that the function is $\Theta(n)$ worst case as well. Doing so is a bit more clever, but it is possible, using the **median of medians** algorithm. First, we assume that there are at least 15 elements, so $n = 10r + 5 + \theta$, where $r \ge 1, 0 \le \theta \le 9$.

We now split $A$ into subarrays of exactly 5 elements: $B_1, \ldots, B_{2r + 1}$ (we ignore the last $\theta$ elements entirely here), and then find the median of each $B_i$, as $m_i$. Then, we use a selection algorithm to find the median of this array of medians.

We claim that $y$ is always a good pivot - that is is guaranteed to be between the 25th and 75th percentile inclusive. We will prove this later, but first, we will integrate it into quickselect:

```python
def median_of_medians_quickselect(array, k):
    # generate a good pivot
    if len(array) < 15: return sorted(array)[k]
    write $n = 10r + 5 + \theta$
    construct $B_1, \ldots, B_{2r + 1}$
    find medians $M = [m_1, \ldots, m_{2r + 1}]$
    pivot = median_of_medians_quickselect(M, r + 1)
    
    # do the rest of the steps from quickselect
    partition1, partition2 = [x for x in array if x < pivot], [x for x in array if x > pivot]
    if k - 1 == len(partition1):
        return pivot
    elif k - 1 < len(partition1):
        return quickselect(partition1, k)
    else:
        return quickselect(partition2, k - len(partition1) - 1)
```

We have two recursive calls now, since the median of medians algorithm also required us to do a selection to find `pivot` ($y$).

There are $r + 1$ medians at or below $y$ in $m_1, \ldots, m_{2r + 1}$ - let these be represented $m_{i_1}, \ldots, m_{i_{r + 1}}$. Clearly, in each $B_{i_1}, \ldots, B_{i_{r + 1}}$, there are at least 3 out of the 5 elements that are less than $y$ (since they are less than the median). Therefore, there must be at least $3(r + 1)$ elements that are less than $y$. Since there are $10r + 5$ elements (since we ignored $\theta$), the ratio of elements below $y$ to the total number of elements is $\frac{3(r + 1)}{10r + 5}$, or basically $\frac 3 {10}$. Therefore, 30% or more of the elements must be less than or equal to $y$.

Using a similar argument, but with the elements at or above $y$ rather than at or below, we can prove that 30% or more of the elements must be greater or equal to $y$. Therefore, $y$ is within the 30th to 70th percentile inclusive.

Therefore, the sloppy recurrence is $T(n) \le T(\frac n 5) + T(\frac{7n}{10}) + \Theta(n)$, which we can solve using the recurrence tree method - each node has two children, which are the node's value times $\frac 1 5$ and $\frac 7 {10}$, respectively, and the root node's value is $n$. When we draw the tree, we see that each level $i$ from the root (level 0) has sum $\left(\frac 9 {10}\right)^in$, and the sum of all levels to infinity converges to $10n$, so $T(n) \in \Theta(n)$.

The exact recurrence for this is $T(n) \le T(\floor{\frac n 5}) + T(\floor{\frac{7n + 12}{10}}) + \Theta(n)$. This accounts for the rounding down in the partitioning, and accounts for the last $\theta$ values we were ignoring earlier.

Greedy Algorithms
-----------------

Greedy algorithms are often used for **optimization problems**, which are problems where the goal is to find a solution that both satisfies the **problem constraints**, and maximizes/minimizes an **objective/profit/cost function**.

**Feasible solutions** are those that satisfy the problem constraints, and given a problem instance $I$, are represented as $\mathrm{feasible}(I)$. The objective function is in $\mathrm{feasible}(I) \to \mb{R}$.

For greedy algorithms, it is generally the proof of correctness that is the hard part. Greedy algorithms build up partial solutions, making locally optimal decisions at each step, with the goal of getting a globally optimal result.

A feasible solution should be writeable as an $n$-tuple $\tup{x_1, \ldots, x_n}$. A **partial solution** is an $i$-tuple $\tup{x_1, \ldots, x_i}$ where $i < n$ and none of the problem constraints are violated (note that this is not necessarily a feasible solution). A partial solution isn't always extendible to a feasible solution - some partial solutions are dead ends, and greedy algorithms can't get an optimal solution here.

A **choice set** for a partial solution $\tup{x_1, \ldots, x_i}$ is the set $\mathcal{X}$ of all elements we can append to $\tup{x_1, \ldots, x_i}$ to make another, longer partial or feasible solution $\tup{x_1, \ldots, x_i, y}$ - the set of all possible $y$.

# 14/10/15

For any $y$ in the choice set $\mathcal{X}$, we have a **local evaluation criterion** $g(y)$, that measures the profit/cost of including $y$ in the current partial solution. **Extension** of a partial solution is the process of adding the $y$ with the highest $g(y)$ value to the partial solution. A greeedy algorithm, in general, is repeated extension until we have a feasible solution $X$.

Greedy algorithms do no backtracking or lookahead - they only consider the elements of the choice set at each step. That means they are often faster than algorithms that do backtracking, but the solution is possibly not as good. When implementing greedy algorithms, it is often efficient to do a preprocessing step to compute the local evaluation criterion, like sorting the elements of an array.

For some greedy algorithms, it is possible to always obtain an optimal solution. However, these proofs are often rather difficult.

Suppose we have a set $A$ of pairs representing intervals. What is the largest subset $B$ of these pairs such that none of the intervals intersect?

One greedy strategy might be to repeatedly choose the interval with the earliest start time that doesn't intersect with any intervals in the current partial solution. Another might be to choose the interval with the smallest duration that doesn't intersect. A third is to choose the interval with the earliest end time that doesn't intersect.

Clearly, the first one is not valid because we can make a counterexample: an interval that starts early but is really long is chosen over multiple non-intersecting short intervals that start a bit later, which means this strategy doesn't always give the optimal solution.

It is also possible to find a counterexample for the second strategy - a short interval that intersects with two non-intersecting longer intervals is chosen over those two, which means this strategy also doesn't always give the optimal solution.

The last strategy is correct. We can implement it as follows:

```python
def select_interval(intervals):
    intervals = sorted(intervals, key=lambda interval: interval[1])
    solution = [intervals[0]]
    latest_finish = intervals[0][1]
    for interval in intervals[1:]:
        if interval[0] >= latest_finish:
            solution.append(interval)
            latest_finish = interval[1]
    return solution
```

Most correctness proofs for greedy algorithms use strong induction. The proof for the greedy interval selection algorithm is relatively straightforward:

> Let $A$ be an array of intervals, sorted by their end values.  
> Let $\mathcal{B} = \tup{A_{i_1}, \ldots, A_{i_k}}$ where $i_1 < \ldots < i_k$ be the greedy solution.  
> Let $\mathcal{O} = \tup{A_{j_1}, \ldots, A_{j_k}}$ where $j_1 < \ldots < j_l$ be the optimal solution.  
> Clearly, $k \le l$ since $O$ is the optimal solution, and it is impossible for the greedy solution to be better than optimal.  
> We want to first prove that $i_m \le j_m$ for all $1 \le m \le k$, which means that the ends of the intervals in the greedy solution are always before the ends of the corresponding intervals in the optimal solution.  
> For $m = 1$, $i_1 \le j_1$ since $i_1 = 1$ - the greedy algorithm always chooses the first interval.  
> Assume $i_{m - 1} \le j_{m - 1}$. So in the greedy solution, the $m - 1$th interval ends no later than the $m - 1$th interval in the optimal solution.  
> Suppose $i_m > j_m$. Clearly, the optimal solution's $m$th interval ends earlier than the greedy algorithm's $m$th interval, since $A$ is sorted by end values.  
> However, this is not possible, since that means the greedy algorithm didn't choose the earliest end time. So $i_m \le j_m$.  
> Suppose $k < l$. Clearly, the optimal solution must have at least $k + 1$ intervals.  
> Clearly, $i_k \le j_k$. So the optimal solution's $k + 1$th interval must be in the choice set for the greedy algorithm at the $k$th step.  
> However, this isn't possible since the greedy algorithm didn't choose that interval, so $k \ge l$.  
> Since $k \le l$ and $k \ge l$, $k = l$ and the greedy solution is the same length as the optimal solution, and so the greedy algorithm must be optimal.  

The time complexity of this algorithm is easy to prove - it's $\Theta(n \log n)$ for the sort and $\Theta(n)$ for the traversal, so $\Theta(n \log n)$ overall.

Suppose we want to partition $A$ into disjoint sets of non-intersecting intervals, minimizing the number of these sets. This is called the interval coloring problem - we want to assign a color to each interval such that all intervals with a given color do not intersect each other.

An optimal greedy strategy for this is to sort the intervals by start values ascending, and for each interval in order, attempt to insert each interval into existing sets (assign it an existing color), and create a new set if not possible (assign it a new color):

```python
def partition_intervals(intervals):
    intervals = sorted(intervals, key=lambda interval: interval[0])
    partitions = [{intervals[0]}]
    partition_latest_finishes = [intervals[0][1]]
    for i in range(1, len(intervals)):
        interval = intervals[i]
        for partition_index, latest_finish in enumerate(partition_latest_finishes): # try to assign interval to existing partitions
            if latest_finish <= interval[0]:
                partitions[partition_index].add(interval)
                partition_latest_finishes[partition_index] = interval[1]
                break
        else: # no existing colors can accept the interval
            partition_index = len(partition_latest_finishes)
            partitions[partition_index] = {interval}
            partition_latest_finishes.append(interval[1])
    return partitions
```

The correctness proof for this algorithm is a bit unusual:

> Suppose the greedy algorithm uses $D$ colors. Since we only create new colors if existing colors can't accept the interval, for every color $c < D$, there exists an interval $A_c$ that overlaps $A_i$.  
> That means that there are at least $D$ intervals that all overlap each other, since they all overlap $A_i$.  
> Therefore there must be at least $D$ colors, which means that having $D$ colors is optimal.  

Clearly, this algorithm is $O(n^2)$, or more specifically $O(nD)$ since $D$ is $O(n)$. We can actually do better by using a different data structure for the inner loop - the inner loop simply is trying to find the lowest end value in a partition, so if we use a heap for the partitions instead, we can have the algorithm run in $O(n \log D)$, or more simply, $O(n \log n)$:

```python
import heapq
def partition_intervals(intervals):
    intervals = sorted(intervals, key=lambda interval: interval[0])
    partitions = [{intervals[0]}]
    partition_latest_finishes_heap = [(intervals[0][1], 0)]
    for i in range(1, len(intervals)):
        interval = intervals[i]
        latest_finish, partition_index = heapq.heappop(partition_latest_finishes_heap)
        if latest_finish <= interval[0]: # try to assign interval to the existing partition that ends earliest
            partitions[partition_index].add(interval)
            heapq.heappush(partition_latest_finishes_heap, (interval[1], partition_index))
        else: # no existing colors can accept the interval
            partition_index = len(partition_latest_finishes_heap)
            partitions[partition_index] = {interval}
            heapq.heappush(partition_latest_finishes_heap, (interval[1], partition_index))
    return partitions
```

# 19/10/15

Consider the **knapsack problem**: given profits $P = [P_1, \ldots, P_n]$ and weights $W = [W_1, \ldots, W_n]$, and a capacity $M$, all positive integers, find the $x = \tup{x_1, \ldots, x_n}$ that maximises $\sum P_i x_i$ given that $\sum W_i x_i \le M$.

There are several variations of this problem: for the 0-1 knapsack problem, $x_i$ is either 0 or 1; in the rational knapsack problem, $0 \le x_i \le 1$.

As it turns out, the knapsack problem can be solved greedily - we simply take as much as possible of each item, considering them in decreasing order of profit divided by weight:

```python
def greedy_rational_knapsack(profits, weights, capacity):
    items = sorted(zip(profits, weights), lambda x: x[0] / x[1])
    x = [0] * len(items)
    current_weight = 0
    for item in items:
        profit, weight = item
        if current_weight + weight <= capacity:
            result.append(1)
            current_weight += weight
        else:
            result.append((capacity - current_weight) / weight)
            break
    return x
```

Proof of correctness:

> Suppose the greedy algorithm is not optimal. Then for certain inputs the output must differ from optimal algorithms.  
> Let the result of the greedy algorithm be $G = [G_1, \ldots, G_n]$ and a result $B = [B_1, \ldots, B_n]$ of all optimal algorithms for this problem such that $B$ and $G$ agree for as many items as possible, disagreeing at index $d$, so $B = [G_1, \ldots, G_{d - 1}, B_d, \ldots, B_n]$.  
> The greedy algorithm always chooses the item with the highest profit per weight, so the actual profit from $G_1, \ldots, G_d$ is at least as much as the actual profit of $B_1, \ldots, B_d$, because we will pick the most.  
> Construct $B' = [B_1, \ldots, B_{d - 1}, G_d, B_{d + 1}, \ldots, B_n]$. Clearly, $B'$ is at least as profitable as $B$, but it agrees with $G$ for 1 more element than $B$, a contradiction.  
> Therefore, the greedy algorithm is optimal.  

Consider the **coin changing problem**: given a list of coin denominations $d_1, \ldots, d_n$ and a positive integer $T$ (target sum), find $A = [A_1, \ldots, A_n]$ such that $T = \sum a_i d_i$ and $N = \sum A_i$ is minimised. In other words, we are trying to find the smallest set of coins that add up to a certain value.

We can do this simply by considering the coins from largest denomination to smallest, filling up the sum as close to the target sum as possible. Although this works for a coin system like $1, 5, 10, 25, 100, 200$, this isn't always optimal: the coin system $12, 5, 1$ doesn't give the optimal solution for something like a target sum of 15. The study of where these algorithms are optimal is an area of active research.

Consider the **stable marriage problem**: suppose we have sets of $n$ men and $n$ women, $M = [M_1, \ldots, M_n], W = [W_1, \ldots, W_n]$. Define $\mathrm{pref}(M_i, k)$ is the $k$th favourite $w \in W$ for $M_i$, and $\mathrm{pref}(W_i, k)$ is the $k$th favourite $m \in M$ for $W_i$. Find the matching $\set{(M_{i_1}, W_{j_1}), \ldots, (M_{i_n}, W_{j_n})}$ such that there does not exist any $(M_u, W_v)$ that are not in the set, but prefer each other to their matches in the set - a **stable matching**.

Formally, we want to find a matching such that 

The Gale-Shaley algorithm is an algorithm for this problem, for which Gale and Shapely won a Nobel prize for economics. In this algorithm, men propose to women from their most preferable to least preferable, and women accept proposals if not engaged or if a proposal is better than their currently accepted proposal (the new proposal replaces the old one), and women reject proposals otherwise.

Basically, the men propose starting from their most preferable to least preferable, and the women are proposed to starting from the least preferable to most preferable - the men always move down in their preferences, and the women always move up in their preferences:

```python
def woman_prefers(woman, preference_function, man1, man2):
    i = 1
    while True:
        man = preference_function(woman, i)
        if man == man1: return True
        if man == man2: return False
        i += 1

def gale_shapley(men, women, preference_function):
    engagements = {}
    unengaged_men = {man: 0 for man in men}
    while unengaged_men:
        man = next(unengaged_men.keys())
        unengaged_men[man] += 1 # next rank in preferences
        woman = preference_function(man, unengaged_men[man])
        if woman not in engagements or woman_prefers(woman, preference_function, man, engagements[woman]):
            engagements[woman] = man
    return set(engagements.items())
```

This algorithm is good for the men, but not the women - suiters can ask any of the reviewers, but reviewers can only pick from the suiters that proposed to them. For example, given 3 men and 3 women with preferences $M_1: W_2, W_1, W_3; M_2: W_3, W_2, W_1; M_3: W_1, W_3, W_2; W_1: M_2, M_1, M_3; W_2: M_3, M_2, M_1; W_3: M_1, M_3, M_2$, all men get their first choices, while all women get their last choice. The men get the best possible match that will accept them, while the women get the worst possible match they will accept.

This algorithm always terminates because women cannot reject offers if unengaged, there is a man for every woman, and at some point a man will ask that woman. In the worst case, the men will all have their last choise, and have to propose to all $n$ of their preferences. Since there are $n$ men, the algorithm takes $O(n^2)$ proposals, specifically $n^2 - n + 1$ or less.

Proof of correctness:

> We want to prove that there is no man that prefers a woman that also prefers that man. Suppose Alice prefers Bob over her current partner, and Bob also prefers Alice over his current partner.  
> If Bob prefers Alice, then he must have proposed to her before his current partner.  
> Alice must have rejected the proposal, since Bob is engaged to a less preferable choice.  
> However, Alice is with a less preferable partner, which means Alice should have accepted Bob and rejected her current partner when he proposed.  
> This is a contradiction, so Alice and Bob cannot exist, and the matching is stable.  

# 21/10/15

To implement the Gale-Shapely algorithm more efficiently, we can represent the set of unengaged men as a linked list, represent the men's preferences using arrays/linked lists, and represent engagements using an array. We could also precompute a lookup table for $M_j = \mathrm{pref}(W_i, k)$ to look up $j$ given $W_i$ and $M_j$.


Dynamic Programming
-------------------

Consider a naive function to compute the $n$th Fibonacci number, based on its recurrence definition:

```python
def fibinacci(n):
    if n <= 1: return n
    return fibinacci(n - 1) + fibinacci(n - 2)
```

However, this is extremely inefficient. Using the recurrence tree method, we find that it takes $2f_{n + 1}$ total calls of the function where $f_n$ is the $n$th Fibonacci number. Using the closed form for the Fibonacci number, $f_n = \frac{\phi^n - (-\phi)^{-n}}{\sqrt{5}}$ where $\phi$ is the golden ratio, or around 1.62. Therefore, this function takes $\Omega(\phi^n)$ time.

This function basically is inefficient because it computes the same thing over and over again - for example, for `fibonacci(100)` we will call `fibonacci(10)` millions of times, when it guves the same result each time.

The idea behind dynamic programming is to avoid recomputing the same thing over and over again, by keeping those results around and simply looking them up when necessary. As a result dynamic programming trades off memory for time.

For example, if we were to write the above function iteratively, we might start from the base case and build upward toward the result:

```python
def fibinacci(n):
    saved = [0, 1]
    for i in range(2, n + 1):
        saved[i] = saved[i - 1] + saved[i - 2]
    return saved[n]
```

However, note that at any given time, we only really use the last two elements of `saved`. Therefore, we can just store the last two elements:

```python
def fibinacci(n):
    if n <= 1: return n
    last1, last2 = 0, 1
    for i in range(2, n + 1):
        current = last1 + last2
        last1, last2 = current, last1
    return current
```

This is much better - we now have $\Theta(n)$ iterations and constant memory. Since the numbers grow exponentially, however, and addition takes $\Theta(\log n)$ (using multiprecision arithmetic), addition will actually take $\Theta(n)$, so the algorithm overall takes $\Theta(n^2)$.

Dyanmic programming is used to solve optimization problems, like greedy algorithms are often used to do.

To solve a problem using dynamic programming, we examine the structure of an optimal solution - the **optimal structure**. Given a problem instance $I$:

1. Define all the relevant subproblems $S(I)$ that allows us to compute $I$.
    * This is just the direct subproblems, not the subproblems of the subproblems. For example, for `fibonacci(n)` we would have $S(I)$ be `fibonacci(n - 1)` and `fibonacci(n - 2)`.
2. Write a recurrence relation for $I$ in terms of $S(I)$ and the base cases.
    * The references to $S(I)$ allow us to look up the answers later in the table.
3. Compute the optimal solution to $I$ using the recurrence relation from the bottom-up (starting from the base cases of the recurrence relation) - filling in a table of values for the solutions to each subproblem.
    * Whenever we have a subproblem we've previously encountered, we can simply look up the answer in the table to avoid recomputing it.
    * When all subproblems have been solved, we can often compute the solution to $I$ very simply using values from the table.

This is somewhat similar to divide and conquer, where we have subproblems and solve them recursively, up until we actually need to combine the solutions - instead of combining recursively, we solve them from the bottom up.

While the 0-1 knapsack problem we saw previously is NP-complete, we can solve it relatively well using dynamic programming:

> Suppose $[x_1, \ldots, x_n]$ is an optimal solution for a given problem instance where $P = [P_1, \ldots, P_n]$ and weights $W = [W_1, \ldots, W_n]$, and capacity $M$.  
> Clearly, either $x_n = 0$ or $x_n = 1$.  
> If $x_n = 0$, then $[x_1, \ldots, x_{n - 1}]$ is an optimal solution to a smaller problem instance where $P = [P_1, \ldots, P_{n - 1}]$ and weights $W = [W_1, \ldots, W_{n - 1}]$, and capacity $M$.  
> If $x_n = 1$, then $[x_1, \ldots, x_{n - 1}]$ is an optimal solution to a smaller problem instance where $P = [P_1, \ldots, P_{n - 1}]$ and weights $W = [W_1, \ldots, W_{n - 1}]$, and capacity $M - W_n$.  
> So $[x_1, \ldots, x_{n - 1}]$ can be computed by considering $x_n = 0$ and $x_n = 1$.  