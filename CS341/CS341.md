CS341
=====

Algorithms.

    Doug Stinson
    Section 001
    https://www.student.cs.uwaterloo.ca/~cs341/

$$
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
$$

# 14/9/15

Most materials will be on LEARN. Questions and answers can be posted on Piazza.

Study of the design and the analsis of algorithms - in particular, the correctness (proved via formal proofs) and efficiency (proved using time complexity analysis).

Useful metrics for algorithms are asymptotic complexity (best case/average case/worst case time/space complexity), number of specific computations used (like comparisons in sorting algorithms), whether an algorithm is the most efficient for a particular problem, etc.

Interesting things to know are lower bounds on possible algorithms to solve a particular problems, problems that cannot be solved by any algorithm (undecidability), and problems that cannot be solved efficiently by any algorithm, but can be solved (NP-hardness).

Useful design strategies we will go over are divide and conquer, greedy algorithms, dynamic programming, breadth-first and depth-first search, local search, and linear programming.

An example of an algorithm design is the maximum problem: given an array of integers $A$, find the maximum integer in $A$. A simple solution to this is as follows:

    def find_max(array):
        current = array[0]
        for elem in array[1:]:
            if elem > current: current = elem
        return elem

This seems to be obviously correct, but we can also use program verification to prove it correct. In this case, we will use induction to prove the loop invariant - at the end of each iteration of the loop, `current` is equal to the largest element encountered so far in the array. We'd check the base case for arrays of length 1, then prove the inductive hypothesis to formally verify the program.


Clearly, this algorithm is $\Theta(n)$, since it loops over the entire array once. Formal time complexity analysis can also be done by noting that the loop body takes $\Theta(1)$ time, and the loop runs $\Theta(n)$ times.

We also know that this algorithm is asymptotically optimal, at least in terms of the number of comparisons done. Since a correct solution to the Maximum problem must look at every element in the array, it must therefore do at least $n - 1$ comparisons. Since each comparison takes $\Theta(1)$ time, the best possible algorithm must take $\Omega(n)$ time. Let's formally prove this:

> Suppose there was an algorithm that could determine the maximum element of an array using fewer than $n - 1$ comparisons.  
> Let $G$ be a graph such that each vertex in $G$ corresponds to an element in $A$, and each comparison done by a run of the algorithm between any two elements results in an edge between those two elements.  
> Clearly, there are $n - 2$ edges or less, and $n$ vertices.  
> Therefore, there are at least 2 components in $G$, since the graph cannot be connected.  
> Clearly, the solution could be in any of the components, and can only be found by comparing them.  
> Therefore, the algorithm cannot exist.  

An algorithm to find the minimum and the maximum element of an array can also trivially be designed, using $2n - 2$ comparisons and $\Theta(n)$ time. However, it is possible to design another algorithm that does fewer than $2n - 2$ comparisons (though the time complexity might be worse).

One way to do this is to consider elements two at a time - we compare elements one pair at a time, the larger of the pair with the maximum, and the smaller of which with the minimum. That means we need 3 comparisons per pair, or $\ceil{\frac 3 2 n} - 2$ in total, a ~25% improvement:

    def find_min_max(array):
        if array[0] < array[1]: min, max = array[0], array[1]
        else: max, min = array[0], array[1
        for i in range(2, len(array), 2):
            if array[i] < array[i + 1]: small, big = array[i], array[i + 1]
            else: big, small = array[i], array[i + 1]
            if big > max: max = big
            if small < min: min = small
        if len(array) % 2:
            if array[-1] > max: max = array[-1]
            if array[-1] < min: min = array[-1]

It can actually be proven, using graph theory, that the number of comparisons for this algorithm is optimal. However, it's too lengthy and complicated to cover here.

Suppose we want to determine whether and which three elements of an array of integers sum to 0. This is pretty easy to do in $O(n^3)$ (specifically, $n \choose 3$ runs of the inner loop) simply by checking all possible triples in the array. Basically, we pick two elements $A[i]$ and $A[j]$, then try to search for a $A[k] = -(A[i] + A[j])$ using linear search. However, it is actually possible to do so in $O(n^2 \log n)$ by sorting the array first, and searching for $k$ using binary search instead.

In fact, it's possible to do even better by sorting the array, taking each $A[i]$, and searching from both ends of the array inward for $j$ and $k$ such that $A[j] + A[k] = -A[i]$. After each step in the search, we either move inward from the left if $A[j] + A[k] < -A[i]$, or inward from the right if $A[j] + A[k] > -A[i]$

# 16/9/15

The pseudocode for the above algorithm looks like the following:

    def better_3sum(array):
        array = sorted(array)
        result = []
        for i in range(n - 2):
            j, k = i + 1, n
            while j < k:
                triple_sum = array[i] + array[j] + array[k]
                if triple_sum < 0: j += 1
                elif triple_sum > 0: k -= 1
                else:
                    result.append((i, j, k))
                    j += 1
                    k -= 1

Basically, this goes through each value of $i$, and does an $O(n)$ search for two values that would make it possible to have all three sum to 0. Proving this correct is left as an exercise to the reader - prove that $k - j$ is monotonically decreasing, and the pairs cover all possible pairs that can possibly sum up to $A[i]$. It's somewhat reminiscent of the array merge in merge sort.

This algorithm is $O(n^2)$, since the search is $O(n^2)$ and the sort is $O(n \log n)$. There are actually even better algorithms, and the best currently known ones are $O\left(n^2 \left(\frac{\log \log n}{\log n}\right)^2\right)$.

A **problem** is a computational task. A **problem instance** is the input for the computational task. The **problem solution** is the output. The **size of a problem instance** is a positive integer that is a measure of the size of the instance, and depends on the problem itself. The size is usually fairly intuitive, such as the size of an array for array sum, but sometimes it gets a bit more tricky.

An **algorithm** is a high level description of a computation. An algorithm **solves** a problem if it finds a valid solution for any instance in finite time. A **program** is an implementation of an algorithm using a computer language.

Time Complexity Analysis
------------------------

The running time of a program $M$ for a problem instance $I$ is $T_M(I)$. The worst case running time for problem instances of size $n$ for the program is $T_M(n)$. The average case running time is $T_M^{avg}(n)$.

The **worst case time complexity** of an algorithm $A$ is $f(n)$ such that a program $M$ exists implementing $A$ such that $T_M(n) \in \Theta(f(n))$.

Running time can only be found by running the program on a computer. Complexity is independent, but we lose information such as the constant factors.

To get the time complexity of an algorithm, we can either use $\Theta(f(n))$ throughout our analysis, or prove $O(f(n))$ and then $\Omega(f(n))$, which is often easier due to being able to make more assumptions for each separate proof.

The complexity of a loop in code is sum of the complexity of its body over each iteration of the loop. The complexity of consecutive operations is the sum of the complexities of the operations they are composed of.

Prove that $(\ln n)^a \in o(n^b)$ for any $a$ and $b$:

> Let $L = \lim_{n \to \infty} \frac{(\ln n)^a}{n^b} \lH \lim_{n \to \infty} \frac{a (\ln n)^{a - 1}}{b n^b} \lH \ldots \lH \lim_{n \to \infty} \frac{a!}{b^a n^b} = 0$.  
> By the limit order rule, $(\ln n)^a \in o(n^b)$.  

# 21/9/15

Useful to know:

* $\sum_{i = 0}^{n - 1} (a + di) = na + \frac{dn(n - 1)} 2$ - arithmetic sequence
* $\sum_{i = 0}^{n - 1} ar^i = \begin{cases}
a\frac{r^n - 1}{r - 1} &\text{if } r > 1 \\
na &\text{if } r = 1 \\
a\frac{1 - r^n}{1 - r} &\text{if } r < 1 \\
\end{cases}$ - geometric sequence
* $\sum_{i = 0}^{n - 1} (a + di)r^i = \frac a {1 - r} - \frac{(a + (n - 1)d)r^n}{1 - r} + \frac{dr(1 - r^{n - 1})}{(1 - r)^2}$ -arithmetic-geometric sequence
* $\sum_{i = 1}^n \frac 1 {i^2} = \frac{\pi^2} 6 = \Theta(1)$
* $_n = \sum_{i = 1}^n \frac 1 i \in \Theta(\log n)$ - harmonic sequence

The order of $n!$ is $\Theta(n^2 \sqrt n e^{-n})$. This is similar to Striling's approximation, $\sqrt{2 \pi n} n^n e^{-n}$ (this gets more and more accurate for larger values).

For order notation, the base of logarithmic terms don't matter because $\log n = \frac 1 {\log_b 10} \log_b n$, and $\frac 1 {\log_b 10}$ is a constant factor.

**Polynomial growth rates** are those in $O(n^k), k \in \mb{R}$. For example, the best known algorithm for graph isomorphism is $n^{\sqrt n \log_2 n}$, so it is not polynomial.

# 23/9/15

Consider the following program:

```python
def useless(n):
    for i in range(n):
        while j >= 1:
            j /= 2
```

Clearly, each iteration of the inner loop will run $\Theta(\log n)$ times, so the overall time complexity is $\sum_{i = 1}^n \Theta(\log i) = \Theta(\sum_{i = 1}^n \log i) = \Theta(\log(\prod_{i = 1}^n i)) = \Theta(\log(n!)) = \Theta(n \log n)$.

A **recurrence relation** specifies $a_n$ in the infinite sequence of real numbers $a_1, a_2, a_3, \ldots$ in terms of the previous terms $a_1, \ldots, a_{n - 1}$ ($a_1$ must therefore be a constant - the **initial value**). A **solution** to a recurrence relation is a closed form formula for $a_n$ - one that does not depend on the previous values of $a_1, \ldots, a_{n - 1}$. These are generally solved using techniques like guess and check or recursion trees, but there aren't any methods that are guaranteed to solve all recurrences, and in fact, reurrences may not necessarily even have solutions.

Recurrence relations can also be written using function notation, like $T(1) = 2, T(n) = T(n - 1) + 1$.

The **guess and check** method involves computing enough $a_n$ instances to guess the form of the solution (without any constants). Then, we solve for the constants using the computed values, and check if our solution is correct - if it is, we must prove it is correct using induction, and if not, we start over with a different guess.

Solve $a_0 = 4, a_n = a_{n - 1} + 6n - 5$:

> Guess and check: the first few elements of the sequence are $a_0 = 4, a_1 = 5, a_2 = 12, a_3 = 25, a_4 = 44$.  
> This seems to be quadratic growth, so we guess $an^2 + bn + c, a, b, c \in \mb{R}$ as the solution.  
> Solving for $a, b, c$, $c = 4, a + b + c = 5, 4a + 2b + c = 12$, we get $a = 3, b = -2, c = 4$.  
> So we guess that $a_n = 3n^2 - 2n + 4$. We can verify this for $0 \le n \le 4$ as our inductive base case.  
> Assume for some $k \in \mb{N}$ that $a_k = 3k^2 - 2k + 4$.  
> Clearly, $a_{k + 1} = 3k^2 - 2k + 4 + 6(k + 1) - 5 = 3(k + 1)^2 - 2(k + 1) + 4$.  
> So by induction, $a_n = 3n^2 - 2n + 4$.  

The **recurrence tree** method is often used for divide-and-conquer algorithms. This technique involves building a call tree for the recurrence. then summing up the results of the function on each level of the tree.

To construct a recurrence tree, we start with the timing function $T(n)$, then add children for each recursive call. The recursive calls are then removed from the parent. This is repeated for the children, all the way until we reah the base case. Note that after each step of growing the tree, the sum of all the nodes are always equivalent to $T(n)$.

Then, we sum the nodes at each level in the tree. The sum of all these sums is equal to $T(n)$.

Use the recurrence tree method to find the running time of merge sort:

> Clearly, $T(n) = \begin{cases}
2T\left(\frac n 2\left) + cn &\text{if } n > 1 \wedge n \text{ is a power of 2} \\
d &\text{if } n = 1 \\
\end{cases}$ where $c, d$ are constants.  
> Construct a recursion tree: start with the root node $N$ with value $T(n)$, then add 2 children, $N_1, N_2$, both $T\left(\frac n 2\left)$, and replace $N$'s value with $cn$.  
> What we get is a binary tree with every internal node having 2 children, and value $c2^i$ (where $i$ is the level in the tree, where 0 is the bottommost level, the leaf nodes), and each leaf node is $d$.  
> Let $n = 2^j$. Then at the top level each node has value $c2^j$, at the second level $c2^{j - 1}$, at the third level $c2^{j - 2}$, and so on. All leaf nodes must have value $d$, since it is the only base case.  
> Clearly, the height of the tree must then be $j + 1$, with $j$ interior levels. Clearly, at the bottom level there are $2^j$ leaf nodes, since each level $i$ has $2^{j - i}$ nodes. The sum of the bottom level is therefore $2^j d = dn$.  
> Clearly, at each internal level $i$ each node has value $c 2^i$. Since there are $2^{j - i}$ nodes, the sum of each interior level is $c2^{j - i}2^i = c2^j = cn$.  
> So $T(n) = dn + jcn = dn + cn \log_2 n = \Theta(n \log n)$.  

The **master thoerem** is a generalized formula for solving certain forms of recurrences. One thing it says is that given $T(n) = aT( \frac n b) + \Theta(n^y)$ where $a \ge 1, b > 1$, and $n$ is a power of $b$, then $T(n) \in \begin{cases}
\Theta(n^x) &\text{if } y < x \\
\Theta(n^x \log n) &\text{if } y = x \\
\Theta(n^y) &\text{if } y > x \\
\end{cases}$ where $x = \log_b a$.

Also, the exact value is $T(n) = da^j + cn^y \sum_{i = 0}^{j - 1} \left(\frac a {b^y}\right)^i$, or more simply $T(n) = dn^x + cn^y \sum_{i = 0}^{j - 1} r^i$ where $x = \log_b a$ and $r = \frac a {b^y} = b^{x - y}$.

This can be proved by constructing tree, figuring out the geometric sequence for the tree sums, and then solving for its value for each of the three cases.

A more general version, which will not be proved here, is that given $T(n) = aT( \frac n b) + f(n)$ where $a \ge 1, b > 1$, and $n$ is a power of $b$, then $T(n) \in \begin{cases}
\Theta(n^x) &\text{if } f(n) \in O(n^{x - \epsilon}) \text{ for some } \epsilon > 0 \\
\Theta(n^x \log n) &\text{if } f(n) \in O(n^x) \\
\Theta(f(n)) &\text{if } \frac{f(n)}{n^{x + \epsilon}} \text{ is an incresing function of } n \text{ for some } \epsilon > 0 \\
\end{cases}$ where $x = \log_b a$.
