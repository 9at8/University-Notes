<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS341 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>
function highlight() { // highlight all code blocks using HighlightJS
  var code_blocks = document.getElementsByTagName("code");
  for (var i = 0; i < code_blocks.length; i++)
    hljs.highlightBlock(code_blocks[i]);
}
</script>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
  </ul>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1 \right\rceil}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
\]</span></p>
<h1 id="cs341">CS341</h1>
<p>Algorithms.</p>
<pre><code>Doug Stinson
Section 001
https://www.student.cs.uwaterloo.ca/~cs341/</code></pre>
<h1 id="section">14/9/15</h1>
<p>Most materials will be on LEARN. Questions and answers can be posted on Piazza.</p>
<p>Study of the design and the analsis of algorithms - in particular, the correctness (proved via formal proofs) and efficiency (proved using time complexity analysis).</p>
<p>Useful metrics for algorithms are asymptotic complexity (best case/average case/worst case time/space complexity), number of specific computations used (like comparisons in sorting algorithms), whether an algorithm is the most efficient for a particular problem, etc.</p>
<p>Interesting things to know are lower bounds on possible algorithms to solve a particular problems, problems that cannot be solved by any algorithm (undecidability), and problems that cannot be solved efficiently by any algorithm, but can be solved (NP-hardness).</p>
<p>Useful design strategies we will go over are divide and conquer, greedy algorithms, dynamic programming, breadth-first and depth-first search, local search, and linear programming.</p>
<p>An example of an algorithm design is the maximum problem: given an array of integers <span class="math">\(A\)</span>, find the maximum integer in <span class="math">\(A\)</span>. A simple solution to this is as follows:</p>
<pre><code>def find_max(array):
    current = array[0]
    for elem in array[1:]:
        if elem &gt; current: current = elem
    return elem</code></pre>
<p>This seems to be obviously correct, but we can also use program verification to prove it correct. In this case, we will use induction to prove the loop invariant - at the end of each iteration of the loop, <code>current</code> is equal to the largest element encountered so far in the array. We'd check the base case for arrays of length 1, then prove the inductive hypothesis to formally verify the program.</p>
<p>Clearly, this algorithm is <span class="math">\(\Theta(n)\)</span>, since it loops over the entire array once. Formal time complexity analysis can also be done by noting that the loop body takes <span class="math">\(\Theta(1)\)</span> time, and the loop runs <span class="math">\(\Theta(n)\)</span> times.</p>
<p>We also know that this algorithm is asymptotically optimal, at least in terms of the number of comparisons done. Since a correct solution to the Maximum problem must look at every element in the array, it must therefore do at least <span class="math">\(n - 1\)</span> comparisons. Since each comparison takes <span class="math">\(\Theta(1)\)</span> time, the best possible algorithm must take <span class="math">\(\Omega(n)\)</span> time. Let's formally prove this:</p>
<blockquote>
<p>Suppose there was an algorithm that could determine the maximum element of an array using fewer than <span class="math">\(n - 1\)</span> comparisons.<br />Let <span class="math">\(G\)</span> be a graph such that each vertex in <span class="math">\(G\)</span> corresponds to an element in <span class="math">\(A\)</span>, and each comparison done by a run of the algorithm between any two elements results in an edge between those two elements.<br />Clearly, there are <span class="math">\(n - 2\)</span> edges or less, and <span class="math">\(n\)</span> vertices.<br />Therefore, there are at least 2 components in <span class="math">\(G\)</span>, since the graph cannot be connected.<br />Clearly, the solution could be in any of the components, and can only be found by comparing them.<br />Therefore, the algorithm cannot exist.</p>
</blockquote>
<p>An algorithm to find the minimum and the maximum element of an array can also trivially be designed, using <span class="math">\(2n - 2\)</span> comparisons and <span class="math">\(\Theta(n)\)</span> time. However, it is possible to design another algorithm that does fewer than <span class="math">\(2n - 2\)</span> comparisons (though the time complexity might be worse).</p>
<p>One way to do this is to consider elements two at a time - we compare elements one pair at a time, the larger of the pair with the maximum, and the smaller of which with the minimum. That means we need 3 comparisons per pair, or <span class="math">\(\ceil{\frac 3 2 n} - 2\)</span> in total, a ~25% improvement:</p>
<pre><code>def find_min_max(array):
    if array[0] &lt; array[1]: min, max = array[0], array[1]
    else: max, min = array[0], array[1
    for i in range(2, len(array), 2):
        if array[i] &lt; array[i + 1]: small, big = array[i], array[i + 1]
        else: big, small = array[i], array[i + 1]
        if big &gt; max: max = big
        if small &lt; min: min = small
    if len(array) % 2:
        if array[-1] &gt; max: max = array[-1]
        if array[-1] &lt; min: min = array[-1]</code></pre>
<p>It can actually be proven, using graph theory, that the number of comparisons for this algorithm is optimal. However, it's too lengthy and complicated to cover here.</p>
<p>Suppose we want to determine whether and which three elements of an array of integers sum to 0. This is pretty easy to do in <span class="math">\(O(n^3)\)</span> (specifically, <span class="math">\(n \choose 3\)</span> runs of the inner loop) simply by checking all possible triples in the array. Basically, we pick two elements <span class="math">\(A[i]\)</span> and <span class="math">\(A[j]\)</span>, then try to search for a <span class="math">\(A[k] = -(A[i] + A[j])\)</span> using linear search. However, it is actually possible to do so in <span class="math">\(O(n^2 \log n)\)</span> by sorting the array first, and searching for <span class="math">\(k\)</span> using binary search instead.</p>
<p>In fact, it's possible to do even better by sorting the array, taking each <span class="math">\(A[i]\)</span>, and searching from both ends of the array inward for <span class="math">\(j\)</span> and <span class="math">\(k\)</span> such that <span class="math">\(A[j] + A[k] = -A[i]\)</span>. After each step in the search, we either move inward from the left if <span class="math">\(A[j] + A[k] &lt; -A[i]\)</span>, or inward from the right if <span class="math">\(A[j] + A[k] &gt; -A[i]\)</span></p>
<h1 id="section-1">16/9/15</h1>
<p>The pseudocode for the above algorithm looks like the following:</p>
<pre><code>def better_3sum(array):
    array = sorted(array)
    result = []
    for i in range(n - 2):
        j, k = i + 1, n
        while j &lt; k:
            triple_sum = array[i] + array[j] + array[k]
            if triple_sum &lt; 0: j += 1
            elif triple_sum &gt; 0: k -= 1
            else:
                result.append((i, j, k))
                j += 1
                k -= 1</code></pre>
<p>Basically, this goes through each value of <span class="math">\(i\)</span>, and does an <span class="math">\(O(n)\)</span> search for two values that would make it possible to have all three sum to 0. Proving this correct is left as an exercise to the reader - prove that <span class="math">\(k - j\)</span> is monotonically decreasing, and the pairs cover all possible pairs that can possibly sum up to <span class="math">\(A[i]\)</span>. It's somewhat reminiscent of the array merge in merge sort.</p>
<p>This algorithm is <span class="math">\(O(n^2)\)</span>, since the search is <span class="math">\(O(n^2)\)</span> and the sort is <span class="math">\(O(n \log n)\)</span>. There are actually even better algorithms, and the best currently known ones are <span class="math">\(O\left(n^2 \left(\frac{\log \log n}{\log n}\right)^2\right)\)</span>.</p>
<p>A <strong>problem</strong> is a computational task. A <strong>problem instance</strong> is the input for the computational task. The <strong>problem solution</strong> is the output. The <strong>size of a problem instance</strong> is a positive integer that is a measure of the size of the instance, and depends on the problem itself. The size is usually fairly intuitive, such as the size of an array for array sum, but sometimes it gets a bit more tricky.</p>
<p>An <strong>algorithm</strong> is a high level description of a computation. An algorithm <strong>solves</strong> a problem if it finds a valid solution for any instance in finite time. A <strong>program</strong> is an implementation of an algorithm using a computer language.</p>
<h2 id="time-complexity-analysis">Time Complexity Analysis</h2>
<p>The running time of a program <span class="math">\(M\)</span> for a problem instance <span class="math">\(I\)</span> is <span class="math">\(T_M(I)\)</span>. The worst case running time for problem instances of size <span class="math">\(n\)</span> for the program is <span class="math">\(T_M(n)\)</span>. The average case running time is <span class="math">\(T_M^{avg}(n)\)</span>.</p>
<p>The <strong>worst case time complexity</strong> of an algorithm <span class="math">\(A\)</span> is <span class="math">\(f(n)\)</span> such that a program <span class="math">\(M\)</span> exists implementing <span class="math">\(A\)</span> such that <span class="math">\(T_M(n) \in \Theta(f(n))\)</span>.</p>
<p>Running time can only be found by running the program on a computer. Complexity is independent, but we lose information such as the constant factors.</p>
<p>To get the time complexity of an algorithm, we can either use <span class="math">\(\Theta(f(n))\)</span> throughout our analysis, or prove <span class="math">\(O(f(n))\)</span> and then <span class="math">\(\Omega(f(n))\)</span>, which is often easier due to being able to make more assumptions for each separate proof.</p>
<p>The complexity of a loop in code is sum of the complexity of its body over each iteration of the loop. The complexity of consecutive operations is the sum of the complexities of the operations they are composed of.</p>
<p>Prove that <span class="math">\((\ln n)^a \in o(n^b)\)</span> for any <span class="math">\(a\)</span> and <span class="math">\(b\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(L = \lim_{n \to \infty} \frac{(\ln n)^a}{n^b} \lH \lim_{n \to \infty} \frac{a (\ln n)^{a - 1}}{b n^b} \lH \ldots \lH \lim_{n \to \infty} \frac{a!}{b^a n^b} = 0\)</span>.<br />By the limit order rule, <span class="math">\((\ln n)^a \in o(n^b)\)</span>.</p>
</blockquote>
<h1 id="section-2">21/9/15</h1>
<p>Useful to know:</p>
<ul>
<li><span class="math">\(\sum_{i = 0}^{n - 1} (a + di) = na + \frac{dn(n - 1)} 2\)</span> - arithmetic sequence</li>
<li><span class="math">\(\sum_{i = 0}^{n - 1} ar^i = \begin{cases} a\frac{r^n - 1}{r - 1} &amp;\text{if } r \ne 1 \\ na &amp;\text{if } r = 1 \\ \end{cases}\)</span> - geometric sequence</li>
<li><span class="math">\(\sum_{i = 0}^{n - 1} (a + di)r^i = \frac a {1 - r} - \frac{(a + (n - 1)d)r^n}{1 - r} + \frac{dr(1 - r^{n - 1})}{(1 - r)^2}\)</span> -arithmetic-geometric sequence</li>
<li><span class="math">\(\sum_{i = 1}^n \frac 1 {i^2} = \frac{\pi^2} 6 = \Theta(1)\)</span></li>
<li><span class="math">\(_n = \sum_{i = 1}^n \frac 1 i \in \Theta(\log n)\)</span> - harmonic sequence</li>
</ul>
<p>The order of <span class="math">\(n!\)</span> is <span class="math">\(\Theta(n^2 \sqrt n e^{-n})\)</span>. This is similar to Striling's approximation, <span class="math">\(\sqrt{2 \pi n} n^n e^{-n}\)</span> (this gets more and more accurate for larger values).</p>
<p>For order notation, the base of logarithmic terms don't matter because <span class="math">\(\log n = \frac 1 {\log_b 10} \log_b n\)</span>, and <span class="math">\(\frac 1 {\log_b 10}\)</span> is a constant factor.</p>
<p><strong>Polynomial growth rates</strong> are those in <span class="math">\(O(n^k), k \in \mb{R}\)</span>. For example, the best known algorithm for graph isomorphism is <span class="math">\(n^{\sqrt n \log_2 n}\)</span>, so it is not polynomial.</p>
<h1 id="section-3">23/9/15</h1>
<p>Consider the following program:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> useless(n):
    <span class="kw">for</span> i in <span class="dt">range</span>(n):
        <span class="kw">while</span> j &gt;= <span class="dv">1</span>:
            j /= <span class="dv">2</span></code></pre>
<p>Clearly, each iteration of the inner loop will run <span class="math">\(\Theta(\log n)\)</span> times, so the overall time complexity is <span class="math">\(\sum_{i = 1}^n \Theta(\log i) = \Theta(\sum_{i = 1}^n \log i) = \Theta(\log(\prod_{i = 1}^n i)) = \Theta(\log(n!)) = \Theta(n \log n)\)</span>.</p>
<p>A <strong>recurrence relation</strong> specifies <span class="math">\(a_n\)</span> in the infinite sequence of real numbers <span class="math">\(a_1, a_2, a_3, \ldots\)</span> in terms of the previous terms <span class="math">\(a_1, \ldots, a_{n - 1}\)</span> (<span class="math">\(a_1\)</span> must therefore be a constant - the <strong>initial value</strong>). A <strong>solution</strong> to a recurrence relation is a closed form formula for <span class="math">\(a_n\)</span> - one that does not depend on the previous values of <span class="math">\(a_1, \ldots, a_{n - 1}\)</span>. These are generally solved using techniques like guess and check or recursion trees, but there aren't any methods that are guaranteed to solve all recurrences, and in fact, reurrences may not necessarily even have solutions.</p>
<p>Recurrence relations can also be written using function notation, like <span class="math">\(T(1) = 2, T(n) = T(n - 1) + 1\)</span>.</p>
<p>The <strong>guess and check</strong> method involves computing enough <span class="math">\(a_n\)</span> instances to guess the form of the solution (without any constants). Then, we solve for the constants using the computed values, and check if our solution is correct - if it is, we must prove it is correct using induction, and if not, we start over with a different guess.</p>
<p>Solve <span class="math">\(a_0 = 4, a_n = a_{n - 1} + 6n - 5\)</span>:</p>
<blockquote>
<p>Guess and check: the first few elements of the sequence are <span class="math">\(a_0 = 4, a_1 = 5, a_2 = 12, a_3 = 25, a_4 = 44\)</span>.<br />This seems to be quadratic growth, so we guess <span class="math">\(an^2 + bn + c, a, b, c \in \mb{R}\)</span> as the solution.<br />Solving for <span class="math">\(a, b, c\)</span>, <span class="math">\(c = 4, a + b + c = 5, 4a + 2b + c = 12\)</span>, we get <span class="math">\(a = 3, b = -2, c = 4\)</span>.<br />So we guess that <span class="math">\(a_n = 3n^2 - 2n + 4\)</span>. We can verify this for <span class="math">\(0 \le n \le 4\)</span> as our inductive base case.<br />Assume for some <span class="math">\(k \in \mb{N}\)</span> that <span class="math">\(a_k = 3k^2 - 2k + 4\)</span>.<br />Clearly, <span class="math">\(a_{k + 1} = 3k^2 - 2k + 4 + 6(k + 1) - 5 = 3(k + 1)^2 - 2(k + 1) + 4\)</span>.<br />So by induction, <span class="math">\(a_n = 3n^2 - 2n + 4\)</span>.</p>
</blockquote>
<p>The <strong>recurrence tree</strong> method is often used for divide-and-conquer algorithms. This technique involves building a call tree for the recurrence. then summing up the results of the function on each level of the tree.</p>
<p>To construct a recurrence tree, we start with the timing function <span class="math">\(T(n)\)</span>, then add children for each recursive call. The recursive calls are then removed from the parent. This is repeated for the children, all the way until we reach the base case. Note that after each step of growing the tree, the sum of all the nodes are always equivalent to <span class="math">\(T(n)\)</span>.</p>
<p>Then, we sum the nodes at each level in the tree. The sum of all these sums is equal to <span class="math">\(T(n)\)</span>.</p>
<p>Use the recurrence tree method to find the running time of merge sort:</p>
<blockquote>
<p>Clearly, <span class="math">\(T(n) = \begin{cases} 2T\left(\frac n 2\right) + cn &amp;\text{if } n &gt; 1 \wedge n \text{ is a power of 2} \\ d &amp;\text{if } n = 1 \\ \end{cases}\)</span> where <span class="math">\(c, d\)</span> are constants.<br />Construct a recursion tree: start with the root node <span class="math">\(N\)</span> with value <span class="math">\(T(n)\)</span>, then add 2 children, <span class="math">\(N_1, N_2\)</span>, both <span class="math">\(T\left(\frac n 2\right)\)</span>, and replace <span class="math">\(N\)</span>'s value with <span class="math">\(cn\)</span>.<br />What we get is a binary tree with every internal node having 2 children, and value <span class="math">\(c\frac n {2^i}\)</span> (where <span class="math">\(i\)</span> is the level in the tree, where 0 is the root level), and each leaf node has value <span class="math">\(d\)</span>.<br />Let <span class="math">\(n = 2^j\)</span>. Clearly, the height of the tree must then be <span class="math">\(j + 1\)</span>, with <span class="math">\(j\)</span> interior levels. Clearly, at the bottom level there are <span class="math">\(2^j\)</span> leaf nodes, since each level <span class="math">\(i\)</span> has <span class="math">\(2^{j - i}\)</span> nodes. The sum of the bottom level is therefore <span class="math">\(2^j d = dn\)</span>.<br />Clearly, at each internal level <span class="math">\(i\)</span> each node has value <span class="math">\(c\frac n {2^i}\)</span>. Since there are <span class="math">\(2^i\)</span> nodes, the sum of each interior level is <span class="math">\(c2^i\frac n {2^i} = cn\)</span>.<br />So <span class="math">\(T(n) = dn + jcn = dn + cn \log_2 n = \Theta(n \log n)\)</span>.</p>
</blockquote>
<p>The <strong>master theorem</strong> is a generalized formula for solving certain forms of recurrences. One thing it says is that given <span class="math">\(T(n) = aT(\frac n b) + \Theta(n^y)\)</span> where <span class="math">\(a \ge 1, b &gt; 1\)</span>, and <span class="math">\(n\)</span> is a power of <span class="math">\(b\)</span>, then <span class="math">\(T(n) \in \begin{cases} \Theta(n^x) &amp;\text{if } y &lt; x \\ \Theta(n^x \log n) &amp;\text{if } y = x \\ \Theta(n^y) &amp;\text{if } y &gt; x \\ \end{cases}\)</span> where <span class="math">\(x = \log_b a\)</span>.</p>
<p>Also, the exact value is <span class="math">\(T(n) = da^j + cn^y \sum_{i = 0}^{j - 1} \left(\frac a {b^y}\right)^i\)</span>, or more simply <span class="math">\(T(n) = dn^x + cn^y \sum_{i = 0}^{j - 1} r^i\)</span> where <span class="math">\(x = \log_b a\)</span> and <span class="math">\(r = \frac a {b^y} = b^{x - y}\)</span>.</p>
<p>This can be proved by constructing tree, figuring out the geometric sequence for the tree sums, and then solving for its value for each of the three cases.</p>
<p>A more general version, which will not be proved here, is that given <span class="math">\(T(n) = aT(\frac n b) + f(n)\)</span> where <span class="math">\(a \ge 1, b &gt; 1\)</span>, and <span class="math">\(n\)</span> is a power of <span class="math">\(b\)</span>, then <span class="math">\(T(n) \in \begin{cases} \Theta(n^x) &amp;\text{if } f(n) \in O(n^{x - \epsilon}) \text{ for some } \epsilon &gt; 0 \\ \Theta(n^x \log n) &amp;\text{if } f(n) \in \Theta(n^x) \\ \Theta(f(n)) &amp;\text{if } \frac{f(n)}{n^{x + \epsilon}} \text{ is an increasing function of } n \text{ for some } \epsilon &gt; 0 \text{ after } n \text{ is greater than some } n_0 \\ \end{cases}\)</span> where <span class="math">\(x = \log_b a\)</span>.</p>
<h1 id="section-4">28/9/15</h1>
<p>Solve <span class="math">\(T(1) = 1, T(n) = 3T(\frac n 4) + n \log n\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(a = 3, b = 4, f(n) = n \log n\)</span>, so <span class="math">\(x = \log_4 3\)</span> and <span class="math">\(f(n) / n^{x + \epsilon}\)</span> is an increasing function of <span class="math">\(n\)</span> for <span class="math">\(\epsilon = 1 - \log_4 3\)</span> since <span class="math">\(f(n) / n^{x + \epsilon} = \log n\)</span>.<br />So <span class="math">\(T(n) \in \Theta(n \log n)\)</span>.</p>
</blockquote>
<p>The main task when using the master theorem to prove time complexities is choosing the right <span class="math">\(\epsilon\)</span> to make one of the cases valid. For example, the master theorem cannot be used for something like <span class="math">\(T(1) = 1, T(n) = 2T(\frac n 2) + n \log n\)</span>, since none of the cases apply. Instead, we can use the recurrence tree method to solve the recurrence directly:</p>
<blockquote>
<p>Each level <span class="math">\(i\)</span> (from the bottom) of the tree has <span class="math">\(2^i\)</span> nodes, each with value <span class="math">\(2^{j - i}(j - i)\)</span> where <span class="math">\(n = 2^j\)</span>.<br />So at each level, the sum is <span class="math">\(2^i 2^{j - i} (j - i) = (j - i) 2^j\)</span>.<br />So the sum of the tree is <span class="math">\(\sum_{i = 0}^j (j - i) 2^j = j^2 2^j - 2^j \sum_{i = 0}^j i = 2^j\left(j^2 - \frac{j(j + 1)}{2}\right) = \Theta(n \log^2 n)\)</span>.</p>
</blockquote>
<h2 id="divide-and-conquer">Divide and Conquer</h2>
<p>Divide and conquer is a design strategy for algorithms where we divide a problem into smaller subproblems, solve those individually, then combine them back together to get a good result. Formally:</p>
<ul>
<li>Divide the problem instance <span class="math">\(I\)</span> into smaller subproblems <span class="math">\(I_1, \ldots, I_n\)</span>.</li>
<li>Solve <span class="math">\(I_1, \ldots, I_n\)</span> recursively to get sollutions <span class="math">\(S_1, \ldots, S_n\)</span>.</li>
<li>Use <span class="math">\(S_1, \ldots, S_n\)</span> to compute <span class="math">\(S\)</span> - combine the results. This is usually the hardest step.</li>
</ul>
<p>For example, mergesort splits an array into two subarrays, and then recursively sorts those. Then, the merge is performed to combine the two sorted subarrays.</p>
<p>The time complexity of this is <span class="math">\(T(n) = \begin{cases} T(\ceil{\frac n 2}) + T(\floor{\frac n 2}) + cn &amp;\text{if } n &gt; 1 \\ d &amp;\text{if } n = 1 \\ \end{cases}\)</span> for some constants <span class="math">\(c, d\)</span>. This is called the <strong>exact recurrence</strong>. If we remove the ceilings and floors, we get a <strong>sloppy recurrence</strong> <span class="math">\(T(n) = \begin{cases} 2T(\frac n 2) + cn &amp;\text{if } n &gt; 1 \\ d &amp;\text{if } n = 1 \\ \end{cases}\)</span>, which only works the same when <span class="math">\(n\)</span> is a power of 2, but is a lot simpler to work with.</p>
<p>The master theorem can prove that mergesort is <span class="math">\(\Theta(n \log n)\)</span> for the sloppy recurrence, and therefore only for when <span class="math">\(n\)</span> is a pwoer of 2. However, if we want to get the time complexity for all <span class="math">\(n\)</span>, we need to use induction on the exact recurrence.</p>
<h1 id="section-5">30/9/15</h1>
<h3 id="non-dominated-points">Non-dominated points</h3>
<p>Suppose we have a set of 2D coordinates/points. One point <strong>dominates</strong> another if it has a greater or equal X and Y coordinate than the other. We want to find those points that are non-dominated - the points that are not dominated by any other point. The non-dominated points form a sort of boundary below which all the dominated points lie.</p>
<p>Clearly, this can easily be solved simply by checking every pair of points to see which dominate which, but this would take <span class="math">\(\Theta(n^2)\)</span> time.</p>
<p>A better solution is to sort the points by their X coordinate, and then apply divide and conquer.</p>
<p>Dividing larger sets of points <span class="math">\(S\)</span> is also easy - we can just split the sorted points into two sets <span class="math">\(S_1, S_2\)</span>, and then recursively solve the problem. It is the combining step that is the difficult step.</p>
<p>Given a point and the highest point on its right, it is easy to determine whether it is dominated. This is our base case</p>
<p>Clearly, no point in <span class="math">\(S_1\)</span> can dominate a point in <span class="math">\(S_2\)</span>, since the X coordinates of points in <span class="math">\(S_1\)</span> is strictly less than points in <span class="math">\(S_2\)</span>. So we find the point in <span class="math">\(S_2\)</span> with the highest Y coordinate, and all points in <span class="math">\(S_1\)</span> with Y coordinate at or below this value are guaranteed to be dominated by at least one point in <span class="math">\(S_2\)</span>. In this way, we can eliminate all the points in <span class="math">\(S_1\)</span> that are dominated by points in <span class="math">\(S_2\)</span> in <span class="math">\(O(n)\)</span> time - this is the combining step.</p>
<p>In code, this looks like:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> non_dominated(points):
    points = <span class="dt">sorted</span>(points, key=<span class="kw">lambda</span> p: p.x)
    <span class="kw">if</span> <span class="dt">len</span>(points) == <span class="dv">1</span>: <span class="kw">return</span> {points[<span class="dv">0</span>]}
    pivot = floor(<span class="dt">len</span>(points) / <span class="dv">2</span>)
    left = non_dominated(points[:pivot])
    right = non_dominated(points[pivot + <span class="dv">1</span>:])
    i = <span class="dv">0</span>
    <span class="kw">while</span> i &lt; <span class="dt">len</span>(left) and left[i].y &gt; right[<span class="dv">1</span>].y:
        i += <span class="dv">1</span>
    <span class="kw">return</span> left[:i] + right</code></pre>
<p>Clearly, the time complexity of this algorithm is <span class="math">\(T(n) = T\left(\floor{\frac n 2}\right) + T\left(\ceil{\frac n 2}\right) = O(n \log n)\)</span> (the sloppy version of this can be proven using the master theorem).</p>
<p>Without divide and conquer, we could also just sort the list, and then iterate though it from right to left, keeping track of the largest Y axis coordinate seen so far and eliminating points at or below this Y coordinate:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> non_dominated(points):
    largest = <span class="dt">float</span>(<span class="st">&quot;-inf&quot;</span>)
    <span class="kw">for</span> point in <span class="dt">reversed</span>(<span class="dt">sorted</span>(points, key=<span class="kw">lambda</span> p: p.x)):
        <span class="kw">if</span> point.y &lt;= largest:
            <span class="kw">yield</span> point
        <span class="kw">else</span>:
            largest = point.y</code></pre>
<h3 id="closest-pair">Closest pair</h3>
<p>Suppose we have a set <span class="math">\(Q\)</span> of <span class="math">\(n\)</span> distinct 2D points in Euclidean space. We want to find the two points that are closest together - the points <span class="math">\(u, v\)</span> for which <span class="math">\(\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}\)</span> is minimised.</p>
<p>Clearly, minimising <span class="math">\(\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}\)</span> is the same as minimising <span class="math">\((u_x - v_x)^2 + (u_y - v_y)^2\)</span>. We will again sort the points in <span class="math">\(Q\)</span> by X coordinate, taking <span class="math">\(\Theta(n \log n)\)</span> time.</p>
<p>The dividing step is again trivial to get two partitions <span class="math">\(Q_1, Q_2\)</span>, but the combining step takes some work.</p>
<p>If we are given the distance <span class="math">\(\delta\)</span> between the closest pair of points seen so far in the left and right partitions, and a point <span class="math">\(p\)</span>, we know that if this point is in the closest pair of points, the other point must have X coordinate within <span class="math">\(\delta\)</span> of this point's X coordinate. Therefore, the other point must be in the <strong>critical strip</strong> - a section of the plane from <span class="math">\(p_x - \delta\)</span> to <span class="math">\(p_x + \delta\)</span> and extending infinitely both up and down.</p>
<p>We now want to find the closest pair between the left and right partitions, if it is less than the .</p>
<p>There is also a lemma that says that if a critical strip <span class="math">\(R\)</span> of <span class="math">\(Q\)</span> is sorted by Y coordinate, and <span class="math">\(R[j]\)</span> and <span class="math">\(R[k]\)</span> have distance less than <span class="math">\(\delta\)</span> where <span class="math">\(j &lt; k\)</span>, then <span class="math">\(k \le j + 7\)</span>. Here, <span class="math">\(\delta\)</span> is the smaller of the minimum distance between points in <span class="math">\(Q_1\)</span>, and the minimum distance between points in <span class="math">\(Q_2\)</span>. This can be proven by dividing the strip around the point as squares of side length <span class="math">\(\frac \delta 2\)</span>, and then showing that no two points can be in the same square. In other words, within a critical strip and given <span class="math">\(\delta\)</span>, the point closest to a given point can be determined in 7 comparisons or less.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> closest_pair(points, start, end):
    <span class="kw">if</span> start == end: <span class="kw">return</span> <span class="dt">float</span>(<span class="st">&quot;inf&quot;</span>)
    pivot = floor((left + right) / <span class="dv">2</span>)
    closest_left = closest_pair(points, left, pivot)
    closest_right = closest_pair(points, pivot + <span class="dv">1</span>, right)
    closest = <span class="dt">min</span>(closest_left, closest_right)
    candidates = select_candidates(left, right, closest, points[pivot].x) <span class="co"># find points in the critical strip</span>
    candidates = <span class="dt">sorted</span>(candidates, key=<span class="kw">lambda</span> p: p.y)
    closest = check_candidates(candidates, closest)
    <span class="kw">return</span> closest</code></pre>
<p>The time complexity of this is <span class="math">\(O(n \log n \log n)\)</span>. We can improve this by eliminating the Y coordinate sort of the candidates. One of them is to sort all the points <span class="math">\(Q\)</span> by Y coordinate to begin with, and then use that list when checking candidates, and then adding the necessary checks that make it work. Alternatively, we could replace the Y coordinate sort with a merge, and maintaining the candidates in sorted order.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> closest_pair(points, start, end):
    <span class="kw">if</span> start == end: <span class="kw">return</span> <span class="dt">float</span>(<span class="st">&quot;inf&quot;</span>)
    pivot = floor((left + right) / <span class="dv">2</span>)
    closest_left = closest_pair(points, left, pivot)
    closest_right = closest_pair(points, pivot + <span class="dv">1</span>, right)
    closest = <span class="dt">min</span>(closest_left, closest_right)
    merge_in_place(points, start, pivot, end)
    candidates = select_candidates(left, right, closest, points[pivot].x) <span class="co"># find points in the critical strip</span>
    closest = check_candidates(candidates, closest)
    <span class="kw">return</span> closest</code></pre>
<h1 id="section-6">5/10/15</h1>
<p>The <strong>size</strong> of an integer is the number of bits that we use to represent it, rather than the value of the integer itself - this is basically just <span class="math">\(\ceil{\log_2 n}\)</span>. The <strong>bit complexity</strong> of algorithms operating on integers is therefore the complexity as a function of the sizes of those integers.</p>
<h2 id="multiplication">Multiplication</h2>
<h3 id="integer-multiplication">Integer multiplication</h3>
<p>Suppose we have 2 positive binary integers, <span class="math">\(X = [X_{k - 1}, \ldots, X_0]\)</span> and <span class="math">\(Y = [Y_{k - 1}, \ldots, Y_0]\)</span>. We want to compute the <span class="math">\(2k\)</span>-bit product <span class="math">\(Z = [Z_{2k - 1}, \ldots, Z_0]\)</span>. Naively, we can use the simple binary multiplication algorithm, to shift <span class="math">\(Y\)</span> by each position <span class="math">\(0 \le i &lt; k\)</span> and add those shifted values for which <span class="math">\(X_i = 1\)</span>, in <span class="math">\(O(k^2)\)</span>. Note that the fastest way to add two integers is <span class="math">\(\Theta(k)\)</span>.</p>
<p>We can do better than this by using divide and conquer, by using <span class="math">\(XY = (2^{\frac k 2} X_L + X_R)(2^{\frac k 2} Y_L + Y_R) = 2^k X_L Y_L + 2^{\frac k 2}(X_L Y_R + X_R Y_L) + X_R Y_R\)</span>, then solving for those four multiplications between <span class="math">\(X_L, X_R, Y_L, Y_R\)</span> (multiplying the powers of 2 are simply shifts, so they're basically free):</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> multiply(x, y):
    <span class="kw">if</span> <span class="dt">len</span>(x) == <span class="dv">1</span>: <span class="kw">return</span> x[<span class="dv">0</span>] * y[<span class="dv">0</span>]
    x_left, x_right = x[:<span class="dt">len</span>(x) // <span class="dv">2</span>], x[<span class="dt">len</span>(x) // <span class="dv">2</span>:]
    y_left, y_right = y[:<span class="dt">len</span>(y) // <span class="dv">2</span>], y[<span class="dt">len</span>(y) // <span class="dv">2</span>:]
    z_top = multiply(x_left, y_left)
    z_middle = add(multiply(x_left, x_right), multiply(x_right, y_left))
    z_bottom = multiply(x_right, y_right)
    z = add(
        shift_left(z_top, <span class="dt">len</span>(x)),
        shift_left(z_middle, <span class="dt">len</span>(x) // <span class="dv">2</span>),
        z_bottom
    )
    <span class="kw">return</span> z</code></pre>
<p>Using the master theorem, we can show that this is takes <span class="math">\(\Theta(n^2)\)</span> time - no improvement over the naive algorithm. However, this is a good base for an improved version.</p>
<p>Consider <span class="math">\((X_L + X_R)(Y_L + Y_R) = X_L Y_L + X_L Y_R + X_R Y_L + X_R Y_R\)</span>. So <span class="math">\(X_L Y_R + X_R Y_L = (X_L + X_R)(Y_L + Y_R) - X_L Y_L - X_R Y_R\)</span>. Since we need to calculate <span class="math">\(X_L Y_L\)</span> and <span class="math">\(X_R Y_R\)</span> anyways, we can compute <span class="math">\(X_L Y_R + X_R Y_L\)</span> with just one multiplication by computing <span class="math">\((X_L + X_R)(Y_L + Y_R)\)</span>.</p>
<p>So using this:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> multiply(x, y):
    <span class="kw">if</span> <span class="dt">len</span>(x) == <span class="dv">1</span>: <span class="kw">return</span> x[<span class="dv">0</span>] * y[<span class="dv">0</span>]
    x_left, x_right = x[:<span class="dt">len</span>(x) // <span class="dv">2</span>], x[<span class="dt">len</span>(x) // <span class="dv">2</span>:]
    y_left, y_right = y[:<span class="dt">len</span>(y) // <span class="dv">2</span>], y[<span class="dt">len</span>(y) // <span class="dv">2</span>:]
    z_top = multiply(x_left, y_left)
    z_middle = multiply(add(x_left, x_right), add(y_left, y_right))
    z_bottom = multiply(x_right, y_right)
    z = add(
        shift_left(z_top, <span class="dt">len</span>(x)),
        shift_left(z_middle, <span class="dt">len</span>(x) // <span class="dv">2</span>) - z_top - z_bottom,
        z_bottom
    )
    <span class="kw">return</span> z</code></pre>
<p>Now, we only solve 3 subproblems instead of 4. This gives us a time complexity of around <span class="math">\(\Theta(k^{\log_2 3})\)</span>, or about <span class="math">\(\Theta(k^{1.59})\)</span>.</p>
<p>Even better algorithms exist. One is Toom-Cook, which splits <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> into 3 pieces each, and solves 5 subproblems, all in <span class="math">\(O(k^{\log_3 5})\)</span>. The Stronhage-Strassen method takes <span class="math">\(O(k \log \log \log k)\)</span> time, using Fourier transforms. The Furer method is the most advanced as of the present, using <span class="math">\(O(k \log k 2^{3 \log^* k})\)</span> time where <span class="math">\(\log^* k\)</span> is the inverse Ackermann function.</p>
<h3 id="matrix-multiplication">Matrix multiplication</h3>
<p>We will now assume that integer multiplication can be done in constant time, to avoid having to consider bit complexity.</p>
<p>Suppose we have two <span class="math">\(n\)</span> by <span class="math">\(n\)</span> matrices <span class="math">\(A, B\)</span>. Clearly, the naive algorithm for multiplying them is <span class="math">\(\Theta(n^3)\)</span>, since we just take the dot product of the corresponding row of <span class="math">\(A\)</span> and the corresponding column of <span class="math">\(B\)</span> - the dot product takes <span class="math">\(\Theta(n)\)</span>, and there are <span class="math">\(\Theta(n^2)\)</span> cells in the result.</p>
<p>We can use the same technique as for integer multiplcation - we break each <span class="math">\(n\)</span> by <span class="math">\(n\)</span> matrix into 4 <span class="math">\(\frac n 2\)</span> by <span class="math">\(\frac n 2\)</span> matrices, multiply those, then combine those together, then again try to remove subproblem calculations where possible.</p>
<p>If we split <span class="math">\(A, B\)</span> into 4 blocks <span class="math">\(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}, B = \begin{bmatrix} e &amp; f \\ g &amp; h \end{bmatrix}\)</span>, then <span class="math">\(AB = \begin{bmatrix} ae + bg &amp; af + bh \\ ce + dg &amp; cf + dh \end{bmatrix}\)</span>. If we use divide and conquer to solve these 8 multiplications in the block version, it still takes <span class="math">\(\Theta(n^3)\)</span>. However, the problem is now in a form that is useful for optimizing, by reducing the number of sub-problems.</p>
<p>Strassen's method notes that if <span class="math">\(P_1 = a(f - h), P_2 = (a + b)h, P_3 = (c + d)e, P_4 = d(g - e), P_5 = (a + d)(e + h), P_6 = (b - d)(g + h), P_7 = (a - c)(e - f)\)</span>, then <span class="math">\(ae + bg = P_5 + P_4 - P_2 + P_6, af + bh = P_1 + P_2, ce + dg = P_3 + P_4, cf + dh = P_5 + P_1 - P_3 - P_7\)</span>.</p>
<p>So <span class="math">\(AB = \begin{bmatrix} P_5 + P_4 - P_2 + P_6 &amp; P_1 + P_2 \\ P_3 + P_4 &amp; P_5 + P_1 - P_3 - P_7 \end{bmatrix}\)</span>, and since there are only 7 multiplications needed to calculate <span class="math">\(P_1, \ldots, P_7\)</span>, we only have 7 sub-problems to solve. As a result, the algorithm is <span class="math">\(\Theta(n^{\log_2 7})\)</span> or around <span class="math">\(\Theta(n^{2.81})\)</span>, since the recurrence is <span class="math">\(T(n) = 7T\left(\frac n 2\right) + \Theta(n^2)\)</span> (solve this using the master theorem).</p>
<p>Research into this problem has resulted into better algorithms. The current state of the art is the Coppersmith-Winograd algorithm, taking around <span class="math">\(O(n^{2.373})\)</span> time.</p>
<h3 id="selection">Selection</h3>
<p>Suppose we have an array <span class="math">\(A\)</span> of size <span class="math">\(n\)</span>, and we want to find the <span class="math">\(k\)</span>th smallest integer, where the smallest element is found when <span class="math">\(k = 1\)</span>. The median algorithm is a special case of this where <span class="math">\(k = \ceil{\frac n 2}\)</span>.</p>
<p>Naively, we could just sort the array and pick the <span class="math">\(k\)</span>th element, all in <span class="math">\(\Theta(n \log n)\)</span> time. Another naive approach is to use a modified selection sort (repeatedly finding the smallest element in the remaining part of the array, and putting it at the end of the sorted array), where we stop after <span class="math">\(k\)</span> moves, all in <span class="math">\(\Theta(kn)\)</span> time - this is linear if <span class="math">\(k\)</span> is constant, but quadratic if it is not. Combining the two, we can use a modified heapsort, which can solve the problem in <span class="math">\(\Theta(n + k \log n)\)</span> time.</p>
<h1 id="section-7">7/10/15</h1>
<p>There's a make-up lecture on November 21st or so.</p>
<p>Using divide and conquer, we might do something similar to quicksort, called quickselect - we pick a pivot, partition the elements, and then recursively quickselect within the partition that contains the desired element:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> quickselect(array, k):
    pivot = array[<span class="dv">0</span>]
    partition1, partition2 = [x <span class="kw">for</span> x in array <span class="kw">if</span> x &lt; pivot], [x <span class="kw">for</span> x in array <span class="kw">if</span> x &gt; pivot]
    <span class="kw">if</span> k - <span class="dv">1</span> == <span class="dt">len</span>(partition1):
        <span class="kw">return</span> pivot
    <span class="kw">elif</span> k - <span class="dv">1</span> &lt; <span class="dt">len</span>(partition1):
        <span class="kw">return</span> quickselect(partition1, k)
    <span class="kw">else</span>:
        <span class="kw">return</span> quickselect(partition2, k - <span class="dt">len</span>(partition1) - <span class="dv">1</span>)</code></pre>
<p>Note that unlike quicksort, there's only one recursive call in each invocation. On average, the pivot will be good about 50% of the time, where a good pivot is one that is within the 25th and 75th percentile inclusive. So on average, we would expect that every two calls, we would get a good pivot. So on average, every other call would have partitions that are of size at most 75% of the array's size. Therefore, on average the size of the subproblem would be reduced by 25% on every other call, which makes this function <span class="math">\(\Theta(n)\)</span> average case.</p>
<p>Note that in the worst case, the pivot would be at one of the extremes of the percentiles, and so the subproblem would only be reduced by a constant amount, and there are <span class="math">\(n\)</span> calls. Therefore, the function is <span class="math">\(\Theta(n^2)\)</span> worst case.</p>
<p>However, if we could always choose a good pivot, then we can ensure that the function is <span class="math">\(\Theta(n)\)</span> worst case as well. Doing so is a bit more clever, but it is possible, using the <strong>median of medians</strong> algorithm. First, we assume that there are at least 15 elements, so <span class="math">\(n = 10r + 5 + \theta\)</span>, where <span class="math">\(r \ge 1, 0 \le \theta \le 9\)</span>.</p>
<p>We now split <span class="math">\(A\)</span> into subarrays of exactly 5 elements: <span class="math">\(B_1, \ldots, B_{2r + 1}\)</span> (we ignore the last <span class="math">\(\theta\)</span> elements entirely here), and then find the median of each <span class="math">\(B_i\)</span>, as <span class="math">\(m_i\)</span>. Then, we use a selection algorithm to find the median of this array of medians.</p>
<p>We claim that <span class="math">\(y\)</span> is always a good pivot - that is is guaranteed to be between the 25th and 75th percentile inclusive. We will prove this later, but first, we will integrate it into quickselect:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> median_of_medians_quickselect(array, k):
    <span class="co"># generate a good pivot</span>
    <span class="kw">if</span> <span class="dt">len</span>(array) &lt; <span class="dv">15</span>: <span class="kw">return</span> <span class="dt">sorted</span>(array)[k]
    write $n = 10r + <span class="dv">5</span> + \theta$
    construct $B_1, \ldots, B_{2r + <span class="dv">1</span>}$
    find medians $M = [m_1, \ldots, m_{2r + <span class="dv">1</span>}]$
    pivot = median_of_medians_quickselect(M, r + <span class="dv">1</span>)
    
    <span class="co"># do the rest of the steps from quickselect</span>
    partition1, partition2 = [x <span class="kw">for</span> x in array <span class="kw">if</span> x &lt; pivot], [x <span class="kw">for</span> x in array <span class="kw">if</span> x &gt; pivot]
    <span class="kw">if</span> k - <span class="dv">1</span> == <span class="dt">len</span>(partition1):
        <span class="kw">return</span> pivot
    <span class="kw">elif</span> k - <span class="dv">1</span> &lt; <span class="dt">len</span>(partition1):
        <span class="kw">return</span> quickselect(partition1, k)
    <span class="kw">else</span>:
        <span class="kw">return</span> quickselect(partition2, k - <span class="dt">len</span>(partition1) - <span class="dv">1</span>)</code></pre>
<p>We have two recursive calls now, since the median of medians algorithm also required us to do a selection to find <code>pivot</code> (<span class="math">\(y\)</span>).</p>
<p>There are <span class="math">\(r + 1\)</span> medians at or below <span class="math">\(y\)</span> in <span class="math">\(m_1, \ldots, m_{2r + 1}\)</span> - let these be represented <span class="math">\(m_{i_1}, \ldots, m_{i_{r + 1}}\)</span>. Clearly, in each <span class="math">\(B_{i_1}, \ldots, B_{i_{r + 1}}\)</span>, there are at least 3 out of the 5 elements that are less than or equal to <span class="math">\(y\)</span> (since they are less than or equal to the median). Therefore, there must be at least <span class="math">\(3(r + 1)\)</span> elements in total that are less than or equal to <span class="math">\(y\)</span>. Since there are <span class="math">\(10r + 5\)</span> elements (since we ignored <span class="math">\(\theta\)</span>), the ratio of elements below <span class="math">\(y\)</span> to the total number of elements is <span class="math">\(\frac{3(r + 1)}{10r + 5}\)</span>, or basically <span class="math">\(\frac 3 {10}\)</span>. Therefore, 30% or more of the elements must be less than or equal to <span class="math">\(y\)</span>.</p>
<p>Using a similar argument, but with the elements at or above <span class="math">\(y\)</span> rather than at or below, we can prove that 30% or more of the elements must be greater or equal to <span class="math">\(y\)</span>. Therefore, <span class="math">\(y\)</span> is within the 30th to 70th percentile inclusive.</p>
<p>Therefore, the sloppy recurrence is <span class="math">\(T(n) \le T(\frac n 5) + T(\frac{7n}{10}) + \Theta(n)\)</span>, which we can solve using the recurrence tree method - each node has two children, which are the node's value times <span class="math">\(\frac 1 5\)</span> and <span class="math">\(\frac 7 {10}\)</span>, respectively, and the root node's value is <span class="math">\(n\)</span>. When we draw the tree, we see that each level <span class="math">\(i\)</span> from the root (level 0) has sum <span class="math">\(\left(\frac 9 {10}\right)^in\)</span>, and the sum of all levels to infinity converges to <span class="math">\(10n\)</span>, so <span class="math">\(T(n) \in \Theta(n)\)</span>.</p>
<p>The exact recurrence for this is <span class="math">\(T(n) \le T(\floor{\frac n 5}) + T(\floor{\frac{7n + 12}{10}}) + \Theta(n)\)</span>. This accounts for the rounding down in the partitioning, and accounts for the last <span class="math">\(\theta\)</span> values we were ignoring earlier.</p>
<h2 id="greedy-algorithms">Greedy Algorithms</h2>
<p>Greedy algorithms are often used for <strong>optimization problems</strong>, which are problems where the goal is to find a solution that both satisfies the <strong>problem constraints</strong>, and maximizes/minimizes an <strong>objective/profit/cost function</strong>.</p>
<p><strong>Feasible solutions</strong> are those that satisfy the problem constraints, and given a problem instance <span class="math">\(I\)</span>, are represented as <span class="math">\(\mathrm{feasible}(I)\)</span>. The objective function is in <span class="math">\(\mathrm{feasible}(I) \to \mb{R}\)</span>.</p>
<p>For greedy algorithms, it is generally the proof of correctness that is the hard part. Greedy algorithms build up partial solutions, making locally optimal decisions at each step, with the goal of getting a globally optimal result.</p>
<p>A feasible solution should be writeable as an <span class="math">\(n\)</span>-tuple <span class="math">\(\tup{x_1, \ldots, x_n}\)</span>. A <strong>partial solution</strong> is an <span class="math">\(i\)</span>-tuple <span class="math">\(\tup{x_1, \ldots, x_i}\)</span> where <span class="math">\(i &lt; n\)</span> and none of the problem constraints are violated (note that this is not necessarily a feasible solution). A partial solution isn't always extendible to a feasible solution - some partial solutions are dead ends, and greedy algorithms can't get an optimal solution here.</p>
<p>A <strong>choice set</strong> for a partial solution <span class="math">\(\tup{x_1, \ldots, x_i}\)</span> is the set <span class="math">\(\mathcal{X}\)</span> of all elements we can append to <span class="math">\(\tup{x_1, \ldots, x_i}\)</span> to make another, longer partial or feasible solution <span class="math">\(\tup{x_1, \ldots, x_i, y}\)</span> - the set of all possible <span class="math">\(y\)</span>.</p>
<h1 id="section-8">14/10/15</h1>
<p>For any <span class="math">\(y\)</span> in the choice set <span class="math">\(\mathcal{X}\)</span>, we have a <strong>local evaluation criterion</strong> <span class="math">\(g(y)\)</span>, that measures the profit/cost of including <span class="math">\(y\)</span> in the current partial solution. <strong>Extension</strong> of a partial solution is the process of adding the <span class="math">\(y\)</span> with the highest <span class="math">\(g(y)\)</span> value to the partial solution. A greedy algorithm, in general, is repeated extension until we have a feasible solution <span class="math">\(X\)</span>.</p>
<p>Greedy algorithms do no backtracking or lookahead - they only consider the elements of the choice set at each step. That means they are often faster than algorithms that do backtracking, but the solution is possibly not as good. When implementing greedy algorithms, it is often efficient to do a preprocessing step to compute the local evaluation criterion, like sorting the elements of an array.</p>
<p>For some greedy algorithms, it is possible to always obtain an optimal solution. However, these proofs are often rather difficult.</p>
<p>Suppose we have a set <span class="math">\(A\)</span> of pairs representing intervals. What is the largest subset <span class="math">\(B\)</span> of these pairs such that none of the intervals intersect?</p>
<p>One greedy strategy might be to repeatedly choose the interval with the earliest start time that doesn't intersect with any intervals in the current partial solution. Another might be to choose the interval with the smallest duration that doesn't intersect. A third is to choose the interval with the earliest end time that doesn't intersect.</p>
<p>Clearly, the first one is not valid because we can make a counterexample: an interval that starts early but is really long is chosen over multiple non-intersecting short intervals that start a bit later, which means this strategy doesn't always give the optimal solution.</p>
<p>It is also possible to find a counterexample for the second strategy - a short interval that intersects with two non-intersecting longer intervals is chosen over those two, which means this strategy also doesn't always give the optimal solution.</p>
<p>The last strategy is correct. We can implement it as follows:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> select_interval(intervals):
    intervals = <span class="dt">sorted</span>(intervals, key=<span class="kw">lambda</span> interval: interval[<span class="dv">1</span>])
    solution = [intervals[<span class="dv">0</span>]]
    latest_finish = intervals[<span class="dv">0</span>][<span class="dv">1</span>]
    <span class="kw">for</span> interval in intervals[<span class="dv">1</span>:]:
        <span class="kw">if</span> interval[<span class="dv">0</span>] &gt;= latest_finish:
            solution.append(interval)
            latest_finish = interval[<span class="dv">1</span>]
    <span class="kw">return</span> solution</code></pre>
<p>Most correctness proofs for greedy algorithms use strong induction. The proof for the greedy interval selection algorithm is relatively straightforward:</p>
<blockquote>
<p>Let <span class="math">\(A\)</span> be an array of intervals, sorted by their end values.<br />Let <span class="math">\(\mathcal{B} = \tup{A_{i_1}, \ldots, A_{i_k}}\)</span> where <span class="math">\(i_1 &lt; \ldots &lt; i_k\)</span> be the greedy solution.<br />Let <span class="math">\(\mathcal{O} = \tup{A_{j_1}, \ldots, A_{j_k}}\)</span> where <span class="math">\(j_1 &lt; \ldots &lt; j_l\)</span> be the optimal solution.<br />Clearly, <span class="math">\(k \le l\)</span> since <span class="math">\(O\)</span> is the optimal solution, and it is impossible for the greedy solution to be better than optimal.<br />We want to first prove that <span class="math">\(i_m \le j_m\)</span> for all <span class="math">\(1 \le m \le k\)</span>, which means that the ends of the intervals in the greedy solution are always before the ends of the corresponding intervals in the optimal solution.<br />For <span class="math">\(m = 1\)</span>, <span class="math">\(i_1 \le j_1\)</span> since <span class="math">\(i_1 = 1\)</span> - the greedy algorithm always initially chooses the interval that ends earliest.<br />Assume <span class="math">\(i_{m - 1} \le j_{m - 1}\)</span>. So in the greedy solution, the <span class="math">\(m - 1\)</span>th interval ends no later than the <span class="math">\(m - 1\)</span>th interval in the optimal solution.<br />Suppose <span class="math">\(i_m &gt; j_m\)</span>. Clearly, the optimal solution's <span class="math">\(m\)</span>th interval ends earlier than the greedy algorithm's <span class="math">\(m\)</span>th interval, since <span class="math">\(A\)</span> is sorted by end values.<br />However, this is not possible, since that means the greedy algorithm didn't choose the earliest end time. So <span class="math">\(i_m \le j_m\)</span>.<br />Suppose <span class="math">\(k &lt; l\)</span>. Clearly, the optimal solution must have at least <span class="math">\(k + 1\)</span> intervals.<br />Clearly, <span class="math">\(i_k \le j_k\)</span>. So the optimal solution's <span class="math">\(k + 1\)</span>th interval must be in the choice set for the greedy algorithm at the <span class="math">\(k\)</span>th step.<br />However, this isn't possible since the greedy algorithm didn't choose that interval, so <span class="math">\(k \ge l\)</span>.<br />Since <span class="math">\(k \le l\)</span> and <span class="math">\(k \ge l\)</span>, <span class="math">\(k = l\)</span> and the greedy solution is the same length as the optimal solution, and so the greedy algorithm must be optimal.</p>
</blockquote>
<p>The time complexity of this algorithm is easy to prove - it's <span class="math">\(\Theta(n \log n)\)</span> for the sort and <span class="math">\(\Theta(n)\)</span> for the traversal, so <span class="math">\(\Theta(n \log n)\)</span> overall.</p>
<p>Suppose we want to partition <span class="math">\(A\)</span> into disjoint sets of non-intersecting intervals, minimizing the number of these sets. This is called the interval coloring problem - we want to assign a color to each interval such that all intervals with a given color do not intersect each other.</p>
<p>An optimal greedy strategy for this is to sort the intervals by start values ascending, and for each interval in order, attempt to insert each interval into existing sets (assign it an existing color), and create a new set if not possible (assign it a new color):</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> partition_intervals(intervals):
    intervals = <span class="dt">sorted</span>(intervals, key=<span class="kw">lambda</span> interval: interval[<span class="dv">0</span>])
    partitions = [{intervals[<span class="dv">0</span>]}]
    partition_latest_finishes = [intervals[<span class="dv">0</span>][<span class="dv">1</span>]]
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">1</span>, <span class="dt">len</span>(intervals)):
        interval = intervals[i]
        <span class="kw">for</span> partition_index, latest_finish in <span class="dt">enumerate</span>(partition_latest_finishes): <span class="co"># try to assign interval to existing partitions</span>
            <span class="kw">if</span> latest_finish &lt;= interval[<span class="dv">0</span>]:
                partitions[partition_index].add(interval)
                partition_latest_finishes[partition_index] = interval[<span class="dv">1</span>]
                <span class="kw">break</span>
        <span class="kw">else</span>: <span class="co"># no existing colors can accept the interval</span>
            partition_index = <span class="dt">len</span>(partition_latest_finishes)
            partitions[partition_index] = {interval}
            partition_latest_finishes.append(interval[<span class="dv">1</span>])
    <span class="kw">return</span> partitions</code></pre>
<p>The correctness proof for this algorithm is a bit unusual:</p>
<blockquote>
<p>Suppose the greedy algorithm uses <span class="math">\(D\)</span> colors. Since we only create new colors if existing colors can't accept the interval, for every color <span class="math">\(c &lt; D\)</span>, there exists an interval <span class="math">\(A_c\)</span> that overlaps <span class="math">\(A_i\)</span>.<br />That means that there are at least <span class="math">\(D\)</span> intervals that all overlap each other, since they all overlap <span class="math">\(A_i\)</span>.<br />Therefore there must be at least <span class="math">\(D\)</span> colors, which means that having <span class="math">\(D\)</span> colors is optimal.</p>
</blockquote>
<p>Clearly, this algorithm is <span class="math">\(O(n^2)\)</span>, or more specifically <span class="math">\(O(nD)\)</span> since <span class="math">\(D\)</span> is <span class="math">\(O(n)\)</span>. We can actually do better by using a different data structure for the inner loop - the inner loop simply is trying to find the lowest end value in a partition, so if we use a heap for the partitions instead, we can have the algorithm run in <span class="math">\(O(n \log D)\)</span>, or more simply, <span class="math">\(O(n \log n)\)</span>:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> heapq
<span class="kw">def</span> partition_intervals(intervals):
    intervals = <span class="dt">sorted</span>(intervals, key=<span class="kw">lambda</span> interval: interval[<span class="dv">0</span>])
    partitions = [{intervals[<span class="dv">0</span>]}]
    partition_latest_finishes_heap = [(intervals[<span class="dv">0</span>][<span class="dv">1</span>], <span class="dv">0</span>)]
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">1</span>, <span class="dt">len</span>(intervals)):
        interval = intervals[i]
        latest_finish, partition_index = heapq.heappop(partition_latest_finishes_heap)
        <span class="kw">if</span> latest_finish &lt;= interval[<span class="dv">0</span>]: <span class="co"># try to assign interval to the existing partition that ends earliest</span>
            partitions[partition_index].add(interval)
            heapq.heappush(partition_latest_finishes_heap, (interval[<span class="dv">1</span>], partition_index))
        <span class="kw">else</span>: <span class="co"># no existing colors can accept the interval</span>
            partition_index = <span class="dt">len</span>(partition_latest_finishes_heap)
            partitions[partition_index] = {interval}
            heapq.heappush(partition_latest_finishes_heap, (interval[<span class="dv">1</span>], partition_index))
    <span class="kw">return</span> partitions</code></pre>
<h1 id="section-9">19/10/15</h1>
<p>Consider the <strong>knapsack problem</strong>: given profits <span class="math">\(P = [P_1, \ldots, P_n]\)</span> and weights <span class="math">\(W = [W_1, \ldots, W_n]\)</span>, and a capacity <span class="math">\(M\)</span>, all positive integers, find the <span class="math">\(x = \tup{x_1, \ldots, x_n}\)</span> that maximises <span class="math">\(\sum P_i x_i\)</span> given that <span class="math">\(\sum W_i x_i \le M\)</span>.</p>
<p>There are several variations of this problem: for the 0-1 knapsack problem, <span class="math">\(x_i\)</span> is either 0 or 1; in the rational knapsack problem, <span class="math">\(0 \le x_i \le 1\)</span>.</p>
<p>As it turns out, the knapsack problem can be solved greedily (though not optimally) - we simply take as much as possible of each item, considering them in decreasing order of profit divided by weight:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> greedy_rational_knapsack(profits, weights, capacity):
    items = <span class="dt">sorted</span>(<span class="dt">zip</span>(profits, weights), <span class="kw">lambda</span> x: x[<span class="dv">0</span>] / x[<span class="dv">1</span>])
    x = [<span class="dv">0</span>] * <span class="dt">len</span>(items)
    current_weight = <span class="dv">0</span>
    <span class="kw">for</span> item in items:
        profit, weight = item
        <span class="kw">if</span> current_weight + weight &lt;= capacity:
            result.append(<span class="dv">1</span>)
            current_weight += weight
        <span class="kw">else</span>:
            result.append((capacity - current_weight) / weight)
            <span class="kw">break</span>
    <span class="kw">return</span> x</code></pre>
<p>Proof of correctness:</p>
<blockquote>
<p>Suppose the greedy algorithm is not optimal. Then for certain inputs the output must differ from optimal algorithms.<br />Let the result of the greedy algorithm be <span class="math">\(G = [G_1, \ldots, G_n]\)</span> and a result <span class="math">\(B = [B_1, \ldots, B_n]\)</span> of all optimal algorithms for this problem such that <span class="math">\(B\)</span> and <span class="math">\(G\)</span> agree for as many items as possible, disagreeing at index <span class="math">\(d\)</span>, so <span class="math">\(B = [G_1, \ldots, G_{d - 1}, B_d, \ldots, B_n]\)</span>.<br />The greedy algorithm always chooses the item with the highest profit per weight, so the actual profit from <span class="math">\(G_1, \ldots, G_d\)</span> is at least as much as the actual profit of <span class="math">\(B_1, \ldots, B_d\)</span>, because we will pick the most.<br />Construct <span class="math">\(B&#39; = [B_1, \ldots, B_{d - 1}, G_d, B_{d + 1}, \ldots, B_n]\)</span>. Clearly, <span class="math">\(B&#39;\)</span> is at least as profitable as <span class="math">\(B\)</span>, but it agrees with <span class="math">\(G\)</span> for 1 more element than <span class="math">\(B\)</span>, a contradiction.<br />Therefore, the greedy algorithm is optimal.</p>
</blockquote>
<p>Consider the <strong>coin changing problem</strong>: given a list of coin denominations <span class="math">\(d_1, \ldots, d_n\)</span> and a positive integer <span class="math">\(T\)</span> (target sum), find <span class="math">\(A = [A_1, \ldots, A_n]\)</span> such that <span class="math">\(T = \sum a_i d_i\)</span> and <span class="math">\(N = \sum A_i\)</span> is minimised. In other words, we are trying to find the smallest set of coins that add up to a certain value.</p>
<p>We can do this simply by considering the coins from largest denomination to smallest, filling up the sum as close to the target sum as possible. Although this works for a coin system like <span class="math">\(1, 5, 10, 25, 100, 200\)</span>, this isn't always optimal: the coin system <span class="math">\(12, 5, 1\)</span> doesn't give the optimal solution for something like a target sum of 15. The study of where these algorithms are optimal is an area of active research.</p>
<p>Consider the <strong>stable marriage problem</strong>: suppose we have sets of <span class="math">\(n\)</span> men and <span class="math">\(n\)</span> women, <span class="math">\(M = [M_1, \ldots, M_n], W = [W_1, \ldots, W_n]\)</span>. Define <span class="math">\(\mathrm{pref}(M_i, k)\)</span> is the <span class="math">\(k\)</span>th favourite <span class="math">\(w \in W\)</span> for <span class="math">\(M_i\)</span>, and <span class="math">\(\mathrm{pref}(W_i, k)\)</span> is the <span class="math">\(k\)</span>th favourite <span class="math">\(m \in M\)</span> for <span class="math">\(W_i\)</span>. Find the matching <span class="math">\(\set{(M_{i_1}, W_{j_1}), \ldots, (M_{i_n}, W_{j_n})}\)</span> such that there does not exist any <span class="math">\((M_u, W_v)\)</span> that are not in the set, but prefer each other to their matches in the set - a <strong>stable matching</strong>.</p>
<p>Formally, we want to find a matching such that</p>
<p>The Gale-Shapley algorithm is an algorithm for this problem, for which Gale and Shapely won a Nobel prize for economics. In this algorithm, men propose to women from their most preferable to least preferable, and women accept proposals if not engaged or if a proposal is better than their currently accepted proposal (the new proposal replaces the old one), and women reject proposals otherwise.</p>
<p>Basically, the men propose starting from their most preferable to least preferable, and the women are proposed to starting from the least preferable to most preferable - the men always move down in their preferences, and the women always move up in their preferences:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> woman_prefers(woman, preference_function, man1, man2):
    i = <span class="dv">1</span>
    <span class="kw">while</span> <span class="ot">True</span>:
        man = preference_function(woman, i)
        <span class="kw">if</span> man == man1: <span class="kw">return</span> <span class="ot">True</span>
        <span class="kw">if</span> man == man2: <span class="kw">return</span> <span class="ot">False</span>
        i += <span class="dv">1</span>

<span class="kw">def</span> gale_shapley(men, women, preference_function):
    engagements = {}
    men_ranks = {man: <span class="dv">0</span> <span class="kw">for</span> man in men}
    unengaged_men = <span class="dt">set</span>(men)
    <span class="kw">while</span> unengaged_men:
        man = <span class="dt">next</span>(unengaged_men.keys())
        men_ranks[man] += <span class="dv">1</span> <span class="co"># next rank in preferences</span>
        woman = preference_function(man, men_ranks[man])
        <span class="kw">if</span> woman not in engagements or woman_prefers(woman, preference_function, man, engagements[woman]):
            <span class="kw">if</span> woman in engagements: unengaged_men.add(engagements[man])
            unengaged_men.remove(man)
            engagements[woman] = man
    <span class="kw">return</span> <span class="dt">set</span>(engagements.items())</code></pre>
<p>This algorithm is good for the men, but not the women - suiters can ask any of the reviewers, but reviewers can only pick from the suiters that proposed to them. For example, given 3 men and 3 women with preferences <span class="math">\(M_1: W_2, W_1, W_3; M_2: W_3, W_2, W_1; M_3: W_1, W_3, W_2; W_1: M_2, M_1, M_3; W_2: M_3, M_2, M_1; W_3: M_1, M_3, M_2\)</span>, all men get their first choices, while all women get their last choice. The men get the best possible match that will accept them, while the women get the worst possible match they will accept.</p>
<p>This algorithm always terminates because women cannot reject offers if unengaged, there is a man for every woman, and at some point a man will ask that woman. In the worst case, the men will all have their last choise, and have to propose to all <span class="math">\(n\)</span> of their preferences. Since there are <span class="math">\(n\)</span> men, the algorithm takes <span class="math">\(O(n^2)\)</span> proposals, specifically <span class="math">\(n^2 - n + 1\)</span> or less.</p>
<p>Proof of correctness:</p>
<blockquote>
<p>We want to prove that there is no man that prefers a woman that also prefers that man. Suppose Alice prefers Bob over her current partner, and Bob also prefers Alice over his current partner.<br />If Bob prefers Alice, then he must have proposed to her before his current partner.<br />Alice must have rejected the proposal, since Bob is engaged to a less preferable choice.<br />However, Alice is with a less preferable partner, which means Alice should have accepted Bob and rejected her current partner when he proposed.<br />This is a contradiction, so Alice and Bob cannot exist, and the matching is stable.</p>
</blockquote>
<h1 id="section-10">21/10/15</h1>
<p>To implement the Gale-Shapely algorithm more efficiently, we can represent the set of unengaged men as a linked list, represent the men's preferences using arrays/linked lists, and represent engagements using an array. We could also precompute a lookup table for <span class="math">\(M_j = \mathrm{pref}(W_i, k)\)</span> to look up <span class="math">\(j\)</span> given <span class="math">\(W_i\)</span> and <span class="math">\(M_j\)</span>.</p>
<h2 id="dynamic-programming">Dynamic Programming</h2>
<p>Consider a naive function to compute the <span class="math">\(n\)</span>th Fibonacci number, based on its recurrence definition:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> fibinacci(n):
    <span class="kw">if</span> n &lt;= <span class="dv">1</span>: <span class="kw">return</span> n
    <span class="kw">return</span> fibinacci(n - <span class="dv">1</span>) + fibinacci(n - <span class="dv">2</span>)</code></pre>
<p>However, this is extremely inefficient. Using the recurrence tree method, we find that it takes <span class="math">\(2f_{n + 1}\)</span> total calls of the function where <span class="math">\(f_n\)</span> is the <span class="math">\(n\)</span>th Fibonacci number. Using the closed form for the Fibonacci number, <span class="math">\(f_n = \frac{\phi^n - (-\phi)^{-n}}{\sqrt{5}}\)</span> where <span class="math">\(\phi\)</span> is the golden ratio, or around 1.62. Therefore, this function takes <span class="math">\(\Omega(\phi^n)\)</span> time.</p>
<p>This function basically is inefficient because it computes the same thing over and over again - for example, for <code>fibonacci(100)</code> we will call <code>fibonacci(10)</code> millions of times, when it guves the same result each time.</p>
<p>The idea behind dynamic programming is to avoid recomputing the same thing over and over again, by keeping those results around and simply looking them up when necessary. As a result dynamic programming trades off memory for time.</p>
<p>For example, if we were to write the above function iteratively, we might start from the base case and build upward toward the result:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> fibinacci(n):
    saved = [<span class="dv">0</span>, <span class="dv">1</span>]
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">2</span>, n + <span class="dv">1</span>):
        saved[i] = saved[i - <span class="dv">1</span>] + saved[i - <span class="dv">2</span>]
    <span class="kw">return</span> saved[n]</code></pre>
<p>However, note that at any given time, we only really use the last two elements of <code>saved</code>. Therefore, we can just store the last two elements:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> fibinacci(n):
    <span class="kw">if</span> n &lt;= <span class="dv">1</span>: <span class="kw">return</span> n
    last1, last2 = <span class="dv">0</span>, <span class="dv">1</span>
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">2</span>, n + <span class="dv">1</span>):
        current = last1 + last2
        last1, last2 = current, last1
    <span class="kw">return</span> current</code></pre>
<p>This is much better - we now have <span class="math">\(\Theta(n)\)</span> iterations and constant memory. Since the numbers grow exponentially, however, and addition takes <span class="math">\(\Theta(\log n)\)</span> (using multiprecision arithmetic), addition will actually take <span class="math">\(\Theta(n)\)</span>, so the algorithm overall takes <span class="math">\(\Theta(n^2)\)</span>.</p>
<p>Dyanmic programming is used to solve optimization problems, like greedy algorithms are often used to do.</p>
<p>To solve a problem using dynamic programming, we examine the structure of an optimal solution - the <strong>optimal structure</strong>. Given a problem instance <span class="math">\(I\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Define all the relevant subproblems <span class="math">\(S(I)\)</span> that allows us to compute <span class="math">\(I\)</span>.
<ul>
<li>This is just the direct subproblems, not the subproblems of the subproblems. For example, for <code>fibonacci(n)</code> we would have <span class="math">\(S(I)\)</span> be <code>fibonacci(n - 1)</code> and <code>fibonacci(n - 2)</code>.</li>
</ul></li>
<li>Write a recurrence relation for <span class="math">\(I\)</span> in terms of <span class="math">\(S(I)\)</span> and the base cases.
<ul>
<li>The references to <span class="math">\(S(I)\)</span> allow us to look up the answers later in the table.</li>
</ul></li>
<li>Compute the optimal solution to <span class="math">\(I\)</span> using the recurrence relation from the bottom-up (starting from the base cases of the recurrence relation) - filling in a table of values for the solutions to each subproblem.
<ul>
<li>Whenever we have a subproblem we've previously encountered, we can simply look up the answer in the table to avoid recomputing it.</li>
<li>When all subproblems have been solved, we can often compute the solution to <span class="math">\(I\)</span> very simply using values from the table.</li>
</ul></li>
</ol>
<p>This is somewhat similar to divide and conquer, where we have subproblems and solve them recursively, up until we actually need to combine the solutions - instead of combining recursively, we solve them from the bottom up.</p>
<p>While the 0-1 knapsack problem we saw previously is NP-complete, we can solve it relatively well using dynamic programming.</p>
<p>Suppose <span class="math">\([x_1, \ldots, x_n]\)</span> is an optimal solution for a given problem instance where <span class="math">\(P = [P_1, \ldots, P_n]\)</span> and weights <span class="math">\(W = [W_1, \ldots, W_n]\)</span>, and capacity <span class="math">\(M\)</span>. Clearly, either <span class="math">\(x_n = 0\)</span> or <span class="math">\(x_n = 1\)</span>.</p>
<p>If <span class="math">\(x_n = 0\)</span>, then <span class="math">\([x_1, \ldots, x_{n - 1}]\)</span> is an optimal solution to a smaller problem instance where <span class="math">\(P = [P_1, \ldots, P_{n - 1}]\)</span> and weights <span class="math">\(W = [W_1, \ldots, W_{n - 1}]\)</span>, and capacity <span class="math">\(M\)</span>, and the profit of <span class="math">\([x_1, \ldots, x_n]\)</span> is the same as the profit of <span class="math">\([x_1, \ldots, x_{n - 1}]\)</span>.</p>
<p>If <span class="math">\(x_n = 1\)</span>, then <span class="math">\([x_1, \ldots, x_{n - 1}]\)</span> is an optimal solution to a smaller problem instance where <span class="math">\(P = [P_1, \ldots, P_{n - 1}]\)</span> and weights <span class="math">\(W = [W_1, \ldots, W_{n - 1}]\)</span>, and capacity <span class="math">\(M - W_n\)</span>, and the profit of <span class="math">\([x_1, \ldots, x_n]\)</span> is the same as the profit of <span class="math">\([x_1, \ldots, x_{n - 1}]\)</span> plus <span class="math">\(P_n\)</span>.</p>
<p>Clearly, the profit of <span class="math">\([x_1, \ldots, x_n]\)</span> is the larger of the two cases, where either <span class="math">\(x_n = 0\)</span> or <span class="math">\(x_n = 1\)</span>.</p>
<p>These are our relevant subproblems - the knapsack problem instances with the same weights and profits, but only including the first <span class="math">\(0 \le i \le n\)</span> items and for different capacities <span class="math">\(0 \le m \le M\)</span>, so <span class="math">\(P = [P_1, \ldots, P_i]\)</span> and weights <span class="math">\(W = [W_1, \ldots, W_i]\)</span>.</p>
<p>Now we can define a 2D array (a table) <span class="math">\(P[i, m]\)</span> that defines the optimal profit of such a subproblem, considering the first <span class="math">\(i\)</span> items and for a knapsack with capacity <span class="math">\(m\)</span>.</p>
<p>The basic idea will be to compute <span class="math">\(P[i, m]\)</span> for all <span class="math">\(i\)</span> and <span class="math">\(m\)</span>, until we get to <span class="math">\(P(n, M)\)</span>.</p>
<h1 id="section-11">26/10/15</h1>
<p>Now we'll write the recurrence relation: <span class="math">\(P[i, m] = \max\left(P[i - i, m], P[i - 1, m - W_i]\right)\)</span>. Here, <span class="math">\(P[i - i, m]\)</span> represents the case where <span class="math">\(x_i = 0\)</span>, and <span class="math">\(P[i - 1, m - W_i]\)</span> represents the case where <span class="math">\(x_i = 1\)</span>.</p>
<p>We also need some base cases: <span class="math">\(P[i, m] = \begin{cases} 0 &amp;\text{if } i = 1 \wedge m &lt; W_i \\ P_1 &amp;\text{if } i = 1 \wedge m \ge W_i \\ P[i - 1, m] &amp;\text{if } i &gt; 1 \wedge m &lt; W_i \\ \max\left(P[i - i, m - W_i] + P_i, P[i - 1, m]\right) &amp;\text{otherwise} \\ \end{cases}\)</span>.</p>
<p>Now we fill in an <span class="math">\(n\)</span> by <span class="math">\(M + 1\)</span> table of all <span class="math">\(P[i, m]\)</span> values, For each row <span class="math">\(i\)</span>, we fill in all the <span class="math">\(m\)</span> values from smallest to largest. The final answer is therefore at the last cell of the bottommost row.</p>
<p>In code, it looks like this:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> collections <span class="ch">import</span> defaultdict
<span class="kw">def</span> knapsack(profits, weights, max_weight):
    max_profits = defaultdict(<span class="dt">dict</span>)
    <span class="kw">for</span> weight in <span class="dt">range</span>(max_weight):
        <span class="kw">if</span> weight &gt;= weights[<span class="dv">0</span>]:
            max_profits[<span class="dv">0</span>][m] = profits[<span class="dv">0</span>]
        <span class="kw">else</span>:
            max_profits[<span class="dv">0</span>][m] = <span class="dv">0</span>
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">1</span>, <span class="dt">len</span>(profits)):
        <span class="kw">for</span> weight in <span class="dt">range</span>(max_weight):
            <span class="kw">if</span> weight &lt; weights[i]:
                max_profits[i][weight] = P[i - <span class="dv">1</span>][weight]
            <span class="kw">else</span>:
                max_profits[i][weight] = <span class="dt">max</span>(max_profits[i - <span class="dv">1</span>][weight - weights[i]] + profits[i], max_profits[i - <span class="dv">1</span>][weight])
    <span class="kw">return</span> max_profits[<span class="dt">len</span>(profits) - <span class="dv">1</span>][max_weight - <span class="dv">1</span>]</code></pre>
<p>One improvement we could make is that in the last row we only need to compute the single entry <span class="math">\(P[n, M]\)</span>, since it only relies on <span class="math">\(P[n - 1, M - w_i]\)</span> and <span class="math">\(P[n - 1, M]\)</span>.</p>
<p>The above algorithm finds the maximum profit, but not the actual items chosen to get that profit. We can get the items by repeatedly finding which <span class="math">\(\max\left(P[i - i, m - W_i] + P_i, P[i - 1, m]\right)\)</span> we chose at each step, and if <span class="math">\(P[i, m] = P[i - i, m - W_i] + P_i\)</span>, then we include the item in the solution. then, we repeatedly do the same check for <span class="math">\(P[i - i, m - W_i]\)</span> if <span class="math">\(P[i, m] = P[i - i, m - W_i] + P_i\)</span> or do the same check for <span class="math">\(P[i - 1, m]\)</span> if <span class="math">\(P[i, m] = P[i - 1, m]\)</span>, and so on:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> collections <span class="ch">import</span> defaultdict
<span class="kw">def</span> knapsack(profits, weights, max_weight):
    max_profits = defaultdict(<span class="dt">dict</span>)
    <span class="kw">for</span> weight in <span class="dt">range</span>(max_weight):
        <span class="kw">if</span> weight &gt;= weights[<span class="dv">0</span>]:
            max_profits[<span class="dv">0</span>][m] = profits[<span class="dv">0</span>]
        <span class="kw">else</span>:
            max_profits[<span class="dv">0</span>][m] = <span class="dv">0</span>
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">1</span>, <span class="dt">len</span>(profits)):
        <span class="kw">for</span> weight in <span class="dt">range</span>(max_weight):
            <span class="kw">if</span> weight &lt; weights[i]:
                max_profits[i][weight] = P[i - <span class="dv">1</span>][weight]
            <span class="kw">else</span>:
                max_profits[i][weight] = <span class="dt">max</span>(max_profits[i - <span class="dv">1</span>][weight - weights[i]] + profits[i], max_profits[i - <span class="dv">1</span>][weight])
    
    <span class="co"># figure out which items are in this max profit selection</span>
    weight = max_weight
    items = [<span class="ot">None</span>] * <span class="dt">len</span>(profits)
    profit = max_profits[<span class="dt">len</span>(profits) - <span class="dv">1</span>, capacity - <span class="dv">1</span>]
    <span class="kw">for</span> i in <span class="dt">range</span>[<span class="dt">len</span>(profits) - <span class="dv">1</span>, <span class="dv">0</span>, -<span class="dv">1</span>]: <span class="co"># go through indices from the last row&#39;s index to the second row&#39;s index</span>
        <span class="kw">if</span> profit == max_profits[i - <span class="dv">1</span>, weight]:
            items[i] = <span class="ot">False</span>
        <span class="kw">else</span>:
            items[i] = <span class="ot">True</span>
        profit -= profits[i]
        weight -= weights[i]
    items[<span class="dv">0</span>] = profit == <span class="dv">0</span>

    <span class="kw">return</span> items</code></pre>
<p>Clearly, this is an <span class="math">\(\Theta(nM)\)</span> algorithm, since the table has <span class="math">\(nM\)</span> cells and each cell value takes constant time to compute, and finding the actual items can be done in <span class="math">\(\Theta(n)\)</span> since we trace one row per iteration.</p>
<p>Although this seems to be a polynomial time algorithm, this is actually not the case. The size of a given problem instance is the <strong>sum of the sizes of its inputs</strong>.</p>
<p>At first, we might assume that this is <span class="math">\(n + M\)</span>. However, the size of the <span class="math">\(M\)</span> input is <strong>actually the number of bits it has</strong>, which is <span class="math">\(k = \ceil{\log_2 M}\)</span> - the size of the input is <span class="math">\(n + k\)</span>. Therefore, the algorithm is <span class="math">\(O(n2^k)\)</span>. This is consistent with the fact that the knapsack problem is NP-complete.</p>
<p>There is also a recursive backtracking algorithm that tries all the combinations of items and finds the largest profit, which runs in <span class="math">\(\Theta(2^n)\)</span>. This is useful in different in different situations - even though both are exponential, the dynamic programming one is better for small <span class="math">\(M\)</span> and large <span class="math">\(n\)</span>, and the backtracking one is better for small <span class="math">\(n\)</span> and large <span class="math">\(M\)</span>.</p>
<p>We can solve the coin changing problem optimally as well, in almost the same way:</p>
<blockquote>
<p>Note that for <span class="math">\(d_n\)</span>, <span class="math">\(0 \le a_n \le \frac T {d_n}\)</span>.<br />Our dynamic programming table will be <span class="math">\(n\)</span> by <span class="math">\(\ceil{\frac{T}{d_1}}\)</span> - each row is the number of coins, and each column is a different target sum.<br />Let <span class="math">\(N[i, t]\)</span> be a list of coins used in the optimal solution to the coin changing problem instance with only the coin denominations <span class="math">\(d_1, \ldots, d_i\)</span> and target sum <span class="math">\(t\)</span>.<br />Then we have the recurrence relation <span class="math">\(N[i, t] = \begin{cases} j \text{ where } 0 \le j \le \floor{\frac t d_i} \text{ is the index for which } N[i - 1, t - jd_i] + j \text{ is minimized} &amp;\text{if } i &gt; 1 \\ t &amp;\text{if } i = 1 \\ \end{cases}\)</span>.<br />The final answer is in <span class="math">\(N[n, T]\)</span>. We could also save memory by, instead of having <span class="math">\(N[i, t]\)</span> for solutions, having a table <span class="math">\(A[i, t]\)</span> store the number of coins used in the optimal solutions, and then at the end tracing backwards like we did for the knapsack problem.</p>
</blockquote>
<h1 id="section-12">28/10/15</h1>
<h3 id="longest-common-subsequence">Longest Common Subsequence</h3>
<p>Suppose we have two sequences <span class="math">\(X = [X_1 \ldots X_m]\)</span> and <span class="math">\(Y = [Y_1 \ldots Y_n]\)</span> where all elements are in some finite alphabet <span class="math">\(A\)</span>. What is the longest subsequence that is in both <span class="math">\(X\)</span> and <span class="math">\(Y\)</span>?</p>
<p>A subsequence of a sequence <span class="math">\(Z\)</span> is a sequence that contains any number of the elements of <span class="math">\(Z\)</span> in order, but not necessarily consecutively. For example, <span class="math">\(1, 4, 8, 9\)</span> is a subsequence of <span class="math">\(1, 2, 3, 4, 5, 6, 7, 8, 9\)</span>.</p>
<p>We will solve this using dynamic programming. Let <span class="math">\(\mathrm{LCS}(X, Y)\)</span> represent the length of the longest common subsequence.</p>
<p>If <span class="math">\(X_m = Y_n\)</span>, then <span class="math">\(\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_{n - 1}) + 1\)</span>.</p>
<p>If <span class="math">\(X_m \ne Y_n\)</span>, then <span class="math">\(\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \max\left(\mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_n), \mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_{n - 1})\right)\)</span>. This is because there are three cases: if the LCS ends on <span class="math">\(Y_n\)</span>, then it doesn't end on <span class="math">\(X_m\)</span> and we have <span class="math">\(\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_n)\)</span>; if the LCS ends on <span class="math">\(X_m\)</span>, then it doesn't end on <span class="math">\(Y_n\)</span> and we have <span class="math">\(\mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_n) = \mathrm{LCS}(X_1 \ldots X_m, Y_1 \ldots Y_{n - 1})\)</span>. The third case is if the LCS doesn't end on <span class="math">\(X_m\)</span> or <span class="math">\(Y_n\)</span>, but we don't need to consider it in the formula because <span class="math">\(\mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_{n - 1}) \le \mathrm{LCS}(X_1 \ldots X_{m - 1}, Y_1 \ldots Y_n)\)</span> - it is already covered by the other cases.</p>
<p>The basic idea behind the recurrence is that we keep chopping off elements from the ends of <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> - we are computing the LCS for all prefixes of the string. We now define our dynamic programming table, <span class="math">\(c[i, j] = \mathrm{LCS}(X_1 \ldots X_i, Y_1 \ldots Y_j)\)</span> (the rows are values of <span class="math">\(i\)</span> and the columns are values of <span class="math">\(j\)</span>).</p>
<p>Clearly, we now have the recurrence <span class="math">\(c[i, j] = \begin{cases} 0 &amp;\text{if } i = 0 \vee j = 0 \\ c[i - 1, j - 1] &amp;\text{if } X_i = Y_j \wedge i &gt; 0 \wedge j &gt; 0 \\ \max(c[i - 1, j], c[i, j - 1]) &amp;\text{if } X_i \ne Y_j \wedge i &gt; 0 \wedge j &gt; 0 \\ \end{cases}\)</span>. We want to compute <span class="math">\(c[m, n]\)</span>.</p>
<p>Clearly, to compute any <span class="math">\(c[i, j]\)</span>, we need to ensure that <span class="math">\(c[i - 1, j - 1], c[i - 1, j], c[i, j - 1]\)</span> are already computed. We can do this by filling in the table from left to right, top to bottom. In code, it looks like the following:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> collections <span class="ch">import</span> defaultdict
<span class="kw">def</span> LCS(X, Y):
    c = defaultdict(<span class="dt">dict</span>)
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dt">len</span>(X) + <span class="dv">1</span>): c[i][<span class="dv">0</span>] = <span class="dv">0</span>
    <span class="kw">for</span> j in <span class="dt">range</span>(<span class="dt">len</span>(Y) + <span class="dv">1</span>): c[<span class="dv">0</span>][j] = <span class="dv">0</span>
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">1</span>, <span class="dt">len</span>(X) + <span class="dv">1</span>):
        <span class="kw">for</span> j in rnage(<span class="dv">1</span>, <span class="dt">len</span>(Y) + <span class="dv">1</span>):
            <span class="kw">if</span> X[i] = Y[j]:
                c[i, j] = c[i - <span class="dv">1</span>][j - <span class="dv">1</span>]
            <span class="kw">else</span>:
                c[i, j] = <span class="dt">max</span>(c[i][j - <span class="dv">1</span>], c[i - <span class="dv">1</span>][j - <span class="dv">1</span>])
    <span class="kw">return</span> c[<span class="dt">len</span>(X), <span class="dt">len</span>(Y)]</code></pre>
<p>We can also get the actual sequence by doing a traceback like for the knapsack problem. There are three cases: if we moved up and to the left in the table, then we include <span class="math">\(X_i\)</span> or <span class="math">\(Y_j\)</span> in the LCS and set <span class="math">\(i = i - 1, j = j - 1\)</span>; if we moved left in th etable, we set <span class="math">\(j = j - 1\)</span>; if we moved right in the table, we set <span class="math">\(i = i - 1\)</span>. By repeatedly following this until we get a base case, we get the LCS:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> collections <span class="ch">import</span> defaultdict
<span class="kw">def</span> LCS(X, Y):
    c = defaultdict(<span class="dt">dict</span>)
    direction = defaultdict(<span class="dt">dict</span>)
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dt">len</span>(X) + <span class="dv">1</span>): c[i][<span class="dv">0</span>] = <span class="dv">0</span>
    <span class="kw">for</span> j in <span class="dt">range</span>(<span class="dt">len</span>(Y) + <span class="dv">1</span>): c[<span class="dv">0</span>][j] = <span class="dv">0</span>
    <span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">1</span>, <span class="dt">len</span>(X) + <span class="dv">1</span>):
        <span class="kw">for</span> j in rnage(<span class="dv">1</span>, <span class="dt">len</span>(Y) + <span class="dv">1</span>):
            <span class="kw">if</span> X[i] = Y[j]:
                c[i, j] = c[i - <span class="dv">1</span>][j - <span class="dv">1</span>]
                direction[i][j] = <span class="st">&quot;up_left&quot;</span>
            <span class="kw">else</span>:
                c[i, j] = <span class="dt">max</span>(c[i][j - <span class="dv">1</span>], c[i - <span class="dv">1</span>][j - <span class="dv">1</span>])
                direction[i][j] = <span class="st">&quot;left&quot;</span> <span class="kw">if</span> c[i, j] == c[i][j - <span class="dv">1</span>] <span class="kw">else</span> <span class="st">&quot;up&quot;</span>
    
    <span class="co"># retrieve the LCS</span>
    i, j = <span class="dt">len</span>(X), <span class="dt">len</span>(Y)
    sequence = []
    <span class="kw">while</span> i &gt; <span class="dv">0</span> and j &gt; <span class="dv">0</span>:
        <span class="kw">if</span> direction[i][j] == <span class="st">&quot;up_left&quot;</span>:
            sequence = X[i] + sequence <span class="co"># we could also use `Y[j]` rather than `X[i]` since they are identical here</span>
            i -= <span class="dv">1</span>; j -= <span class="dv">1</span>
        <span class="kw">elif</span> direction[i, j] == <span class="st">&quot;up&quot;</span>:
            i -= <span class="dv">1</span>
        <span class="kw">else</span>:
            j -= <span class="dv">1</span>
    <span class="kw">return</span> sequence</code></pre>
<p>Clearly, this algorithm takes <span class="math">\(\Theta(mn)\)</span> time and memory, since computing each table entry takes constant time and there are <span class="math">\((m + 1)(n + 1)\)</span> table entries.</p>
<h3 id="minimum-length-triangulation">Minimum Length Triangulation</h3>
<p>Suppose we have a convex polygon denoted by point <span class="math">\(q_1, \ldots, q_n\)</span>. How can we triangulate it such that the sum of the perimeters of the <span class="math">\(n - 2\)</span> triangles are minimized?</p>
<p>In other words, what is the trangulation such that the sum of the lengths of all the chords (lines within the polygon that divide it into triangles) is minimised? These two formulations are the same because the sum of lengths of all the chords times 2 plus the perimeter of the polygon is the sum of the perimeters of all the triangles.</p>
<p>As it turns out, a polygon with <span class="math">\(n\)</span> vertices has <span class="math">\(C_n\)</span> possible triangulations, where <span class="math">\(C_n\)</span> is the <span class="math">\(n\)</span>th Catalan number (<span class="math">\(C_n = \frac 1 {n + 1} {2n} \choose n = \Theta\left(\frac{4^n}{n^{\frac 3 2}}\right)\)</span>). This is exponential, so we want to avoid computing all possible triangulations.</p>
<p>Clearly, <span class="math">\(q_n q_1\)</span> is in a triangle <span class="math">\(q_n q_k q_1\)</span>, where <span class="math">\(2 \le k \le n - 1\)</span>. Clearly, for each <span class="math">\(k\)</span>, we have the triangle <span class="math">\(q_n q_k q_1\)</span>, which divides the polygon into a polygon with vertices <span class="math">\(q_1, \ldots, q_k\)</span>, and a polygon with vertices <span class="math">\(q_k, \ldots, q_n\)</span>.</p>
<p>Clearly, the smallest triangulation of the polygon that includes the triangle <span class="math">\(q_n q_k q_n\)</span> is this triangle along with the triangles in the smallest triangulation of <span class="math">\(q_1, \ldots, q_k\)</span> and the smallest triangulation of <span class="math">\(q_k, \ldots, q_n\)</span>. If we check all possible <span class="math">\(k\)</span>, then the smallest of all of these is the smallest triangulation of the polygon <span class="math">\(q_1, \ldots, q_n\)</span>. These smaller polygons are our subproblems. Since all of them are polygons of the form <span class="math">\(q_1, \ldots, q_i, \ldots, q_j, \ldots, q_n, 1 \le i &lt; j \le n\)</span>, we have <span class="math">\(\Theta(n^2)\)</span> subproblems in total.</p>
<p>Let <span class="math">\(S[i, j]\)</span> denote the smallest triangulation of the polygon <span class="math">\(q_i, \ldots, q_j\)</span>. Let <span class="math">\(P(q_i, q_k, q_j)\)</span> be the perimeter of the triangle formed by <span class="math">\(q_i, q_k, q_j\)</span>. Then <span class="math">\(S[i, j] = \min\set{P(q_i, q_k, q_j) + S[i, k] + S[k, j] \middle| k \in \mb{Z}, i &lt; k &lt; j}\)</span>.</p>
<h1 id="section-13">2/11/15</h1>
<h2 id="graph-algorithms">Graph Algorithms</h2>
<p>A graph is a pair <span class="math">\(\tup&lt;V, E&gt;\)</span>, where <span class="math">\(V\)</span> is the set of vertices <span class="math">\(\set{v_1, \ldots, v_n}\)</span> and <span class="math">\(E\)</span> is the set of edges <span class="math">\(\set{\set{u_1, v_1}, \ldots, \set{u_m, v_m}}\)</span>, where <span class="math">\(u, v\)</span> are edges. A digraph (directed graph) is also a pair <span class="math">\(\tup{V, E}\)</span>, but <span class="math">\(E\)</span> is a set of edges <span class="math">\(\set{\tup{u_1, v_1}, \ldots, \tup{u_m, v_m}}\)</span> - they have arcs (directed edges) that represent direction. In directed edges, <span class="math">\(u\)</span> is the <strong>tail</strong> and <span class="math">\(v\)</span> is the <strong>head</strong>. We can also write <span class="math">\(\tup{u, v}\)</span> and <span class="math">\(\set{u, v}\)</span> as <span class="math">\(uv\)</span>, which is easier to write and covers both directed and undirected edges.</p>
<p>In this course, we will not allow duplicate edges, so <span class="math">\(m \le n^2\)</span>. We will also not allow loops, so vertices will never have edges connecting them to themselves.</p>
<p>On the computer, we often represent graphs with adjecency matrices and adjacency lists.</p>
<p>An <strong>adjacency matrix</strong> is an <span class="math">\(n\)</span> by <span class="math">\(n\)</span> matrix where each row represents a vertex and each column represents a vertex. The elements of this matrix are 1 if there is an edge from the vertex for its row to the vertex for its column (<span class="math">\(A_{u, v} = \begin{}\)</span>). Formally, the adjacency matrix is <span class="math">\(A = \begin{bmatrix} a_{1, 1} &amp; \ldots &amp; a_{1, n} \\ \vdots &amp; \vdots &amp; a_\vdots \\ a_{n, 1} &amp; \ldots &amp; a_{n, n} \\ \end{bmatrix}\)</span>.</p>
<p>Since loops aren't allowed, the diagonals <span class="math">\(a_{1, 1}, \ldots, a_{n, n}\)</span> are all 0. Note that for an undirected graph, the adjacency matrix has <span class="math">\(2m\)</span> 1 entries, while for a directed graph, the adjacency matrix has <span class="math">\(m\)</span> 1 entries.</p>
<p>An <strong>adjacency list</strong> is an array of <span class="math">\(n\)</span> linked lists, represented as <span class="math">\(\adj(_1), \ldots, \adj(v_n)\)</span>. <span class="math">\(\adj(v_i)\)</span> is the linked list of the vertices that <span class="math">\(v_i\)</span> is directly connected to. This is basically the same as the adjacency mtrix, but as an array of linked lists rather than a matrix. This is more memory efficient for sparse graphs, but make some operations more difficult.</p>
<p>We usually use breath-first search for undirected graphs, and depth-first search for directed graphs.</p>
<p>To do a breadth-first search, we pick a vertex <span class="math">\(s\)</span>, then spread out from <span class="math">\(s\)</span> by keeping track of the vertices we've explored so far, and exploring the unexplored neighbors of the explored vertices. Basically, the search spreads out from <span class="math">\(s\)</span> in layers. Starting from <span class="math">\(s\)</span>, we explore its neighbors, then the neighbors of those, then the neighbors of those, and so on. The full form of breadth-first search appears as follows:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> breadth_first_search(vertices, adjacency_list):
    start_vertex = vertices[<span class="dv">0</span>]
    colors, predecessors = {}, {}
    <span class="kw">for</span> vertex in vertices:
        colors[vertex] = <span class="st">&quot;white&quot;</span>
        predecessors[v] = <span class="dt">set</span>()
    colors[start_vertex] = <span class="st">&quot;gray&quot;</span>
    q = queue()
    q.enqueue(start_vertex)
    <span class="kw">while</span> q:
        vertex = q.dequeue()
        <span class="kw">for</span> neighbor in adjacency_list[vertex]:
            <span class="kw">if</span> colors[neighbor] == <span class="st">&quot;white&quot;</span>:
                colors[neighbors] = <span class="st">&quot;gray&quot;</span>
                predecessors[neighbor] = vertex
                q.enqueue(neighbor)
            colors[vertex] = <span class="st">&quot;black&quot;</span></code></pre>
<p>The colors represent the visit state of vertices - white means a vertex is undiscovered, gray means it's discovered but its neighbors are still being processed, and black means it's discovered and fully processed. The <code>predecessors</code> dict maps all vertices (except the start vertex) to the the vertices they were visited from (their <strong>predecessors</strong>), and is formally represented <span class="math">\(\pi(v_i)\)</span>. The colors represent the visit state of vertices - white means a vertex is undiscovered, gray means it's discovered but its neighbors are still being processed, and black means it's discovered and fully processed. The <code>predecessors</code> dict maps all vertices (except the start vertex) to the the vertices they were visited from (their <strong>predecessors</strong>), and is formally represented <span class="math">\(\pi(v_i)\)</span>.</p>
<p>The edge <span class="math">\(\tup{v_i, \pi(v_i)}\)</span> is a <strong>tree edge</strong> (these are always directed, even when the graph we're running on is undirected). All other edges are <strong>cross edges</strong>. The set of all tree edges forms a <strong>BFS tree</strong>, and if the graph is connected, the BFS tree is a spanning tree.</p>
<p>A simpler form of the algorithm might appear as follows:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> breadth_first_search(vertices, adjacency_list):
    start_vertex = vertices[<span class="dv">0</span>]
    visited = <span class="dt">set</span>()
    q = queue()
    q.enqueue(start_vertex)
    <span class="kw">while</span> q:
        vertex = q.dequeue()
        visited.add(vertex)
        <span class="kw">for</span> neighbor in adjacency_list[vertex]:
            <span class="kw">if</span> neighbor not in visited:
                q.enqueue(neighbor)</code></pre>
<p>Shortest path from a starting vertex:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> shortest_path(vertices, adjacency_list, start_vertex):
    start_vertex = vertices[<span class="dv">0</span>]
    visited = <span class="dt">set</span>()
    q = queue()
    q.enqueue(start_vertex)
    distance_to_start = {start_vertex: <span class="dv">0</span>}
    <span class="kw">while</span> q:
        vertex = q.dequeue()
        visited.add(vertex)
        <span class="kw">for</span> neighbor in adjacency_list[vertex]:
            <span class="kw">if</span> neighbor not in visited:
                q.enqueue(neighbor)
                distance_to_start[neighbor] = distance_to_start[vertex] + <span class="dv">1</span></code></pre>
<p>Proof of correctness:</p>
<blockquote>
<p>Let <span class="math">\(\mathrm{dist}(v)\)</span> represent <code>distance_to_start[v]</code> where <span class="math">\(v\)</span> is a vertex, and <span class="math">\(s\)</span> represent <code>start_vertex</code>.<br />We want to prove that for any vertex <span class="math">\(v\)</span>, <span class="math">\(\mathrm{dist}(v)\)</span> is the length of the shortest path from <span class="math">\(s\)</span> to <span class="math">\(v\)</span>.<br />Define layers as <span class="math">\(L_i = \set{v \middle| \mathrm{dist}(v) = 1}\)</span>.<br />Clearly, <span class="math">\(v \in L_i\)</span> if and only if <span class="math">\(uv\)</span> is an edge such that <span class="math">\(u \in L_{i - 1}\)</span> and there is no <span class="math">\(u&#39; \in L_j, j &lt; i - 1\)</span> - it is adjacent to a vertex in a previous layer and is not adjacent to any vertices in lower layers.<br />Clearly, if <span class="math">\(u\)</span> is discovered before <span class="math">\(v\)</span>, then <span class="math">\(\mathrm{dist}(u) \le \mathrm{dist}(v)\)</span>.<br />Clearly, if <span class="math">\(uv\)</span> is an edge, then <span class="math">\(\abs{\mathrm{dist}(u) - \mathrm{dist}(v)} \le 1\)</span>.<br />Clearly, if <span class="math">\(uv\)</span> is a tree edge, then <span class="math">\(\mathrm{dist}(v) = \mathrm{dist}(u) + 1\)</span>.<br />Let <span class="math">\(\delta(v)\)</span> be the length of the shortest path from <span class="math">\(s\)</span> to <span class="math">\(v\)</span>.<br />Clearly, <span class="math">\(\delta(v) \le \mathrm{dist}(v)\)</span> because <span class="math">\(\mathrm{dist}(v)\)</span> is the length of the sequence <span class="math">\(v, \pi(v), \pi(\pi(v)), \ldots, s\)</span>.<br />We can easily use induction over the shortest path <span class="math">\(s, v_{i_1}, \ldots, v_{i_k}, v\)</span> to prove that <span class="math">\(\delta(v) \le \mathrm{dist}(v)\)</span>.<br />Therefore, <span class="math">\(\delta(v) = \mathrm{dist}(v)\)</span>, and <span class="math">\(\mathrm{dist}(v)\)</span> is the length of the shortest path.</p>
</blockquote>
<p>For most graph algorithms, the steps will be quite intuitive, but the proofs of correctness will be rather difficult.</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>