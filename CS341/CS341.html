<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS341 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>
function highlight() { // highlight all code blocks using HighlightJS
  var code_blocks = document.getElementsByTagName("code");
  for (var i = 0; i < code_blocks.length; i++)
    hljs.highlightBlock(code_blocks[i]);
}
</script>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
  </ul>
<h1 id="cs341">CS341</h1>
<p>Algorithms.</p>
<pre><code>Doug Stinson
Section 001
https://www.student.cs.uwaterloo.ca/~cs341/</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l&#39;H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
\]</span></p>
<h1 id="section">14/9/15</h1>
<p>Most materials will be on LEARN. Questions and answers can be posted on Piazza.</p>
<p>Study of the design and the analsis of algorithms - in particular, the correctness (proved via formal proofs) and efficiency (proved using time complexity analysis).</p>
<p>Useful metrics for algorithms are asymptotic complexity (best case/average case/worst case time/space complexity), number of specific computations used (like comparisons in sorting algorithms), whether an algorithm is the most efficient for a particular problem, etc.</p>
<p>Interesting things to know are lower bounds on possible algorithms to solve a particular problems, problems that cannot be solved by any algorithm (undecidability), and problems that cannot be solved efficiently by any algorithm, but can be solved (NP-hardness).</p>
<p>Useful design strategies we will go over are divide and conquer, greedy algorithms, dynamic programming, breadth-first and depth-first search, local search, and linear programming.</p>
<p>An example of an algorithm design is the maximum problem: given an array of integers <span class="math">\(A\)</span>, find the maximum integer in <span class="math">\(A\)</span>. A simple solution to this is as follows:</p>
<pre><code>def find_max(array):
    current = array[0]
    for elem in array[1:]:
        if elem &gt; current: current = elem
    return elem</code></pre>
<p>This seems to be obviously correct, but we can also use program verification to prove it correct. In this case, we will use induction to prove the loop invariant - at the end of each iteration of the loop, <code>current</code> is equal to the largest element encountered so far in the array. We'd check the base case for arrays of length 1, then prove the inductive hypothesis to formally verify the program.</p>
<p>Clearly, this algorithm is <span class="math">\(\Theta(n)\)</span>, since it loops over the entire array once. Formal time complexity analysis can also be done by noting that the loop body takes <span class="math">\(\Theta(1)\)</span> time, and the loop runs <span class="math">\(\Theta(n)\)</span> times.</p>
<p>We also know that this algorithm is asymptotically optimal, at least in terms of the number of comparisons done. Since a correct solution to the Maximum problem must look at every element in the array, it must therefore do at least <span class="math">\(n - 1\)</span> comparisons. Since each comparison takes <span class="math">\(\Theta(1)\)</span> time, the best possible algorithm must take <span class="math">\(\Omega(n)\)</span> time. Let's formally prove this:</p>
<blockquote>
<p>Suppose there was an algorithm that could determine the maximum element of an array using fewer than <span class="math">\(n - 1\)</span> comparisons.<br />Let <span class="math">\(G\)</span> be a graph such that each vertex in <span class="math">\(G\)</span> corresponds to an element in <span class="math">\(A\)</span>, and each comparison done by a run of the algorithm between any two elements results in an edge between those two elements.<br />Clearly, there are <span class="math">\(n - 2\)</span> edges or less, and <span class="math">\(n\)</span> vertices.<br />Therefore, there are at least 2 components in <span class="math">\(G\)</span>, since the graph cannot be connected.<br />Clearly, the solution could be in any of the components, and can only be found by comparing them.<br />Therefore, the algorithm cannot exist.</p>
</blockquote>
<p>An algorithm to find the minimum and the maximum element of an array can also trivially be designed, using <span class="math">\(2n - 2\)</span> comparisons and <span class="math">\(\Theta(n)\)</span> time. However, it is possible to design another algorithm that does fewer than <span class="math">\(2n - 2\)</span> comparisons (though the time complexity might be worse).</p>
<p>One way to do this is to consider elements two at a time - we compare elements one pair at a time, the larger of the pair with the maximum, and the smaller of which with the minimum. That means we need 3 comparisons per pair, or <span class="math">\(\ceil{\frac 3 2 n} - 2\)</span> in total, a ~25% improvement:</p>
<pre><code>def find_min_max(array):
    if array[0] &lt; array[1]: min, max = array[0], array[1]
    else: max, min = array[0], array[1
    for i in range(2, len(array), 2):
        if array[i] &lt; array[i + 1]: small, big = array[i], array[i + 1]
        else: big, small = array[i], array[i + 1]
        if big &gt; max: max = big
        if small &lt; min: min = small
    if len(array) % 2:
        if array[-1] &gt; max: max = array[-1]
        if array[-1] &lt; min: min = array[-1]</code></pre>
<p>It can actually be proven, using graph theory, that the number of comparisons for this algorithm is optimal. However, it's too lengthy and complicated to cover here.</p>
<p>Suppose we want to determine whether and which three elements of an array of integers sum to 0. This is pretty easy to do in <span class="math">\(O(n^3)\)</span> (specifically, <span class="math">\(n \choose 3\)</span> runs of the inner loop) simply by checking all possible triples in the array. Basically, we pick two elements <span class="math">\(A[i]\)</span> and <span class="math">\(A[j]\)</span>, then try to search for a <span class="math">\(A[k] = -(A[i] + A[j])\)</span> using linear search. However, it is actually possible to do so in <span class="math">\(O(n^2 \log n)\)</span> by sorting the array first, and searching for <span class="math">\(k\)</span> using binary search instead.</p>
<p>In fact, it's possible to do even better by sorting the array, taking each <span class="math">\(A[i]\)</span>, and searching from both ends of the array inward for <span class="math">\(j\)</span> and <span class="math">\(k\)</span> such that <span class="math">\(A[j] + A[k] = -A[i]\)</span>. After each step in the search, we either move inward from the left if <span class="math">\(A[j] + A[k] &lt; -A[i]\)</span>, or inward from the right if <span class="math">\(A[j] + A[k] &gt; -A[i]\)</span></p>
<h1 id="section-1">16/9/15</h1>
<p>The pseudocode for the above algorithm looks like the following:</p>
<pre><code>def better_3sum(array):
    array = sorted(array)
    result = []
    for i in range(n - 2):
        j, k = i + 1, n
        while j &lt; k:
            triple_sum = array[i] + array[j] + array[k]
            if triple_sum &lt; 0: j += 1
            elif triple_sum &gt; 0: k -= 1
            else:
                result.append((i, j, k))
                j += 1
                k -= 1</code></pre>
<p>Basically, this goes through each value of <span class="math">\(i\)</span>, and does an <span class="math">\(O(n)\)</span> search for two values that would make it possible to have all three sum to 0. Proving this correct is left as an exercise to the reader - prove that <span class="math">\(k - j\)</span> is monotonically decreasing, and the pairs cover all possible pairs that can possibly sum up to <span class="math">\(A[i]\)</span>. It's somewhat reminiscent of the array merge in merge sort.</p>
<p>This algorithm is <span class="math">\(O(n^2)\)</span>, since the search is <span class="math">\(O(n^2)\)</span> and the sort is <span class="math">\(O(n \log n)\)</span>. There are actually even better algorithms, and the best currently known ones are <span class="math">\(O\left(n^2 \left(\frac{\log \log n}{\log n}\right)^2\right)\)</span>.</p>
<p>A <strong>problem</strong> is a computational task. A <strong>problem instance</strong> is the input for the computational task. The <strong>problem solution</strong> is the output. The <strong>size of a problem instance</strong> is a positive integer that is a measure of the size of the instance, and depends on the problem itself. The size is usually fairly intuitive, such as the size of an array for array sum, but sometimes it gets a bit more tricky.</p>
<p>An <strong>algorithm</strong> is a high level description of a computation. An algorithm <strong>solves</strong> a problem if it finds a valid solution for any instance in finite time. A <strong>program</strong> is an implementation of an algorithm using a computer language.</p>
<h2 id="time-complexity-analysis">Time Complexity Analysis</h2>
<p>The running time of a program <span class="math">\(M\)</span> for a problem instance <span class="math">\(I\)</span> is <span class="math">\(T_M(I)\)</span>. The worst case running time for problem instances of size <span class="math">\(n\)</span> for the program is <span class="math">\(T_M(n)\)</span>. The average case running time is <span class="math">\(T_M^{avg}(n)\)</span>.</p>
<p>The <strong>worst case time complexity</strong> of an algorithm <span class="math">\(A\)</span> is <span class="math">\(f(n)\)</span> such that a program <span class="math">\(M\)</span> exists implementing <span class="math">\(A\)</span> such that <span class="math">\(T_M(n) \in \Theta(f(n))\)</span>.</p>
<p>Running time can only be found by running the program on a computer. Complexity is independent, but we lose information such as the constant factors.</p>
<p>To get the time complexity of an algorithm, we can either use <span class="math">\(\Theta(f(n))\)</span> throughout our analysis, or prove <span class="math">\(O(f(n))\)</span> and then <span class="math">\(\Omega(f(n))\)</span>, which is often easier due to being able to make more assumptions for each separate proof.</p>
<p>The complexity of a loop in code is sum of the complexity of its body over each iteration of the loop. The complexity of consecutive operations is the sum of the complexities of the operations they are composed of.</p>
<p>Prove that <span class="math">\((\ln n)^a \in o(n^b)\)</span> for any <span class="math">\(a\)</span> and <span class="math">\(b\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(L = \lim_{n \to \infty} \frac{(\ln n)^a}{n^b} \lH \lim_{n \to \infty} \frac{a (\ln n)^{a - 1}}{b n^b} \lH \ldots \lH \lim_{n \to \infty} \frac{a!}{b^a n^b} = 0\)</span>.<br />By the limit order rule, <span class="math">\((\ln n)^a \in o(n^b)\)</span>.</p>
</blockquote>
<h1 id="section-2">21/9/15</h1>
<p>Useful to know:</p>
<ul>
<li><span class="math">\(\sum_{i = 0}^{n - 1} (a + di) = na + \frac{dn(n - 1)} 2\)</span> - arithmetic sequence</li>
<li><span class="math">\(\sum_{i = 0}^{n - 1} ar^i = \begin{cases} a\frac{r^n - 1}{r - 1} &amp;\text{if } r &gt; 1 \\ na &amp;\text{if } r = 1 \\ a\frac{1 - r^n}{1 - r} &amp;\text{if } r &lt; 1 \\ \end{cases}\)</span> - geometric sequence</li>
<li><span class="math">\(\sum_{i = 0}^{n - 1} (a + di)r^i = \frac a {1 - r} - \frac{(a + (n - 1)d)r^n}{1 - r} + \frac{dr(1 - r^{n - 1})}{(1 - r)^2}\)</span> -arithmetic-geometric sequence</li>
<li><span class="math">\(\sum_{i = 1}^n \frac 1 {i^2} = \frac{\pi^2} 6 = \Theta(1)\)</span></li>
<li><span class="math">\(_n = \sum_{i = 1}^n \frac 1 i \in \Theta(\log n)\)</span> - harmonic sequence</li>
</ul>
<p>The order of <span class="math">\(n!\)</span> is <span class="math">\(\Theta(n^2 \sqrt n e^{-n})\)</span>. This is similar to Striling's approximation, <span class="math">\(\sqrt{2 \pi n} n^n e^{-n}\)</span> (this gets more and more accurate for larger values).</p>
<p>For order notation, the base of logarithmic terms don't matter because <span class="math">\(\log n = \frac 1 {\log_b 10} \log_b n\)</span>, and <span class="math">\(\frac 1 {\log_b 10}\)</span> is a constant factor.</p>
<p><strong>Polynomial growth rates</strong> are those in <span class="math">\(O(n^k), k \in \mb{R}\)</span>. For example, the best known algorithm for graph isomorphism is <span class="math">\(n^{\sqrt n \log_2 n}\)</span>, so it is not polynomial.</p>
<h1 id="section-3">23/9/15</h1>
<p>Consider the following program:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> useless(n):
    <span class="kw">for</span> i in <span class="dt">range</span>(n):
        <span class="kw">while</span> j &gt;= <span class="dv">1</span>:
            j /= <span class="dv">2</span></code></pre>
<p>Clearly, each iteration of the inner loop will run <span class="math">\(\Theta(\log n)\)</span> times, so the overall time complexity is <span class="math">\(\sum_{i = 1}^n \Theta(\log i) = \Theta(\sum_{i = 1}^n \log i) = \Theta(\log(\prod_{i = 1}^n i)) = \Theta(\log(n!)) = \Theta(n \log n)\)</span>.</p>
<p>A <strong>recurrence relation</strong> specifies <span class="math">\(a_n\)</span> in the infinite sequence of real numbers <span class="math">\(a_1, a_2, a_3, \ldots\)</span> in terms of the previous terms <span class="math">\(a_1, \ldots, a_{n - 1}\)</span> (<span class="math">\(a_1\)</span> must therefore be a constant - the <strong>initial value</strong>). A <strong>solution</strong> to a recurrence relation is a closed form formula for <span class="math">\(a_n\)</span> - one that does not depend on the previous values of <span class="math">\(a_1, \ldots, a_{n - 1}\)</span>. These are generally solved using techniques like guess and check or recursion trees, but there aren't any methods that are guaranteed to solve all recurrences, and in fact, reurrences may not necessarily even have solutions.</p>
<p>Recurrence relations can also be written using function notation, like <span class="math">\(T(1) = 2, T(n) = T(n - 1) + 1\)</span>.</p>
<p>The <strong>guess and check</strong> method involves computing enough <span class="math">\(a_n\)</span> instances to guess the form of the solution (without any constants). Then, we solve for the constants using the computed values, and check if our solution is correct - if it is, we must prove it is correct using induction, and if not, we start over with a different guess.</p>
<p>Solve <span class="math">\(a_0 = 4, a_n = a_{n - 1} + 6n - 5\)</span>:</p>
<blockquote>
<p>Guess and check: the first few elements of the sequence are <span class="math">\(a_0 = 4, a_1 = 5, a_2 = 12, a_3 = 25, a_4 = 44\)</span>.<br />This seems to be quadratic growth, so we guess <span class="math">\(an^2 + bn + c, a, b, c \in \mb{R}\)</span> as the solution.<br />Solving for <span class="math">\(a, b, c\)</span>, <span class="math">\(c = 4, a + b + c = 5, 4a + 2b + c = 12\)</span>, we get <span class="math">\(a = 3, b = -2, c = 4\)</span>.<br />So we guess that <span class="math">\(a_n = 3n^2 - 2n + 4\)</span>. We can verify this for <span class="math">\(0 \le n \le 4\)</span> as our inductive base case.<br />Assume for some <span class="math">\(k \in \mb{N}\)</span> that <span class="math">\(a_k = 3k^2 - 2k + 4\)</span>.<br />Clearly, <span class="math">\(a_{k + 1} = 3k^2 - 2k + 4 + 6(k + 1) - 5 = 3(k + 1)^2 - 2(k + 1) + 4\)</span>.<br />So by induction, <span class="math">\(a_n = 3n^2 - 2n + 4\)</span>.</p>
</blockquote>
<p>The <strong>recurrence tree</strong> method is often used for divide-and-conquer algorithms. This technique involves building a call tree for the recurrence. then summing up the results of the function on each level of the tree.</p>
<p>To construct a recurrence tree, we start with the timing function <span class="math">\(T(n)\)</span>, then add children for each recursive call. The recursive calls are then removed from the parent. This is repeated for the children, all the way until we reah the base case. Note that after each step of growing the tree, the sum of all the nodes are always equivalent to <span class="math">\(T(n)\)</span>.</p>
<p>Then, we sum the nodes at each level in the tree. The sum of all these sums is equal to <span class="math">\(T(n)\)</span>.</p>
<p>Use the recurrence tree method to find the running time of merge sort:</p>
<blockquote>
<p>Clearly, <span class="math">\(T(n) = \begin{cases} 2T\left(\frac n 2\left) + cn &amp;\text{if } n &gt; 1 \wedge n \text{ is a power of 2} \\ d &amp;\text{if } n = 1 \\ \end{cases}\)</span> where <span class="math">\(c, d\)</span> are constants.<br />Construct a recursion tree: start with the root node <span class="math">\(N\)</span> with value <span class="math">\(T(n)\)</span>, then add 2 children, <span class="math">\(N_1, N_2\)</span>, both <span class="math">\(T\left(\frac n 2\left)\)</span>, and replace <span class="math">\(N\)</span>'s value with <span class="math">\(cn\)</span>.<br />What we get is a binary tree with every internal node having 2 children, and value <span class="math">\(c2^i\)</span> (where <span class="math">\(i\)</span> is the level in the tree, where 0 is the bottommost level, the leaf nodes), and each leaf node has value <span class="math">\(d\)</span>.<br />Let <span class="math">\(n = 2^j\)</span>. Then at the top level each node has value <span class="math">\(c2^j\)</span>, at the second level <span class="math">\(c2^{j - 1}\)</span>, at the third level <span class="math">\(c2^{j - 2}\)</span>, and so on. All leaf nodes must have value <span class="math">\(d\)</span>, since it is the only base case.<br />Clearly, the height of the tree must then be <span class="math">\(j + 1\)</span>, with <span class="math">\(j\)</span> interior levels. Clearly, at the bottom level there are <span class="math">\(2^j\)</span> leaf nodes, since each level <span class="math">\(i\)</span> has <span class="math">\(2^{j - i}\)</span> nodes. The sum of the bottom level is therefore <span class="math">\(2^j d = dn\)</span>.<br />Clearly, at each internal level <span class="math">\(i\)</span> each node has value <span class="math">\(c 2^i\)</span>. Since there are <span class="math">\(2^{j - i}\)</span> nodes, the sum of each interior level is <span class="math">\(c2^{j - i}2^i = c2^j = cn\)</span>.<br />So <span class="math">\(T(n) = dn + jcn = dn + cn \log_2 n = \Theta(n \log n)\)</span>.</p>
</blockquote>
<p>The <strong>master thoerem</strong> is a generalized formula for solving certain forms of recurrences. One thing it says is that given <span class="math">\(T(n) = aT( \frac n b) + \Theta(n^y)\)</span> where <span class="math">\(a \ge 1, b &gt; 1\)</span>, and <span class="math">\(n\)</span> is a power of <span class="math">\(b\)</span>, then <span class="math">\(T(n) \in \begin{cases} \Theta(n^x) &amp;\text{if } y &lt; x \\ \Theta(n^x \log n) &amp;\text{if } y = x \\ \Theta(n^y) &amp;\text{if } y &gt; x \\ \end{cases}\)</span> where <span class="math">\(x = \log_b a\)</span>.</p>
<p>Also, the exact value is <span class="math">\(T(n) = da^j + cn^y \sum_{i = 0}^{j - 1} \left(\frac a {b^y}\right)^i\)</span>, or more simply <span class="math">\(T(n) = dn^x + cn^y \sum_{i = 0}^{j - 1} r^i\)</span> where <span class="math">\(x = \log_b a\)</span> and <span class="math">\(r = \frac a {b^y} = b^{x - y}\)</span>.</p>
<p>This can be proved by constructing tree, figuring out the geometric sequence for the tree sums, and then solving for its value for each of the three cases.</p>
<p>A more general version, which will not be proved here, is that given <span class="math">\(T(n) = aT(\frac n b) + f(n)\)</span> where <span class="math">\(a \ge 1, b &gt; 1\)</span>, and <span class="math">\(n\)</span> is a power of <span class="math">\(b\)</span>, then <span class="math">\(T(n) \in \begin{cases} \Theta(n^x) &amp;\text{if } f(n) \in O(n^{x - \epsilon}) \text{ for some } \epsilon &gt; 0 \\ \Theta(n^x \log n) &amp;\text{if } f(n) \in O(n^x) \\ \Theta(f(n)) &amp;\text{if } \frac{f(n)}{n^{x + \epsilon}} \text{ is an incresing function of } n \text{ for some } \epsilon &gt; 0 \\ \end{cases}\)</span> where <span class="math">\(x = \log_b a\)</span>.</p>
<h1 id="section-4">28/9/15</h1>
<p>Solve <span class="math">\(T(1) = 1, T(n) = 3T(\frac n 4) + n \log n\)</span>:</p>
<blockquote>
<p>Let <span class="math">\(a = 3, b = 4, f(n) = n \log n\)</span>, so <span class="math">\(x = \log_4 3\)</span> and <span class="math">\(f(n) / n^{x + \epsilon}\)</span> is an increasing function of <span class="math">\(n\)</span> for <span class="math">\(\epsilon = 1 - \log_4 3\)</span> since <span class="math">\(f(n) / n^{x + \epsilon} = \log n\)</span>.<br />So <span class="math">\(T(n) \in \Theta(n \log n)\)</span>.</p>
</blockquote>
<p>The main task when using the master theorem o prove time complexities is choosing the right <span class="math">\(\epsilon\)</span> to make one of the cases valid. For example, the master theorem cannot be used for something like <span class="math">\(T(1) = 1, T(n) = 2T(\frac n 2) + n \log n\)</span>, since none of the cases apply. Instead, we can use the recurrence tree method to solve the recurrence directly:</p>
<blockquote>
<p>Each level <span class="math">\(i\)</span> (from the bottom) of the tree has <span class="math">\(2^i\)</span> nodes, each with value <span class="math">\(2^{j - i}(j - i)\)</span> where <span class="math">\(n = 2^j\)</span>.<br />So at each level, the sum is <span class="math">\(2^i 2^{j - i} (j - i) = (j - i) 2^j\)</span>.<br />So the sum of the tree is <span class="math">\(\sum_{i = 0}^j (j - i) 2^j = j^2 2^j - 2^j \sum_{i = 0}^j i = 2^j\left(j^2 - \frac{j(j + 1)}{2}\right) = \Theta(n \log^2 n)\)</span>.</p>
</blockquote>
<h2 id="divide-and-conquer">Divide and Conquer</h2>
<p>Divide and conquer is a design strategy for algorithms where we divide a problem into smaller subproblems, solve those individually, then combine them back together to get a good result. Formally:</p>
<ul>
<li>Divide the problem instance <span class="math">\(I\)</span> into smaller subproblems <span class="math">\(I_1, \ldots, I_n\)</span>.</li>
<li>Solve <span class="math">\(I_1, \ldots, I_n\)</span> recursively to get sollutions <span class="math">\(S_1, \ldots, S_n\)</span>.</li>
<li>Use <span class="math">\(S_1, \ldots, S_n\)</span> to compute <span class="math">\(S\)</span> - combine the results. This is usually the hardest step.</li>
</ul>
<p>For example, mergesort splits an array into two subarrays, and then recursively sorts those. Then, the merge is performed to combine the two sorted subarrays.</p>
<p>The time complexity of this is <span class="math">\(T(n) = \begin(cases) T(\ceil{\frac n 2}) + T(\floor{\frac n 2}) + cn &amp;\text(if } n &gt; 1 \\ d &amp;\text(if } n = 1 \\ \end{cases}\)</span> for some constants <span class="math">\(c, d\)</span>. This is called the <strong>exact recurrence</strong>. If we remove the ceilings and floors, we get a <strong>sloppy recurrence</strong> <span class="math">\(T(n) = \begin(cases) 2T(\frac n 2) + cn &amp;\text(if } n &gt; 1 \\ d &amp;\text(if } n = 1 \\ \end{cases}\)</span>, which only works the same when <span class="math">\(n\)</span> is a power of 2, but is a lot simpler to work with.</p>
<p>The master theorem can prove that mergesort is <span class="math">\(\Theta(n \log n)\)</span> for the sloppy recurrence, and therefore only for when <span class="math">\(n\)</span> is a pwoer of 2. However, if we want to get the time complexity for all <span class="math">\(n\)</span>, we need to use induction on the exact recurrence.</p>
<h1 id="section-5">30/9/15</h1>
<h3 id="non-dominated-points">Non-dominated points</h3>
<p>Suppose we have a set of 2D coordinates/points. One point <strong>dominates</strong> another if it has a greater or equal X and Y coordinate than the other. We want to find those points that are non-dominated - the points that are not dominated by any other point. The non-dominated points form a sort of boundary below which all the dominated points lie.</p>
<p>Clearly, this can easily be solved simply by checking every pair of points to see which dominate which, but this would take <span class="math">\(\Theta(n^2)\)</span> time.</p>
<p>A better solution is to sort the points by their X coordinate, and then apply divide and conquer.</p>
<p>Dividing larger sets of points <span class="math">\(S\)</span> is also easy - we can just split the sorted points into two sets <span class="math">\(S_1, S_2\)</span>, and then recursively solve the problem. It is the combining step that is the difficult step.</p>
<p>Given a point and the highest point on its right, it is easy to determine whether it is dominated. This is our base case</p>
<p>Clearly, no point in <span class="math">\(S_1\)</span> can dominate a point in <span class="math">\(S_2\)</span>, since the X coordinates of points in <span class="math">\(S_1\)</span> is strictly less than points in <span class="math">\(S_2\)</span>. So we find the point in <span class="math">\(S_2\)</span> with the highest Y coordinate, and all points in <span class="math">\(S_1\)</span> with Y coordinate at or below this value are guaranteed to be dominated by at least one point in <span class="math">\(S_2\)</span>. In this way, we can eliminate all the points in <span class="math">\(S_1\)</span> that are dominated by points in <span class="math">\(S_2\)</span> in <span class="math">\(O(n)\)</span> time - this is the combining step.</p>
<p>In code, this looks like:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> non_dominated(points):
    points = <span class="dt">sorted</span>(points, key=<span class="kw">lambda</span> p: p.x)
    <span class="kw">if</span> <span class="dt">len</span>(points) == <span class="dv">1</span>: <span class="kw">return</span> {points[<span class="dv">0</span>]}
    pivot = floor(<span class="dt">len</span>(points) / <span class="dv">2</span>)
    left = non_dominated(points[:pivot])
    right = non_dominated(points[pivot + <span class="dv">1</span>:])
    <span class="kw">while</span> i &lt; <span class="dt">len</span>(left) and left[i].y &gt; right[<span class="dv">1</span>].y:
        i += <span class="dv">1</span></code></pre>
<p>Clearly, the time complexity of this algorithm is <span class="math">\(T(n) = T\left(\floor{\frac n 2}\right) + T\left(\ceil{\frac n 2}\right) = O(n \log n)\)</span> (the sloppy version of this can be proven using the master theorem).</p>
<p>Without divide and conquer, we could also just sort the list, and then iterate though it from right to left, keeping track of the largest Y axis coordinate seen so far and eliminating points at or below this Y coordinate:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> non_dominated(points):
    largest = <span class="dt">float</span>(<span class="st">&quot;-inf&quot;</span>)
    <span class="kw">for</span> point in <span class="dt">reversed</span>(<span class="dt">sorted</span>(points, key=<span class="kw">lambda</span> p: p.x)):
        <span class="kw">if</span> point.y &lt;= largest:
            <span class="kw">yield</span> point
        <span class="kw">else</span>:
            largest = point.y</code></pre>
<h3 id="closest-pair">Closest pair</h3>
<p>Suppose we have a set <span class="math">\(Q\)</span> of <span class="math">\(n\)</span> distinct 2D points in Euclidean space. We want to find the two points that are closest together - the points <span class="math">\(u, v\)</span> for which <span class="math">\(\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}\)</span> is minimised.</p>
<p>Clearly, minimising <span class="math">\(\sqrt{(u_x - v_x)^2 + (u_y - v_y)^2}\)</span> is the same as minimising <span class="math">\((u_x - v_x)^2 + (u_y - v_y)^2\)</span>. We will again sort the points in <span class="math">\(Q\)</span> by X coordinate, taking <span class="math">\(\Theta(n \log n)\)</span> time.</p>
<p>The dividing step is again trivial to get two partitions <span class="math">\(Q_1, Q_2\)</span>, but the combining step takes some work.</p>
<p>If we are given the distance <span class="math">\(\delta\)</span> between the closest pair of points seen so far in the left and right partitions, and a point <span class="math">\(p\)</span>, we know that if this point is in the closest pair of points, the other point must have X coordinate within <span class="math">\(\delta\)</span> of this point's X coordinate. Therefore, the other point must be in the <strong>critical strip</strong> - a section of the plane from <span class="math">\(p_x - \delta\)</span> to <span class="math">\(p_x + \delta\)</span> and extending infinitely both up and down.</p>
<p>We now want to find the closest pair between the left and right partitions, if it is less than the .</p>
<p>There is also a lemma that says that if a critical strip <span class="math">\(R\)</span> of <span class="math">\(Q\)</span> is sorted by Y coordinate, and <span class="math">\(R[j]\)</span> and <span class="math">\(R[k]\)</span> have distance less than <span class="math">\(\delta\)</span> where <span class="math">\(j &lt; k\)</span>, then <span class="math">\(k \le j + 7\)</span>. Here, <span class="math">\(\delta\)</span> is the smaller of the minimum distance between points in <span class="math">\(Q_1\)</span>, and the minimum distance between points in <span class="math">\(Q_2\)</span>. This can be proven by dividing the strip around the point as squares of side length <span class="math">\(\frac \delta 2\)</span>, and then showing that no two points can be in the same square. In other words, within a critical strip and given <span class="math">\(\delta\)</span>, the point closest to a given point can be determined in 7 comparisons or less.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> closest_pair(points, start, end):
    <span class="kw">if</span> start == end: <span class="kw">return</span> <span class="dt">float</span>(<span class="st">&quot;inf&quot;</span>)
    pivot = floor((left + right) / <span class="dv">2</span>)
    closest_left = closest_pair(points, left, pivot)
    closest_right = closest_pair(points, pivot + <span class="dv">1</span>, right)
    closest = <span class="dt">min</span>(closest_left, closest_right)
    candidates = select_candidates(left, right, closest, points[pivot].x) <span class="co"># find points in the critical strip</span>
    candidates = <span class="dt">sorted</span>(candidates, key=<span class="kw">lambda</span> p: p.y)
    closest = check_candidates(candidates, closest)
    <span class="kw">return</span> closest</code></pre>
<p>The time complexity of this is <span class="math">\(O(n \log n \log n)\)</span>. We can improve this by eliminating the Y coordinate sort of the candidates. One of them is to sort all the points <span class="math">\(Q\)</span> by Y coordinate to begin with, and then use that list when checking candidates, and then adding the necessary checks that make it work. Alternatively, we could replace the Y coordinate sort with a merge, and maintaining the candidates in sorted order.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> closest_pair(points, start, end):
    <span class="kw">if</span> start == end: <span class="kw">return</span> <span class="dt">float</span>(<span class="st">&quot;inf&quot;</span>)
    pivot = floor((left + right) / <span class="dv">2</span>)
    closest_left = closest_pair(points, left, pivot)
    closest_right = closest_pair(points, pivot + <span class="dv">1</span>, right)
    closest = <span class="dt">min</span>(closest_left, closest_right)
    merge_in_place(points, start, pivot, end)
    candidates = select_candidates(left, right, closest, points[pivot].x) <span class="co"># find points in the critical strip</span>
    closest = check_candidates(candidates, closest)
    <span class="kw">return</span> closest</code></pre>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>