CS450
=====

Computer Architecture.

    Andrew Morton
    Section 001
    Email: andrew.morton@uwaterloo.ca
    Website: https://www.student.cs.uwaterloo.ca/~cs450/w17/outline.shtml
    Office Hours: Tuesdays/Thursdays 11:30am-12:30pm in EIT-4015
    Tuesdays/Thursdays 1:00pm-2:20pm

# 3/1/16

In this course, the main project (worth 30%) is to design a MIPS processor in Verilog. No required textbooks. The main topic is about how modern CPUs manage to achieve the performance that they do, building on top of the processor basics covered in earlier processor design courses.

There are also two written assignments (worth 20% total) and the final exam (worth 50%). Assignments have 4 grace days allowed.

Hardware description languages are used for modelling hardware, generally for the purpose of simulation or synthesis. Most complex digital hardware is designed using a hardware description language, which is then compiled and converted into specifications for FPGAs, ASICs, programmable logic, and so on.

The main hardware description languages in use today are System Verilog and VHDL. VHDL is more verbose and Ada-like, while Verilog is more minimalist and C-like. Most projects in industry use Verilog for this reason. System Verilog is a superset of plain Verilog, and all Verilog code works in System Verilog. There's also System C, which looks a lot more like C/C++, but it's not commonly used. In this course, we will be using Icarus Verilog (an implementation of System Verilog), version 10 or later (in order to get support for SystemVerilog).

The simplest/lowest-level of hardware modelling in System Verilog is **gate-level modelling**. At this level, we have logic gate primitives like AND/OR, and we can connect them together into circuits via **wires**:

```verilog
# `a1` is the name of the gate, which is optional but useful for labelling gates
# `y` is the gate output, and `a`/`b`/`c` are the gate inputs (`and` supports one or more of these inputs, and one output)
# the inputs and outputs are wires, identified by name - everywhere that same name is used can be thought of as being all connected together by a physical wire
and a1(y, a, b, c)

# every argument except the last are the NOT gate's outputs, and the last one is the input
not (y1, y2, a)
```

Gate-level modelling
--------------------

In gate-level modelling, signal/wire values can be `0` (low), `1` (high), `x`/`X` (unknown/ignored value), or `z`/`Z` (high impedance).

Gate-level modelling for a full adder (addend bits as `a`/`b`, carry-in as `c_in`, carry-out as `c_out`, sum as `sum`):

```verilog
// full_adder_gate_level_modelling.sv
// for a full adder, `sum = a XOR b XOR c_in`, and `c_out = ((a XOR b) AND c_in) OR a AND b`

module full_adder(input a, b, c_in, output c_out, sum);
    // a `logic` is basically a connection/wire - here, we're declaring our wires
    // a `wire` is equivalent to a `wire logic`, which is a logic that supports multiple drivers
    // generally, we just use logic unless we need multiple drivers - the compiler will choose the best structure depending on how they're used
    // there are also `bit` and `byte`, which are basically logics that can only be 0 or 1, not X or Z
    logic w1, w2, w3; // these are wires that are used for connecting gates in the full adder together
    xor x1(w1, a, b);
    xor x2(sum, w1, c_in);
    and a1(w2, a, b);
    and a2(w3, w1, c_in);
    or o1(c_out, w2, w3);
endmodule
```

This is a **design module**. It fully specifies the full adder as a self-contained unit.

While this looks like normal procedural code, with various function calls, it is more intuitive to think of this as a series of logical declarations - as if each of the `xor`/`and`/`or` lines were all continuously running at the same time.

A **test bench** is a set of definitions that instantiates the design module, and simulates the circuit under different input conditions to test for functional correctness. For our full adder example:

```verilog
// full_adder_test_bench.sv
module full_adder_test_bench;
    logic carry_in, x, y;
    logic carry_out, z;

    // "dut" stands for "device under test"
    // below, we're basically instantiating a full adder (defined in the code above) that's connected to our logics, for testing purposes
    // the `.name(other_name)` syntax is a "named port map", which lets us connect logics to devices by signal name
    // alternatively, we can just map the signals in order, like `full_adder dut(x, y, carry_in, carry_out, z)`, but this can cause issues if we change the inputs/outputs of the full adder
    full_adder dut(.c_out(carry_out), .sum(z), .c_in(carry_in), .a(x), .b(y));

    // this is a procedural block - inside, the statements are evaluated sequentially from top to bottom
    initial begin // run once at time 0
        carry_in = 0; x = 0; y = 0;
        // delays must be used to ensure statements actually run sequentially - statements in between delays all execute in parallel
        #10 x = 1; // delay 10 time units, then continue and set x to 1
        #10 y = 1; // delay 10 time units, then continue and set y to 1
        #10 carry_in = 1; // delay 10 time units, then continue and set carry_in to 1
        #10 $stop; // delay 10 time units, then stop the simulation (this will not compile if trying to generate hardware from this)
    end

    // now we want to dump the signals out into waveforms so we can see them
    initial $dumpvars(0, full_adder_test_bench); // the first parameter is the dump level (`0` means "dump all variables in the module and in the modules that instantiate it"), and the rest of the parameters are the module or variables to dump
endmodule
```

To compile, run `iverilog -g2005-sv -s full_adder_test_bench.sv -o full_adder.vvp full_adder.sv full_adder_test_bench.sv`:

* `iverilog` is the Icarus Verilog compiler.
* `-g2005-sv` specifies that the compiler should use the 2005 System Verilog standard.
* `-s full_adder_test_bench.sv` specifies the top-level/root module.
* `-o full_adder.vvp` specifies the output for the compiled Verilog.
* `full_adder.sv full_adder_test_bench.sv` specifies the modules to include in the compilation.

To simulate, run `vvp -n full_adder.vvp -lx2`:

* `vvp` is the Icarus Verilog simulation tool.
* `-n` specifies that `$stop` and Control-C should complete the simulation.
* `full_adder.vvp` is the compiled Verilog file for the full adder.
* `-lx2` specifies that the simulation should be written in LXT2 format, which is slower to process but more compact allows viewing waveforms while the simulation is running.

To view the waveforms: `gtkwave dump.lx2`:

* `gtkwave` is the GTK waveform visualizer tool.
* `dump.lx2` is the simulation result generated by `vvp`.

# 5/1/17

Live demonstration of the above Verilog workflow, including an overview of GtkWave's features and a shell script that builds and simulates the module.

**Heirarchical design** is the idea of using lower-level modules in higher-level modules. For example, a carry-lookahead adder module might instantiate a few ripple-carry adders, which in turn might instantiate a bunch of full-adders:

```verilog
// adder_4_bit.sv
module adder_4_bit(output c_out, output [3:0] sum, input c_in, input [3:0] a, b);
    logic [3:1] carry;
    full_adder fa0(.c_out(carry[1]), .sum(sum[0]), .c_in(c_in), .a(a[0]), .b(b[0]));
    full_adder fa0(.c_out(carry[2]), .sum(sum[1]), .c_in(carry[1]), .a(a[1]), .b(b[1]));
    full_adder fa0(.c_out(carry[3]), .sum(sum[2]), .c_in(carry[2]), .a(a[2]), .b(b[2]));
    full_adder fa0(.c_out(c_out), .sum(sum[3]), .c_in(carry[3]), .a(a[3]), .b(b[3]));
endmodule
```

Note that there's a lot of similar statements for the full adder definitions. In these cases, we can use generate statements to remove duplicated code. It looks somewhat like a loop, but it's more of a macro that gets expanded into multiple statements at compile time:

```verilog
// adder_4_bit_generate.sv
module adder_4_bit(output c_out, output [3:0] sum, input c_in, input [3:0] a, b);
    logic [4:0] carry;
    
    generate
        genvar i;
        for(i=0; i<4; i=i+1) begin: adder
            full_adder fa(.c_out(carry[i+1]), .sum(sum[i]), .c_in(carry[i]) ,.a(a[i]), .b(b[i]));
        end
    endgenerate

    // since it would be really inconvenient to specify our c_in and c_out in the generate statement, we just used extra indices in the carry wire, and then assign the inputs/outputs accordingly
    assign carry[0] = c_in; // this is a continuous assignment statement - it's sort of like a one-way wire for the c_in signal in this case, but it can also do things like combinational logic with different gates and stuff
    assign c_out = carry[4]; // since the assignment is unidirectional, the inputs are on the right side of the equal sign, and the outputs are on the left side
endmodule
```

Usually for loops are discouraged, but they're fine for generate statements - in other contexts, they might not compile to actual hardware.

Dataflow modelling
------------------

Dataflow modelling is the next level of abstraction above gate-level modelling. Dataflow modelling deals only with combinational circuits, by representing them with continuous assignment statements.

Basically, we represent what logical operations the circuit needs to do, and the compiler will figure out what kind of gate layout we need. For example, for a full adder it might automatically decide to reuse the `xor(a, b)` gate.

For example, we might model a full adder using dataflow modelling as follows:

```verilog
// full_adder_dataflow.sv
module full_adder(output c_out, sum, input c_in, a, b);
    // the value of `sum` is continuously updated based on the values of `a`, `b`, and `c_in` - whenever any of those change, `sum` reflects the new value
    // this updating is not true of all continuous assignment statements, however
    assign sum = a ^ b ^ c_in;
    assign c_out = ((a ^ b) & c_in) | (a & b);

    // actually, Verilog has a `+` operator, so it's really as simple as `assign {c_out, sum} = a + b + c_in`
    // `assign` will truncate the result of evaluating the right hand side, if the right hand side has more bits than the left
    // `assign` will zero-extend the result of evaluating the right hand side, if the right hand side has fewer bits than the left
endmodule
```

Some of the available operators are `!` (logical NOT), `&&` (logical AND), `||` (logical OR), `+` (two's complement addition), `-` (two's complement subtraction), `*` (;wip), `/` (;wip), `%` (;wip), `**` (;wip), `<`, `>`, `<=`, `>=`, `==`, `!=`, `~` (NOT), `&` (AND), `|` (OR), `^` (XOR), `^~` (XNOR), `<<` (bit shift left), `>>` (bit shift right), `<<<` (;wip), `>>>` (signed shift right - extends sign bit), `{a_1, ..., a_n}` (concatenate `a_1` through to `a_n`), `{n{a}}` (replicate `a` by `n` times - repeatedly concatenate `a`), `c?a:b` (MUX - select `a` if `c` is high, `b` otherwise). They generally have precedence similar to their equivalents in C and Python.

For example, the nibbles of a byte can be swapped with `assign {b[3:0], b[7:4]} = a[7:0]`, and an 8-bit signed integer can be sign-extended to 16 bits with `assign b[15:0] = {{8{a[7]}}, a[7:0]}`.

With all these useful operators, it's pretty straightforward to make a 4-bit adder:

```verilog
// full_adder_dataflow.sv
module full_adder(output c_out, output [3:0] sum, input c_in, input [3:0] a, b);
    assign {c_out, sum} = a + b + c_in`
endmodule
```

Behaviour modelling
-------------------

Behaviour modelling is the next level of abstraction above dataflow-level modelling. It uses procedural blocks that are `initial`.

An `initial` block runs the statements inside of it once starting at time 0. An `always` block runs the statements inside of it whenever certain things change.

```verilog
initial // run once at time 0
```

Here's a full adder implemented with behaviour modelling:

```verilog
module full_adder(input c_in, a, b, output logic c_out, sum);
    // the `@ (c_in, a, b)` specifies the "sensitivity list", which means the procedural block should be reevaluated whenever any of the logics in the list change
    // in contrast, the `initial` statement doesn't have a sensitivity list because it simply runs once at the beginning
    // the left hand sides of each assignment must be a `logic` or `reg` - that's why the outputs of this module are declared as `logic`
    // we could also have written `always @ *` to make it re-evaluate the block whenever any of the signals on the right hand sides of any assignment statement changes
    always @ (c_in, a, b) begin
        sum = a ^ b ^ c_in;
        c_out = ((a ^ b) & c_in) | (a & b);
    end
```

Alternatively:

```verilog
module full_adder(input c_in, a, b, output logic c_out, sum);
    always @ (c_in, a, b) begin
        {c_out, sum}  = a + b + c_in;
    end
```

# 10/1/17

First assignment is posted, it shouldn't take more than 5 minutes to complete.

For behaviour modelling, we have a lot of useful SystemVerilog constructs:

* if-else: `if (expression) statement1 else statement2`
* case-swtich: `case (expression) expression1: statement1; expression2: statement2; default: statement4 endcase` (default case is optional)
* for/while/repeat/forever: `repeat(10) begin statements end` (avoid these if at all possible - they aren't always sythesizable, only usable in simulations).

Note that with if statements and other conditionals, if we don't explicitly set the value of a logic, then it retains its current value.

Let's implement a mux:

```verilog
module mux_2_input(output logic d, input i0, i1, select);
    always @ *
        case (select)
            0: d = i0;
            1: d = i1;
            default: d = z; // the default case handles unknown and high impedance - we'll just turn the output off (high impedance)
```

Synchronous Circuits
--------------------

Recall that a synchronous circuit is one that only updates on a clock edge. In this course our synchronous circuits are updated on the rising edge of the clock (CMOS is actually clocked on the falling edge).

A test bench for a synchronous circuit needs to generate a clock signal. This might look like the following:

```verilog
module synchronous_test_bench;
    logic clk;
    initial begin
        clk = 1;
        forever #5 clk = ~clk;
    end
endmodule
```

Blocks can be triggered on edge events, like `always @ (posedge clk)` (trigger on rising edge) or `always @ (negedge clk)`.

SystemVerilog actually has two assignment operators. `=` is a blocking assignment, where all updates happen in order (we use this for combinational logic). `<=` is non-blocking assignment, where all the right hand sides are computed before the updates happen (this is used for register-like or latched output). For example, `a <= b; b <= a` swaps `a` and `b`, while `a = b; b = a` just sets `a` to `b`. Generally, we prefer `<=` over `=` when possible, so everywhere outside of combinational logic (we generally want outputs to all update at once, at the end of an update).

System Verilog uses **discrete event simulation**. Simulations consist of concurrent processes, primitives, modules, procedural blocks, and continuous assignments. Changes to signals are **update events**, and processes respond to events they are sensitive to and add them to a process-specific event queue, which are then executed.

System Verilog lets us define constants with `parameter some_constant = 8`. This is often used to define how many bits a particular element is. There's also enumerations, like `enum bit[1:0] {IDLE, REQUEST, WAIT, RECEIVE} state`. Logics can also be defined as arrays, like `logic [7:0] some_array [0:3];` (array of 4 8-bit registers), and are accessed with `y = a[2]` and so on.

System Verilog has various "system tasks", which trigger actions during simulations. For example, `$display(...)` outputs to console, `$monitor(...)` outputs to console whenever the given signals are updated, and `$stop` stops the simulation.

# 12/1/17

Complex number multiplier:

```verilog
// recall that given two complex numbers $a$ and $b$, $p = a \times b = (\Re(a)\Re(b) - \Im(a)\Im(b)) + \imag (\Re(a)\Im(b) + \Im(a)\Re(b))$
// this needs 4 multiplications, 1 subtract, and 1 add
// we will use a synchronous circuit with one multiplier and one adder/subtractor to do this, in order to demonstrate synchronous design, and also to save space (at the expense of it being slower)
// therefore, we break the operation into 5 steps, each one only using one multiplication or one addition: `pp1 <= a_real * b_real`, `pp2 <= a_imag * b_imag`, `p_real <= pp1 - pp2`, `pp1 <= a_real * b_imag`, `pp2 <= a_imag * b_real`, and `p_imag = pp1 + pp2`, which we can implement with an FSM
module complex_multiplier(
    output logic signed [7:-24] p_r, p_i, // this defines `pr_` and `p_i` as fixed-point numbers with 24 bits to the right of the decimal point, and 7 bits to the left (actual value is the value of `p_r`/`p_i` as a two's complement integer, divided by $2^{24}$)
    input signed [3:-12] a_r, a_i, b_r, b_i,
    input clk, reset, go // the clock input, reset signal, and the trigger that starts the multiplication
);
    logic a_sel, b_sel, pp1_en, pp2_en, sub, p_r_en, p_i_en;
    logic signed [3:-12] a_operand, b_operand;
    logic signed [7:-24] pp, sum; // partial product, sum
    logic signed [7:-24] pp1, pp2;
    
    // multiplier with two multiplexers for input, to select inputs between real/imaginary components of `a` and `b`
    assign a_operand = ~a_sel ? a_r : a_i;
    assign b_operand = ~b_sel ? b_r : b_i;
    assign pp = a_operand * b_operand;

    // update pp1 and pp2 registers on clock edge
    always @ (posedge clk)
        if (pp1_en) pp1 <= pp;
    always @ (posedge clk)
        if (pp2_en) pp2 <= pp;
    
    assign sum = ~sub ? pp1 + pp2 : pp1 - -pp2;
    
    // update output registers on clock edge
    always @ (posedge clk)
        if (p_r_en) p_r <= sum;
    always @ (posedge clk)
        if (p_i_en) p_i <= sum;
    
    // finite state machine to drive the above hardware
    enum bit [2:0] { step0, step1, step2, step3, step4 } current_state, next_state;
    always @ (posedge clk or posedge reset)
        if (reset) current_state <= step0;
        else current_state <= next_state;
    always @ * begin // update whenever go or current_state change
        case (current_state)
            step0: next_state = go ? step1 : step0;
            step1: next_state = step2;
            step2: next_state = step3;
            step3: next_state = step4;
            step4: next_state = step0;
        endcase
    end
    always @ (current_state, reset) begin
        a_sel; b_sel = 0; pp1_en = 0; pp2_en = 0;
        sub = 0; p_r_en = 0; p_i_en = 0;
        case (current_state)
            step0: begin pp1_en = 0; end
            step1: begin a_sel = 1; b_sel = 1; pp2_en = 1; end
            step2: begin b_sel = 1; pp1_en = 1; sub = 1; sub = 1; p_r_en = 1; end
            step3: begin a_sel = 1; pp2_en = 1; end
            step4: begin p_i_en = 1; end
        endcase
    end
endmodule
```

Verilog has sized literals for numbers. For example, `16'h0001` means a 16-bit hexadecimal value 1, `1'b1` means the 1-bit binary value 1, and `4'd8` means the 4-bit decimal value 8.

Overview of assignment 1. Assignment involves filling out Verilog module stubs and writing test benches.

# 17/1/17

Room has been moved to HH 1102, which should be a lot closer to campus than OPT.

Processor Design
----------------

There are multiple levels of abstraction when talking about a processor design. From most abstract to least abstract, the main ones are:

* Instruction-set architecture (ISA) - defines functional behaviour like instruction format, addressing modes, hazards, registers, privileges, exceptions, etc. For example, IA32 (also known as x86), IA64/AMD64 (also known as x86-64), PowerPC, MIPS, ARM, RISC-V.
* Microarchitecture - defines the hardware structure like pipeline structure, cache organization, branch prediction schemes. For example, Intel i386, Intel i486, Intel Skylake, MIPS R16000.
* Implementation - defines physical realization like gate design, transistor technology, fabrication technology.

In the 1980s the main focus in processor design was improving single core performance and clock speed. Today, focus is on more cores at a lower clock speed, and doing more with each cycle (modern Intel CPUs can do 4-8 instructions per cycle!). New microarchitecture-level techniques like instruction-level parallelism (parallelism within 1 thread) and hyperthreading (parallelism between multiple threads on one core), speculative execution (running both sides of a branch and then throwing away the wrong one once the branch is done), out of order execution (rearranging instructions to improve parallelism) have also resulted in significant performance gains. The main issue preventing more single core performance and clock speed is the **power wall**.

**Dynamic power consumption** is the power consumed that depends on the processor workload. When a transistor switches, it must use energy proportional to the capacitive load times the voltage squared. So the power used (switching frequency times switching energy) quicky increases linearly with the switching frequency, resulting in increased cooling requirements and worse battery life. One technique modern processors use to try to get around this is to reduce the switching voltage to quadratically reduce the switching energy required - for example, a 15% reduction in voltage results in a 28% reduction in switching energy.

There's also **static energy consumption**, the power consumed regardless of the processor workload. As transistors get smaller, the leakage current increases (due to quantum tunneling in the transistor junctions), which means more power used per transistor.

Modern techniques involve thread-level parallelism (doing multiple tasks at once), and data-level parallelism (doing something to multiple pieces of data at once).

**Fynn's taxonomy** characterizes four classes of parallelism:

* Single-instruction single-data - single processing unit acting on single pieces of data. For example, a core calculating one scalar value. For these, we can take advantage of instruction-level parallelism.
* Single-instruction multiple-data - single processing unit acting on multiple pieces of data. For example, DSP units, GPUs, vector instructions. For these, we can take advantage of data-level and instruction-level parallelism.
* Multiple-instruction single-data - multiple processing units acting on single pieces of data. Not really used in practice.
* Multiple-instruction multiple-data - multiple processing units acting on multiple pieces of data. For example, multithreading and multiprocessing. For these, we can take advantage of thread-level, data-level, and instruction-level parallelism.

Recall MIPS:

* 32-bit instructions, in one of two formats. First format: 6 bits for opcode, 5 bits for register operand 1, 5 bits for register operand 2, upper 16 bits for immediate value. Second format: 6-bit opcode, 5 bits each for register operand 1, 2, and 3, 5-bit shift amount, and 6-bit shift type specifier. See CS241 notes for more details about MIPS instructions.
* 32 registers R0 to R31, R0 always has value 0.
* In this course, we'll use a 5-stage pipeline for fetch, decode, execute, memory, and writeback. We can overlap up to 5 instructions at once.
* Instruction flow is linear (statically scheduled), and one instruction is completed per cycle (ideally). In other words, instructions are run in whatever order they're stored as, and we don't dynamically rearrange to improve parallelism.

# 19/1/17

Sometimes instructions in a pipeline depend on others. There are three main types of these dependencies:

* A read-after-write dependency is when an instruction needs to read out the value that some other instruction wrote - if the CPU ran both at once, we might read the wrong value. This is the only real dependency in scalar, statically-scheduled pipelines.
* A write-after-read dependency is actually an anti-dependency - if the CPU reorders these instructions so that they're in the opposite order, we might read the wrong value.
* A write-after-write dependency is an output dependency - if the CPU reorders these instructions so that they're in the opposite order, the final result may be wrong.

When instructions aren't dependent on each other, we can more-or-less freely rearrange them. This allows efficient pipelining and parallelism.

**Total sequential execution** (TSE) is an assumption in our programming model that one instruction finishes before the next starts. TSE is sufficient but not necessary for semantic correctness. In fact, semantic correctness can be satisfied just by satisfying the inter-instruction dependencies of the original program - we can otherwise rearrange and overlap instructions however we want.

Since we have five stages (fetch, decode, execute, memory, writeback), an instruction that writes registers takes 4 cycles after it's been fetched before the write actually takes effect, assuming 1 cycle per pipeline stage. Likewise, an instruction that reads registers takes 1 cycle after fetching before actually performing the read.

A control dependency is basically a special case of a register dependency, where the register is the program counter. An instruction that branches (writes to the program counter) has its write take effect 1 cycle after being fetched (the MIPS datapath reads PC in fetch stage and writes in decode stage).

In the same way, an instruction that writes to memory has a memory dependency, and has its write take effect 3 cycles after being fetched.

Therefore, if we have two instructions $A, B$ that run at clock cycle $i$ and $j$, respectively, such that $i > j$:

* If there is a read-after-write register dependency from $B$ on $A$, we must satisfy $j + 1 > i + 4$. Hazards occur if $j - i \le 3$.
* If there is a write-after-read register dependency from $B$ on $A$, we must satisfy $j + 4 > i + 1$. Hazards are impossible without reordering instructions.
* If there is a write-after-write register dependency from $B$ on $A$, we must satisfy $j + 4 > i + 4$. Hazards are impossible without reordering instructions.
* If there is a read-after-write control dependency from $B$ on $A$, we must satisfy $j + 0 > i + 1$. Hazards occur if $j - i \le 1$.
* If there is a write-after-read control dependency from $B$ on $A$, we must satisfy $j + 1 > i + 0$. Hazards are impossible without reordering instructions.
* If there is a write-after-write control dependency from $B$ on $A$, we must satisfy $j + 1 > i + 1$. Hazards are impossible without reordering instructions.
* If there is a read-after-write memory dependency from $B$ on $A$, we must satisfy $j + 3 > i + 3$. Hazards are impossible without reordering instructions.
* If there is a write-after-read memory dependency from $B$ on $A$, we must satisfy $j + 3 > i + 3$. Hazards are impossible without reordering instructions.
* If there is a write-after-write memory dependency from $B$ on $A$, we must satisfy $j + 3 > i + 3$. Hazards are impossible without reordering instructions.

A **hazard** is a potential for a violated dependency (and therefore, violation of semantic correctness). In our pipelined, statically scheduled CPU, the hazards are a 3-cycle register read-after-write, and a 1-cycle control read-after-write.

To avoid hazards, we can insert no-op instructions - 3 NOP instructions after each register read-after-write, or 1 after every control read-after-write. These can be inserted either by the compiler, or the CPU can stall for the necessary number of cycles when the relevant dependencies are found. Compilers 

However, a 3-cycle stall for a 3-cycle register read-after-write is quite long. We can shorten register/memory dependency stalls by using **data forwarding** - routing the output of later stages directly to earlier stages to avoid the need for a write/read - earlier stages can read directly from the later stages.

For our purposes, we can route the output of the memory stage to the execute stage (for $j = i + 1$), the output of the writeback stage to the execute stage (for $j = i + 2$), and the output of the writeback stage to the decode stage (for $j = i + 3$). With that in place, each stage always has the data it needs, so we never have to stall.

To implement data forwarding in each stage, we can use multiplexers that choose between either reading/writing registers, or taking input directly from later stages.

Turns out, this can't eliminate all our stalls, though it does make a lot of them much shorter. Consider `lw R1, (R2)` followed by `add R4, R1, R1` - the load-word instruction doesn't have its result until the memory stage is done, so we must stall the `add` for 1 cycle so we can forward it from the write-back stage to the execute stage.

To solve the control dependency read-after-write stall (when we have a conditional branch), we might assume that the branch won't happen, keep executing the next instruction, and then squash it in the pipeline if we do end up taking the branch.

MIPS uses a different approach - the CPU will execute the instruction after the branch (the instruction after the branch is in a position called the **branch delay slot**), regardless of whether the branch is taken or not. The compiler will generally try to fill the branch delay slot with an instruction that doesn't depend on the branch, or NOP if it can't find one.

# 24/1/17

Recall that in **static scheduling**, instructions are executed in the order that they appear in, and the compiler is responsible for arranging instructions in a way that avoids hazards.

Code example with instruction reordering by the compiler for hazard avoidance.

The cycles per instruction (CPI) is a measure of CPU performance, and is ideally somewhere around 1 for our scalar processor. However, penalty cycles caused by hazards will increase this. For example, if we have 6 instructions that incur 3 penalty cycles, we get $\frac{6 + 3}{6} = 1.5$ as the CPI.

The program execution time is $\frac{n \times \text{CPI}}{f}$, where $n$ is the number of instructions in the basic block, and $f$ is the clock frequency.

**Local scheduling** is when the compiler rearranges instructions within a basic block. A **basic block** is a sequence of consecutive instructions such that if one instruction is executed, all of them are - that means only the first instruction can be the target of a branch instruction, and only the last instruction can be a branch instruction (or the second to last for MIPS, due to the branch delay slot).

To perform local scheduling, we first identify the hazards in the code, leaving slots to put other instructions in according to how many penalty cycles are present. For example, if we have a load followed by a dependent add instruction, we might have 1 penalty cycle in our architecture, so we would leave one slot for an instruction. Then, we try to fill in the slots with other instructions while keeping dependencies satisfied.

**Global scheduling** is when the compiler rearranges instructions across multiple basic blocks.

A common example of this is loop unrolling, where the body of a loop known to execute $n$ times is simply duplicated $n$ times - this reduces branching overhead, and makes local scheduling better since it may result in larger basic blocks. While loop unrolling can make our code faster, it results in larger programs, which might cause more instruction cache misses. The compiler must make the tradeoff depending on what would be faster.