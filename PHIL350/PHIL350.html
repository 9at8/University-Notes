<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>PHIL350 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>
function highlight() { // highlight all code blocks using HighlightJS
  var code_blocks = document.getElementsByTagName("code");
  for (var i = 0; i < code_blocks.length; i++)
    hljs.highlightBlock(code_blocks[i]);
}
</script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
  </ul>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1 \right\rceil}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
\]</span></p>
<h1 id="phil350">PHIL350</h1>
<p>Theories of Knowledge.</p>
<pre><code>Wesley Buckwalter
Section 001
Office hours: Monday, Wednesday 2:30 PM-3:30 PM, Hagey Hall 325</code></pre>
<h1 id="section">14/9/15</h1>
<p>Articles are all posted on LEARN. Articles marked with &quot;(M)&quot; are for the Monday class, and &quot;(W)&quot; are for the Wednesday class - make sure to read these before class.</p>
<p>Best contact method is emailing wesleybuckwalter@gmail.com with &quot;PHIL 350&quot; in the subject.</p>
<p>Epistemology is the study of knowledge.</p>
<p>Epistemology courses often focus on very abstract, disconnected concepts. This course focuses on the more practical, concrete aspects of modern epistemology - the study of what we know and how we know it, and what we think other people believe or know.</p>
<p>Knowledge tells us how to act, who to trust, which beliefs to form, and how to act virtuously. One good example of the importance of epistemology is in the law - the knowledge and beliefs of the defendant has a significant impact on the proceedings. A defendant that does something unknowingly will be treated differently from one that acts knowingly.</p>
<p>In the law, <strong>ignorance</strong> is not knowing something, and not knowing that one doesn't know. <strong>Willful ignorance</strong> is not knowing something, knowing that one doesn't know, and intentionally avoiding knowing it. Knowledge is awareness and strong belief.</p>
<h1 id="section-1">16/9/15</h1>
<p>Readings: Quine, Kim</p>
<h2 id="naturalized-and-normalized-epistemology">Naturalized and Normalized Epistemology</h2>
<p>Willard Van Orman Quine proposed <strong>naturalized epistemology</strong>, in opposition to &quot;old philosophy&quot;. In &quot;Epistemology naturalized&quot;, Quine complains about various writings about old epistemology. For example, pragmatically speaking if Descartes doubts the senses then all the sciences are suspect - doubting the senses doesn't leave us with anything practical.</p>
<p>According to Quine, old epistemology views itself as superior to and justifying the existance of science, allowing us to rationally generate discoveries. Additionally, old epistemology cares only about certainty, the things that must be true beyond all doubt (the foundationalist view).</p>
<p>Quine criticizes Descartes' arguments by asking where the guarantee of truth comes from. The commonly accepted answer is that the definition of the words themselves mean that the conclusion follows. Quine then asks where the meanings of words come from, to which it is generally agreed that it is simply convention.</p>
<p>Circular arguments are those that assume what they're trying to prove, and circularity is generally considered a negative feature of science.</p>
<p>In old epistemology, we cannot say we have knowledge until we have certainty. However, certainty is only guaranteed by subjective experiences, valid deductive arguments, and the meaning of words. However, science is based on inductive arguments and observation. Epistemology justifies science, but we actually need to assume things happen in order for science to explain them. Quine criticizes this as being circular - &quot;the world exists because the world exists&quot;.</p>
<p>The inductive arguments that are so essential to science cannot give us certain knowledge. To justify this, David Hume adds the key assumption known as the <strong>Principle of the Uniformity of Nature</strong> - that the future will resemble the past. Hume justifies this inductively, by saying that the future has resembled the past in the past, so the future will resemble the past in the future. However, this is obviously a circular argument. According to Quine, <strong>circularity is unavoidable in epistemology</strong>, and maybe that's just fine.</p>
<p>Quine is essentially criticizing that superiority of old epistemology to science, its reliance on certain knowledge, and its avoidance of circularity. In Quine's naturalized epistemology, science comes first, and <strong>epistemology is a branch of science</strong>, specifically related to cognitive science and psychology. Here, the new goal of epistemology is to understand how science and our practices of science actually work, rather than justifying the existance of science.</p>
<p>Additionally, science is the best way of getting information about the world, so we might as well use science to study science. It is circular to some extent, but this is fine since we no longer care about certainty. It is circular, but not in a way that blocks progress - progress can be made without certainty, through incrementally gaining confidence in our fallible beliefs as we obtain more discoveries and scrutinize our beliefs publicly.</p>
<p>Kim's criticism of Quine's naturalized epistemology starts off by agreeing with Quine's criticism of old epistemology - that naturalized epistemology cannot replace old esptemology. According to naturalized epistemology, individuals accept senses asinput and output beliefs. However, Kim says that there is more to believing and knowing things.</p>
<p>Kim says that the central tenet of epistemology is justification, that epistemologists actually catre about how justified beliefs are formed. In other words, justification is normative, while science is descriptive, so naturalized epistemology cannot answer these normative questions - science cannot tell us how we <strong>should</strong> form beliefs, only <strong>how we do</strong>.</p>
<p>For example, naturalized epistemology cannot tell us what makes evidence good, what beliefs count as being justified, what counts as a fallacy, what is rational to believe, and so on. According to Kim, belief isn't even possible without normative rules - to believe something, one needs a baseline of what counts as believable or not.</p>
<p>If a person believes that it's raining when it's sunny outside, naturalized epistemology would only tell us that the person is wrong, or what caused the wrongness. Epistemology must also be able to tell us what the person should believe instead, any why it is unjustified to think that it is raining.</p>
<p>Fundamentally, there is some egalitarian aspect to this sort of epistemology - beliefs are the same regardless of who thinks them. This is another one of the underlying assumptions in old and naturalized epistemology, though modern epistemology is more likely to take the individual into account.</p>
<p>Knowledge is valuable because it's a foundation to base our science on, according to old epistemology. According to Kim, what counts as justified is normative and cannot be deterined by science.</p>
<h1 id="section-2">21/9/15</h1>
<p>Readings: Kvanvig, Lackey</p>
<h2 id="assertion">Assertion</h2>
<p>According to Kvanvig, knowledge is not valuable, so we should stop studying it.</p>
<p>Most philosophers agree that knowledge is better than opinion/belief, even if that belief happens to be true. One of these reasons is because the beliefs can be false, while knowledge cannot - <strong>knowledge requires truth</strong>.</p>
<p>Guessing the right answer on a multiple choice test is different from and worse than knowing the right answer. According to Socrates, belief is &quot;untethered&quot;, while knowledge is &quot;tethered&quot; - <strong>knowledge requires justification</strong>.</p>
<p>This leads us to the Justified True Belief (JTB) - a working definition of knowledge as a belief that is both true and justified.</p>
<p>However, this definition is challenged by the <strong>Gettier Problem</strong>, which proposes situations where someone has justified true beliefs, but obviously doesn't have knowledge. These sitations are <strong>Gettier situations</strong>.</p>
<p>For example, you walk past a farm and see things that look exactly like sheep, and believed that there are sheep on the farm. Getting closer, you see that what you saw were actually dogs with long hair. However, there are actually sheep in another part of the farm. Arguably, you did not know that there are sheep on the farm, yet you had a justified true belief.</p>
<p>This implies that there is something more to knowledge - that it requires truth, justification, belief, and even more things. Trying to figure out what these criteria are is an ongoing problem, but Kvanvig says we shouldn't need to care about it.</p>
<p>Kvanvig says that the goal of epistemology is to figure out what the ultimate cognitive state for humans is, and knowledge is not that state. The ultimate state, he says, is that to be able to tell immediately and easily whether any claim that affects one's life is true or false, and that no cognitive mistakes could occur.</p>
<p>Kvanvig argues that it is possible to be in that state without knowing or even understanding what knowledge is. In the example Gettier situation, you had no knowledge that there is a sheep, but ended up correct anyways. Kvanvig says that ideal cognition is possible without knowledge - that you could accidentally get the right answer and it's just as good as knowing the right answer.</p>
<p>However, many object to this by saying that there are certain virtues to knowledge that accidentally true beliefs don't have. Are these virtues important in themselves? Answering a question correctly because of your own knowledge feels better than answering correctly by guessing.</p>
<p>An <strong>assertion</strong> is a claim that something is true. An objection to Kvanvig is that knowledge is needed as a basis for assertion. The <strong>knowledge rule for assertion</strong> says that one should only assert what one knows. Assertion is very practically useful, and if knowledge is the basis of assertion as per the knowledge rule for assertion, then knowledge is not useless, and is worth studying.</p>
<p>The knowledge rule for assertion is suppoorted by real-world usage of assertions. Intuitively, what we want to assert as true are things that are true - when we ask someone what the current time is, we want the right answer. We might also want to ask them how they came up with that answer - how did they get the current time? We can challenge assertions by presenting evidence that the assertion was false, or saying that there is no evidence that what they asserted is actually true. In other words, assertions are proper not when we are wrong, but when we don't have knowledge of what we assert.</p>
<p>Lackey presents <strong>selfless assertion</strong> - when someone makes an assertion without having any knowledge about it, but the assertion is still proper. In the case of askin someone what the current time is, Lackey says that the mind of the person being asked doesn't matter.</p>
<p>For example, a pediatrician who, in grief of his daughter being diagnosed with autism, starts believe that vaccines cause autism, even though he also respects the teachings of science. To his patients, he says that this is not the case, because he thinks that this is most likely to be true, and this is what is in the best interests of the patients. The argument is that the pediatrician does not believe something, and therefore did not have knowledge of something, but his assertion is still proper, even if the patients know that he doesn't have knowledge of what he is asserting. Basically, he believes one thing, but asserts another in the best interests of his patients.</p>
<p>In other words, selfless assertion challenges either the knowledge rule, or the idea that knowledge requires belief (this is supported by the pediatrician thinking that there being no connection is most likely to be true). Perhaps the pediatrician believes both (even though they are conflicting), or the &quot;best interests&quot; qualifier has some significance.</p>
<p>;wip: potentially a good topic for the paper is whether a better rule for assertion is whether what is asserted is in the best interests of people being asserted to</p>
<p>Another example is a Catholic teacher who believes in creationism, but still teaches students the evidence presented by science. Although the teacher's faith has her believe one thing, she asserts another, without having knowledge, and this seems to be a proper assertion. However, the examples provided so far have been quite complex, and it is important to consider other factors in play that could influence how we view the assertions or knowledge.</p>
<h1 id="section-3">23/9/15</h1>
<p>Readings: Turri</p>
<p>There are different types of speech acts, and each one have their own norms and rules. For example, guessing has different norms from speculating, conjecturing, asserting, promising, or guaranteeing. The norms and rules are what we are trying to define for each relevant speech act.</p>
<p>There are different types of normativity. <strong>Constitutive normalivity</strong> are the internal rules that consitute a practice - the rules that basically define the practice. <strong>Moral normativity</strong> are rules that say when a practice is morally permissible - the rules than define when we should do the practice. <strong>Prudential normativity</strong> are rules that say when a practice is prudent/wise - when it would be a good idea to do a practice.</p>
<p>For example, the rules of chess is contitutive normativity, while going easy on new chess players is moral normativity. If you have a bet that you will lose a chess game, it would be prudent to lose it in order to win the bet, and this is prudential normativity.</p>
<p>What is the constitutive norm of assertion? In other words, what is the fundamental rule of assertion? Do assertions need to be factive (requires truth)?</p>
<p>One factive account of assertion is the <strong>truth rule</strong> - one should assert something only if it's true, and knowledge/justification/belief is not necessary.</p>
<p>Another factive account is the <strong>knowledge rule</strong>, which we looked at earlier - one should assert something only if you know it, so that needs truth, belief, and evidence.</p>
<p>A non-fictive account is the <strong>belief rule</strong> - one should assert something only if one believes it, so it's still fine even if the assertion is totally false.</p>
<p>Another non-fictive account is the <strong>justification rule</strong> - one should assert something only one can justify it, so that needs belief and justification.</p>
<p>Lackey's account is non-factive, and modern literature tends to favor non-factive accounts. It feels like when people make well-justified assertions that turn out to be false, they didn't do anything wrong and are not to blame. In other words, there are <strong>reasonable false assertions</strong>.</p>
<p>For example, if we have 99.9% certainty that something would happen, it seems proper to assert it as true, even though we could turn out to be wrong - we should not be to blame for making such an assertion.</p>
<p>Turri's account is a factive one, and he argues for the knowledge rule. According to Turri, if assertion is rule governed, then we should look at how people are actually acting out these rules in real life, particularly people who are competent at using the language - we should obtain empirical evidence for rules governing assertion.</p>
<p>Turri conducted social psychology experiments, testing how people reacted to assertions in a game for whether people approve or disapprove of well justified false assertions, both when it turns out true, and when it turns out false.</p>
<p>As it turns out, people think false but justified assertions should not be made, even when that justification is very good. However, various objections to these results were raised, such as the experiment also measuring prudential/moral normativity rather than just constitutive normativity. Also, there was a sliding scale from &quot;very wrong&quot; to &quot;very right&quot;, to eliminate the possibility of a false dichotomy.</p>
<p>In response, Turri conducted further experiments with different situations in order to eliminate variables. The reasoning was <strong>excuse validation</strong> - the maker of the false assertion has a reasonable excuse for being wrong. Turri concludes that factive accounts of assertion better explain people's reactions to assertions.</p>
<p>Interestingly, blame depends on character judgements of the person - when people perceived as good broke a rule, they were blamed less or not considered to have broken rules more often than people perceived as bad. In other words, people will simply say someone did not break the rule if they are not to blame, because blame is associated with punishment, and a blameless person doesn't deserve punishment.</p>
<h1 id="section-4">28/9/15</h1>
<p>Readings: Fantl, McGrath</p>
<p>We now look at the act of asserting itself. Do the practical consequences of acting affect justification?</p>
<p>For now, we will say that one is justified in believing something if one has good enough evidence to know it. We will assume that justification is required for knowledge.</p>
<p><strong>Evidentialism</strong> is a theory of what justification is like, which states that whether one is justified in believing something depends only on <strong>how much evidence is available</strong> - the person believing doesn't matter, so two people with the same evidence should always have the save belief. Evidentialism ignores motives, pre-existing beliefs, and preconceptions in favor of following the evidence, and only the evidence.</p>
<p>However, Fantl and McGrath deny evidentialism - there are things other than evidence that can affect whether beliefs are justified and whether one has knowledge. For example, if you know X, it is no problem to act as if X. However, if it is a problem to act as if X, then you could explain it away as though you didn't know X, even though you did. The main idea is that if you know that something is true, then it is fine to act on it - it is rational to act on your knowledge. Also, if you're justified in beliecing something is true, then it is also fine to act on it.</p>
<p>However, how much is at stake affects how rational it is o perform certain actions - what is the cost of getting something wrong? For example, suppose we were vacationing, and we ask someone whether a train stops in Foxboro, and we don't really care if it's right or not. Since this is pretty good evidence that it does stop there, we would believe the person and get on the train. However, if we were on the way to a critical job interview, we would probably double check just in case. In this case, the evidence is not good enough to believe the person, and would get more evidence instead.</p>
<p>In other words, in one case it is rational to believe the justification, and in another case it is not - so in one case, your belief is justified, and in another, it is not, even though there is exactly the same evidence in both cases. This implies that there is more than just evidence at play in belief and actions. This is at odds with evidentialism - perhaps each person actually has their own personal threshold of justification.</p>
<p>The rationality of acting, according to Fantl and McGrath, sets the standard for how much evidence is needed in order to know or be justified in believing something. In their example, the stakes of the situation influence whether a belief is justified.</p>
<h1 id="section-5">30/9/15</h1>
<p>Readings: Buckwalter, Turri</p>
<p>The accepted norm of assertion is knowledge - you should assert something only if you know it. This is supported by empirical studies of assertion in the real world, as discussed earlier. In the real world, showing is more costly than telling - so if knowledge is important for showing (asserting), it should also be important for showing.</p>
<p>In other words, if knowledge is the norm of assertion, is knowledge also the norm of showing? The readings seem to suggest that yes, this is the case - <strong>we should only show something if we know it</strong>:</p>
<ul>
<li>&quot;How is this done?&quot; (asking for instruction/demonstration/showing) is similar to &quot;do you know how this is done?&quot; (asking for the presence of knowledge), just like &quot;what is this?&quot; works like &quot;do you know what this is?&quot;. If one knows, then one can show.</li>
<li>Saying &quot;I don't know how to do this&quot; is a legitimate reason to not demonstrate it. If you don't know how to do something, it tells everyone that you should not need to demonstrate it. Without knowledge, one should not show.</li>
<li>&quot;How do you know that?&quot; challenges one's authority, and implicitly requests a demonstration. Challenges to knowledge are challenges to demonstrate.</li>
<li>Being able to show something, but not knowing how to do something seems inconsistent - saying &quot;I don't know how to drive&quot; while driving perfectly seems weird. Showing something seems to imply knowing something.</li>
</ul>
<p>If knowledge is the norm of both showing and telling, knowledge is also the norm of instruction - the criteria by which we should decide whether to transmit information.</p>
<p>An interesting thing to consider is muscle memory - if muscle memory is knowledge, then it is possible to have knowledge without belief, justification, or truth. For example, muscle memory might store the relationship between the gas pedal and the current acceleration, but one need not even believe that the gas pedal affects acceleration to act on it.</p>
<p>&quot;Descartes' Schism, Locke's Reunion&quot; looks at whether things like the train thought experiments held up empirically. Descartes' view is that knowledge is completely separate from action, while Locke states that they are intricately linked.</p>
<p>Locke basically says that knowledge is truth, evidence, belief, and practical factors - practical factors directly influene knowledge. Descartes basically says that knowledge is truth, evidence, and belief, all of which are affected by practical factors - practical factors are removed from knowledge. We want to test these empirically, using psychological and sociological experiments.</p>
<p>These experiments tested things like how knowledge was affected by two identical sitations with different stakes, as well as many other variables. Each of the 600+ particupants were given situations with different stakes, and then questioned regarding the state of belief, truth, evidence, action, knowledge, and importance.</p>
<p>As it turns out, stakes had a negative correlation with action, truth, and evidence, and these in turn had a positive correlation with knowledge. So if the stakes are high, actionability, confidence in truth, and amount of evidence are decreased, and if they are low they are all increased. Acting had the most significant impact on having knowledge, truth a little less, but evidence a lot less.</p>
<p>So essentially, action does have a direct impact on knowledge, like Locke's model, but the stakes are also somewhat removed from knowledge, like Descartes' model. So the ability to act on something had the largest impact on whether one thinks one has knowledge. Note that since this experiment only measures correlation, it doesn't tell us anything about which way the knowledge/action link goes - does knowledge result in actionability, or does actionability imply knowledge?</p>
<h1 id="section-6">5/10/15</h1>
<p>Readings: Fricker, Lackey</p>
<h2 id="testimony">Testimony</h2>
<p>We want to investigate the relationship between knowledge, and the person hearing it - in particular, what is testimony, when should be believe it, and when does it count as knowledge?</p>
<p>Justification is necessary for knowledge. Justification, in turn, is done through good evidence - from memory, observation, reasoning, and testimony.</p>
<p>In legal systems, testimony is rigidly defined and is given on the witness stand. For us, giving testimony is making an assertion, claim, or presents something as true. Where we previously looked at the speaker's side (when we should say something), we will now look at the listener's side (when we should believe something).</p>
<p>A <strong>testimonial belief</strong> is a belief formed by testimony. Most of our beliefs actually seem to be testimonial beliefs. When we believe things people say, we seem to often gain real knowledge, often without us thinking about it much at all. We also seem to sometimes gain false beliefs, and our goal is to find a criteria for belief that helps us gain only real knowledge.</p>
<p>One informal one people often use is the speaker's track record - how good their testimony has been in the past. Others include the stakes, relationship with the speaker, and the motives of the speaker.</p>
<p>According to <strong>reductionism</strong>, testimonial beliefs are justified in the same way as any other source of evidence, like induction, past observations, and reasoning - <strong>testimony is not special</strong>. <strong>Anti-reductionism</strong> is the opposite view - that testimonial beliefs are justified by a special principle.</p>
<p>Fricker investigates the reductionist/anti-reductionist distinction, and argues for reductionism.</p>
<p>The <strong>PR thesis</strong> is presented as a principle for justifying testimonial beliefs, in favour of anti-reductionism: that hearers of testimony can assume, without other evidence, that the speaker is trustworthy, unless there are special circumstances that prevent this (like the speaker not being in the right state of mind, or known to say false things often). In other words, listeners can just assume the speakers are trustworthy, unless given a good reason not to.</p>
<p>Fricker criticizes this by saying that hearers are never justified in simply trusting speakers - according to Fricker, hearers of testimony should always assess the speakers for trustworthiness. In other words, gullible/blind beliefs are not justified. Instead, hearers should <strong>critically assess</strong> the testimony, be able to <strong>explain the testimony in their own words</strong>, and be able to <strong>defend the beliefs formed from the testimony</strong>. This is called the <strong>NC thesis</strong> (negative claim).</p>
<p>This is a very internal view - all the criteria are judged in the mind of the hearer.</p>
<p>One objection to this is that it is often not possible for hearers of testimony to independently confirm that speakers are trustworthy - it is unrealistic to expect hearers to evaluate all these criteria. Fricker responds that it is realistic - people actually are always evaluating trustworthiness subconsciously, and this affects our evaluations of speakers' trustworthiness. <strong>Counterfactuals</strong> are essentially thoughts about hypothetical situations - thoughts about &quot;what would happen if X happened instead of Y?&quot;. According to Fricker, testimonial justification requires <strong>counterfactual sensitivity</strong> - thinking about &quot;what if the speaker is untrustworthy?&quot; and &quot;if they were untrustworthy, would I have spotted that?&quot;. Basically, testimonial belief is justified when the hearer would be able to tell if the speaker is being untrustworthy.</p>
<p>Fricker's evidence for the NC thesis is mostly common sense and ordinary/everyday experiences, and normative linguistic theories - is this good enough evidence to support the thesis? Also, this doesn't tell us how much evidence we need, or how trustworthy speakers must be before forming beliefs.</p>
<p>One interesting case is self-knowledge (assertions about the speaker's own experiences). In these cases, hearers generally simply trust what the speakers say, and it isn't really possible to critically assess the trustworthiness in this case. Also, should be assess strangers more heavily than trusted friends? Are there some roles when the critical assessment is already done for you (for example, when a doctor tells a patient to do something)?</p>
<p>Lackey investigates the properties of testimony transfer - is testimony like passing a baton? In other words, can hearers get knowledge from the speaker even if the speaker doesn't have that knowledge?</p>
<p>Basically, Lackey talks about whether testimony simply transmits knowledge, or whether it can actually generate new knowledge, and argues for the latter. For example, a creationist teacher that teaches evolution (despite not believing it) doesn't have knowledge of the material (she is simply following orders), yet it seems like the students can obtain knowledge, even though the speaker doesn't have knowledge.</p>
<h1 id="section-7">7/10/15</h1>
<p>A <strong>defeater</strong> is something that defeats evidence - it takes away confidence in evidence that one has. When people make testimony, sometimes the defeaters don't get transferred - people may not transmit counterarguments, conflicting facts, and other things that might throw the evidence into question</p>
<p>An interesting case of testimony: person A has good vision, but the person's doctor tells them that their vision is unreliable. Person A sees X, and tells person B as such, but does not tell person B that the doctor said person A's vision is unreliable. Person A's evidence of seeing X is defeated by the doctor's statement. However, from person B's perspective, there is nothing wrong with the evidence.</p>
<p>Lackey concludes, from these and other situations, that testimony can actually generate knowledge, in contrast to memory, which can only preserves knowledge.</p>
<p>Does the way testimony work depend on what you're talking about? Hazlett investigates this by looking at several case studies.</p>
<p>If someone knows a lot about football, and you ask them who won a particular game, you would probably accept the answer on the basis of testimony alone. If someone knows a lot about ethics, and you ask them whether military intervention in some country is right, you would probably not accept the answer on the basis of testimony alone.</p>
<p>These two situations have the same form, with only the content changed. Perhaps it's because there's just so many possible options that need to be weighed to make a decision in the latter case, or there is a difference when there are multiple plausible answers?</p>
<p>If someone knows a lot about art, and you ask them whether a piece of art is beautiful, it doesn't really make sense. If someone knows a lot about metaphysics, and you ask them whether god exists, you would probably not accept the answer on the basis of testimony alone.</p>
<p>Hazlett says there are different types of testimony, including factual testimony (&quot;what is this thing?&quot;), ethical testimony (&quot;is this thing right?&quot;), aesthetic testimony (&quot;is this thing beautiful?&quot;), religious testimony (&quot;does god exist?&quot;), and so on. In each type of testimony, there seems to be different norms. When the content of testimony is about ethics, religion, or aesthetics, there seems to be something wrong about forming beliefs based on just that testimony - there is <strong>testimonial asymmetry</strong>.</p>
<p>Hazlett assumes testimonial asymmetry exists, and attempts to diagnose the cause. For children, moral/religious/aesthetic testimony seems to be fine - parents telling children what's right or wrong seems to be good testimony.</p>
<p>This view is non-reductionist in that the good/bad judgement is for the testimony itself, not about the beliefs that cause the testimony. Hazlett proposes a few theories of the causes of some testimony being different.</p>
<p>By understanding theory, moral/religious/aesthetic knowledge requires understanding in addition to just facts, and understanding cannot be transferred via testimony.</p>
<p>By acquaintance theory, moral/religious/aesthetic knowledge requires acquaintance with the subjects - perception and experience with the things being talked about, that also can't be transferred via testimony.</p>
<p>By virtue theory, moral/religious/aesthetic knowledge requires virtues, such as the virtue of figuring this out for oneself, a virtue that cannot be transferred via testimony.</p>
<p>Basically, these theories push the testimonial asymmetry into knowledge asymmetry. Hazlett rejects all of these, and proposes a social theory instead: moral/religious/aesthetic knowledge is socially valuable, and testimony cannot transfer the social value via testimony - beliefs are not worth as much when they were just formed by testimony, since they reduce diversity of knowledge. In other words, testomonial beliefs are bad when they are bad for society.</p>
<h1 id="section-8">14/10/15</h1>
<p>The second critical response is due next wednesday.</p>
<p>Readings: Huemer</p>
<h2 id="memories">Memories</h2>
<p>When are memory beliefs justified? When should we believe in our memories?</p>
<p>Just like through testimony and observation, memory is a way to get justification, which is important for obtaining knowledge.</p>
<p>The majority of what we believe comes from our memories. Often, our knowledge comes from information stored in our brains. However, we can sometimes remember something very accurately as true, when it's actually false.</p>
<p>We believe that the sun is around 93 million miles from the Earth. However, most of us don't remember what our original reason for memorising this is.</p>
<p>Huemer has three possible answers to when we should believe our memories - inferential, foundational, and preservative. Huemer then rejects them in favor of a dualistic view.</p>
<p>In the inferential theory, memory beliefs are justified by that memory being reliable in the past - if it worked out in the past, then we are justified in believing it. However, this theory is rather circular - the only things that tell us that a memory was reliable are our other memories. If we use only our present experiences and insights, it's unlikely we could judge whether memories are reliable under this theory.</p>
<p>In the foundational theory, memories are the foundation of justification, and just as we usually trust perception, we should also usually trust remembering something - remembering something automatically gives a good reason to believe it. However, the passage of time alone shouldn't make false beliefs justified - it's easy to create a memory of an unjustified belief, and then say it's justified because it's a memory.</p>
<p>In the preservation theory, memory preserves the original justification of the belief - whatever the justification for the original belief was is also the justification for memory beliefs. Even if one does not remember what the original justification was for believing something, the belief is still justified. However, if someone gets cloned, the clone remembers doing all the things the original did, without actually having done those things - the clone should have justification to believe it did those things, since it has all the memories of the original, but under this theory it does not.</p>
<p>The dualistic theory takes parts of all of these theories. It focuses on a few issues with the other ones: beliefs can't increase in justification just by becoming memories, and justification should depend only on the current state of people - if clones have the same memory, they should also have the same justification.</p>
<p>According to Huemer, the justification for believing a memory belief is the justification for forming the belief in the first place, combined with the justification in being confident that the memory was retained correctly over time. The justifications of the two, both numbers between 0 and 1 (where 0 is infallible justification that something is false and 1 is that something is true), are multiplied together to get the overall justification.</p>
<p>This is somewhat similar to what foundational theory and preservation theory says, but also taking memory retention into account.</p>
<h1 id="section-9">19/10/15</h1>
<p>Readings: Myers-Schulz/Schwitzgebel, Murray/Sytsma/Livengood</p>
<h2 id="belief">Belief</h2>
<p>Does knowledge require beliefs? So far we have assumed this is the case - traditionally, we need belief, truth, and justification in order to have knowledge. Assuming that knowledge requires belief is called <strong>belief entailment</strong>, or the <strong>entailment thesis</strong>.</p>
<p>We usually accept belief entailment becase it captures how minds try to get a mental picture of the world from information it receives - belief associates knowledge to minds. Basically, books contain information, and adding people believing it is needed to make knowledge.</p>
<p>If knowledge is an achievement, then beliefs determine who gets credit for having it - belief is required to get credit for having knowledge. Also, it is hard to think of any knowledge of a topic when there are no beliefs about it. We will examine several situations where it seems like we have knowledge without belief.</p>
<p>Radform proposes a thouht experiment: suppose a student feels certain they don't know a subject, but when asked questions about it, suddenly remembers and answers correctly. The student then concludes that they actually did have any knowledge, even though she didn't believe it before. Basically, the student believes they didn't have the knowledge, but it was stored in their brain nonetheless as knowledge. It seems that if they had knowledge even before believing they had it, then belief is not required for knowledge. In other words, the student forgot that they had previously learned the fact, and so doesn't have belief.</p>
<p>Intuitively, this case is controversial because of the question of whether the student actually had knowledge of the subject. It seems like due to circumstances and a temporary memory deficit, the access to the belief was blocked temporarily. Myers-Schulz and Schwitzgebel also try to find these counterexamples.</p>
<p>Suppose a teacher has a subconscious bias against a certain group of people in their class, treating them as less intelligent even though she has studied biases and examined their intelligence to find them the same as the rest of the class, consciously avoiding being prejudiced. Even though she knows they are just as intelligent, she still believes deep down that they are not. Though the belief is irrational, it is still a belief, at odds with her knowledge.</p>
<p>Suppose a person likes to watch horror films that they are scared by, even though they know the plot is very unrealistic. When a situation in real life resembles one in the movie, the person momentarily believes it will play out like in the movie and feels fear, even though the movie was unrealistic and the person knows it would never happen.</p>
<p>As it turns out, in all of these cases most people thought that in this situation, the person had knowledge without belief. In other words, it empirically does not seem that belief is required for knowledge.</p>
<p>Also, phobias are a good example - for arachnophobia, the person has knowledge that spiders will generally not harm them if they act correctly, yet they still believe the spiders will hurt them, since they feel fear.</p>
<p>Myers-Schulz and Schwitzgebel propose the <strong>capacity-tendency account</strong> - knowledge requires the capacity to read the truth, even if one is not able to at a particular time. This accounts for things like forgetfulness, emotional effects, and personal biases/phobias. Instead of having belief, we simply need the capacity to reach the truth in ideal circumstances.</p>
<h1 id="section-10">21/10/15</h1>
<p>Murray, Sytsma, and Livengood provide additional examples of belief not being required for knowledge, and try to address some of the criticisms of Myers-Schwitzgebel's ideas of knowledge not requiring belief.</p>
<p>One objection is that there are two different types of beliefs: <strong>dispositional belief</strong> (what one is prone to accepting, one's natural inclination), and <strong>occurrent belief</strong> (consciously entertained beliefs, happening at a particular time rather than being natural inclined). Although knowledge may not require occurrent belief, according to this objection, knowledge does require dispositional belief, and there is empirical evidence to back this up (would these people accept it even if they were asleep ans dreaming about it?).</p>
<p>Murray responds to this by presenting situations in which the distinction between dispositional and occurrent belief isn't clear. For example, most people would say god knows but does not believe that <span class="math">\(2 + 2 = 4\)</span>. Another situation is a dog specially trained to do math - most people say the dog doesn't believe that <span class="math">\(2 + 2 = 4\)</span>, but in many cases will say that the dog does know it.</p>
<p>A student is told by parents that the earth is the center of the solar system, but learns in school that the sun is. Although they still believe that the earth is at the center, they answer on tests and assignments that the sun is at the center. This seems to imply that the student has knowledge that the sun is at the center, but the student believes that the earth is. However, this might be explained by saying that the student knows not that the sun is at the center, but that the student knows that to obtain a good grade, they need to answer that the sun is at the center - she believes that the earth is at the center, but answers otherwise since they want a good grade.</p>
<p>Murray finds that in the presented situations, a significant fraction of people ascribe knowledge but not belief, which implies belief may not necessarily be required for knowledge. One objection to these new cases is that they involve things like dogs and deities, and therefore don't accurately represent beliefs and knowledge in humans.</p>
<p>According to Murray, belief requires <strong>conviction</strong>, while knowledge doesn't - without conviction, there can be knowledge,without belief.</p>
<p>Readings: Buckwalter/Rose/Turri</p>
<p>A <strong>pro-attitude</strong> is the urge or desire that drives an action.</p>
<p>Besides dispositional and occurrent, we can further distinguish between <strong>thick</strong> and <strong>thin</strong> beliefs. Thin beliefs mean that you think something is true, but without much pro-attitude toward it (no urge to act on it). Thick beliefs are those where you not only think something is true, but like that it is true, and emotionally endorse and assert it - there is a lot more pro-attitude (strong urge to act on it). Thick beliefs require not only the belief, but also a lot of higher level activity such as willpower and desire.</p>
<p>This paper defends the idea that knowledge requires belief, by saying that knowloedge only requires thin belief, while all the studies so far have been measuring whether people have thick belief. It justifies this by saying that knowledge is itself a pro-attitude - knowing something means to take something to be true, which is by definition a thin belief.</p>
<p>Consider that &quot;you don't believe that&quot; and &quot;you don't know that&quot; are essentially equal ways of challenging assertions - challenging belief means that you challenge knowledge as well.</p>
<p>Buckwalter/Rose/Turri replicate the Murray experiments where they presented participants with various situations and asked them questions about it, but in addition they also implied the people had thick belief, and asked &quot;at some level, does the person think that X is true?&quot; to test thin belief.</p>
<p>The results imply that while people think thick belief is not always present when knowledge is, people think thin belief is. In other words, knowledge seems to require thin belief, though not thick belief - to know something, we need to think it is true.</p>
<h1 id="section-11">26/10/15</h1>
<p>Readings: Connee/Feldman, Goldman</p>
<h2 id="justification">Justification</h2>
<p>What are the things that justify belief? <strong>Internalism</strong> is the view that the factors that determine whether a belief is justified are all internal to the believer's experience (contained in one's mind - thoughts, memories, dispositional beliefs, inference, etc.), while <strong>externalism</strong> is the view that there are also external factors (evidence/facts that one has forgotten or never learned).</p>
<p>Internal experiences also include things like reflexes, muscle memory, instincts, and other non-conscious skills. External experiences also include things that cause beliefs, like reading something in a newspaper or a book (externalism says there is a distinction between beliefs formed from good newspapers vs. bad newspapers).</p>
<p><strong>Accessibilism justification</strong> is justification determined by factors that a person can consciously access. <strong>Mentalism justification</strong> is justification that is simply in one's mind - it is still justification even if one cannot consciously access it (for example, the student who forgot something while under the stress of an exam still has justification, even though they can't access it).</p>
<p>Conee and Feldman support mentalism and defend it from accessibilism internalism and externalism. Mentalism says that if two people are the same mentally, they must have the same justification for their beliefs - when people have different levels of justification, they must have different mental states.</p>
<p>For example, two people read the newspaper indoors, which says it is sunny outside, but one goes outside and sees that it is sunny. While both people believe it is sunny outside, the one who went outside seems to have more justification. The mentalism view says that the internal experiences and memories of seeing that it is sunny accounts for the difference in justification.</p>
<p>For example, an expert birdwatcher and a novice birdwatcher both see a woodpecker, but it seems like the expert has more justification in believing that the bird is a woodpecker than the novice. Though the situation is the same, the justification depends on the mental state of the believer. In other words, the justification changed due to internalizing an external fact, or a purely internal difference in the person holding the belief.</p>
<p>Conee and Feldman say that these cases, as well as four more given in their paper, are representative of how justification works in all situations.</p>
<p>One objection to this is that some beliefs can be justified even without internal mental state, like forgotten justification, logic/probabilistic relations, stored bliefs, and impulsive beliefs. For example, if one forgets where they learned that broccoli is healthy, yet continues to believe that broccolli is healthy, they are arguably still justified in believing that broccolli is healthy. A response to this might be that the person still has memory of there being evidence in the world that broccoli is healthy, and is justified in believing that their memory is generally accurate.</p>
<p>Goldman responds by saying that in the externalist view, justification depends on a belief's <strong>etiology</strong> - where the belief comes from, as well as the belief's history. For example, a person who reads a fact in an unreliable newspaper and forgets where they read it from seems to have less justification than a person who reads a fact in a reliable newspaper and forgets where they read it - this seems to imply that there are external factors in justification, since both people have the same mental state yet seem to have different levels of justification.</p>
<p>Also, consider the broccolli example again, but the person believes that it is onion rings that are healthy, misremembering that they read onion rings rather than broccoli. Although the person's mental state is the same as in the original example, it seems like in this case the person is not as justified.</p>
<p>Some beliefs are justified by logic or probability - logic/probabilistic relations. Although they are not mental states, it still seems like they are still valid justifications.</p>
<h1 id="section-12">28/10/15</h1>
<p>Readings: Goldman, Cohen</p>
<p>Final exam is on December 15 at 9AM in HH 2107. The third critical response is due on November 4.</p>
<p>Are there justifiers (things that make beliefs proper) that are not internally available to the believer? Internalism says no, while externalism says yes.</p>
<p>Justifiers must make your belief likely enough to be true (&quot;likely enough&quot; being an arbitrary threshold). The role of justifiers is to define our intellectual duties - reaching truth, avoiding falsehoods, etc.</p>
<p>Accessibilism internalism says that justifiers are those things that we have access to and are aware of, while mentalist internalists says that justifiers are always mental states. The evidentialism theory of justification says that your justification depends on your own evidence.</p>
<p>Justification is normative - it defines what people should believe and when beliefs are okay to hold. Interalism says that as a result, fulfilling the duty of justifying beliefs is an internal matter, and they can't fulfill this duty of the matters are outside of their head. In other words, evidentialism is an internalist view.</p>
<p>Accordng to externalism, mental states are important, but don't cover all cases. For example, one can obtain information from a source, but the trustworthiness of the source is a justifier that is external - even if one isn't aware of the trustworthiness, it still affects the justification. Therefore, trustworthiness is an external justifier.</p>
<p>Interesting side effect: small children and animals can have justified beliefs if externalism is true, but not if internalism is true.</p>
<p>One externalism theory is <strong>reliablism</strong>, proposed by Goldman. In this theory, one should believe things that are likely to be true, so justification is simply those things that are likely to lead to true beliefs - you don't have to have mental states or anything to hold a justified belief. Justification is based on <strong>process reliablism</strong> - beliefs are justified when they are formed by reliable cognitive processes, like sensory perception, good reasoning, and clear memories.</p>
<p>A reliable cognitive process must tend to produce more true beliefs than false beliefs. Some unreliable cognitive processes include hunches, guesses and biased reasoning. Process reliablism is external because whether a process is reliable can often depend on the environment or other external factors, and one doesn't have to realize a belief is justified to have justification.</p>
<p>For example, if one's memory is often correct, then memory is a reliable cognitive process. If one's memory is often incorrect, then memory is not a reliable cognitive process.</p>
<p>Consider a person who is running and a brain in a vat who is deceived into thinking it is running. Both of their internal states are the same, and they both believe they are running, but the brain in the vat is not actually running. According to internalism, the justiifcation is the same for both the runner and the brain in a vat - they both believe they are running and are justified in believing so. However, by process reliablism, the brain's belief that it is running is not justified, because it is not actually running.</p>
<p>According to Cohen, a proponent of internalism, the brain in a vat is justified in believing it is running. If we were all brains in vats, then no beliefs would ever be justified, which definitely seems to be false. If we have every reason to believe something, then the fact that the cognitive process is unreliable and we don't know shouldn't matter - <strong>justified beliefs are produced by good reasoning, regardless of whether it results in reliable outcomes or not</strong>. Cohen suggests that <strong>there is no connection between justification and truth</strong>.</p>
<p>One objection to this is the following example: if we see a red object, we should be justified in believing that there is a red object in front of us. However, we notice that the room is lit with red light. If justification is disconnected from truth, then even though we realize the room is lit with red light, we are still justified in believing the object is red. However, it seems like we should actually not be justified in believing the object is red in this case, since it could just be white and look red due to lighting.</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>