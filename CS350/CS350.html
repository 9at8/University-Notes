<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS350 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="/anthony-zhang.asc" class="info">GPG key</a></li>
  </ul>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1 \right\rceil}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
\]</span></p>
<h1 id="cs350">CS350</h1>
<p>Operating systems.</p>
<pre><code>Kevin Lanctot
Section 003
Email: klanctot@uwaterloo.ca
Office Hours: Wednesdays at 2:00 PM-3:30 PM in DC 2131
Website: https://www.student.cs.uwaterloo.ca/~cs350/S16/
Wednesdays/Fridays 4:00 PM in MC 4040</code></pre>
<p><strong>Note:</strong> I made a Docker image containing a full <a href="https://github.com/Uberi/uw-cs350-development-environment#readme">development environment</a>, current as of Sprint 2016. This is basically all of the tools you need to complete the CS350 coursework, in a format that will work without any fuss. Notably, this image fixes several issues with setting up compiler versions and ncurses incompatibilities.</p>
<h1 id="section">4/5/16</h1>
<p>Assignments and questions will be distributed via Piazza.</p>
<p>There are 5 assignments, each one building on top of the previous ones. Assignments will be software that runs on top of the OS/161 operating system, on top of a MIPS VM, SYS/161. There are 5 slip days that can be used to extend assignment deadlines, and 3 or fewer can be used for any single assignment.</p>
<p>The role of the operating system is to both help applications do what they need to do, and to stop applications from doing what they shouldn't. For example, an operating system might expose a nice abstract filesystem for storing data in, but it might prevent applications from, say, writing to a file that was being written to by another application.</p>
<p>To applications, the OS provides an execution environment, with interfaces to various resources, like networking, storage, and graphics (most of the code in a modern operating system lives in the various drivers). The OS is also responsible for isolating different applications from each other, making sure they don't step on each other's toes. This is the <strong>application view</strong> of operating systems.</p>
<p>The OS is responsible for keeping all of the hardware in line, making sure that these resources get allocated among the programs in a fair way. This is the <strong>system view</strong> of operating systems.</p>
<p>The OS is also a concurrent, real-time program (at least, most modern OSs). It needs to be able to support multiple things going on at once, but also satisfy timing constraints, such as feeding audio data to the sound card to output uninterrupted sound. This is the <strong>implementation view</strong> of the operating system.</p>
<h1 id="section-1">6/5/16</h1>
<p>The OS <strong>kernel</strong> is the part of the OS that responds to system calls, interrupts, and exceptions. The rest of the operating system includes all of the utilities, shells, and programming libraries.</p>
<p>Applications run on top of the OS, which in turn runs on top of the hardware. Applications interact with the OS kernel via system calls, and the OS interacts with the hardware through commands, data, and interrupts. The user space is all of the parts above the kernel, and is isolated by the kernel from the actual resources - in the user space, you cannot interact directly with resources, only through the OS system calls. Note that system calls are very different from procedure calls.</p>
<p>Some examples of abstractions include:</p>
<ul>
<li>Files/filesystems abstract secondary storage.</li>
<li>Address spaces abstract primary memory.</li>
<li>Processes/threads abstract CPU and other executor resources.</li>
<li>Sockets/pipes abstract network and other messaging channels.</li>
</ul>
<p>In the 1940's, computers didn't use OSs - programmers would work directly on the hardware. By the 1950's, these huge, time-shared computers started getting monitoring and scheduling software, and critically, shared libraries and software - this was called batch processing. UNIX followed naturally from these shared libraries and utilities, first written in assembly, then C. Since UNIX was very accessible, and the source code was often available, improvements came rapidly and soon became dominant in the computing industry..</p>
<p>Five things make it possible for OSs to robustly manage software and hardware:</p>
<ul>
<li>Timers - to prevent infinite loops in applications from hanging the netire computer, we can give each application a certain number of time slices at a time, where control is returned to the OS once applications run out of time. This ensures that other software also gets a chance to run, and also that we can kill any runawway software.</li>
<li>Interrupts: to allow devices to notify the computer of events, like for keyboard input or timer triggers, we can use special CPU inputs that can immediately modify the control flow.</li>
<li>Memory protection - to stop one program from being able to modify others, we can use virtual mmeory and memory protection, to ensure that each application can work as if it has the entire address space to itself.</li>
<li>Kernel/user separation - to prevent bugs in user programs from messing up the entire system, there are special, privileged instructions that can only be accessed in kernel mode. In user mode applications, it is not possible to directly access resources, like secondary storage or networks. This ensures that the OS can enforce access restrictions.</li>
<li>Device independence - things like I/O buffering and asynchronous I/O mean that programs don't block on slow resource operations.</li>
</ul>
<h2 id="concurrency">Concurrency</h2>
<p><strong>Concurrency</strong> is the ability for multiple jobs to be running at the same time. Generally, this happens through multitasking (quickly switching between different jobs on a single processor) and multiprocessing (running different jobs on different processors). Jobs are simply threads from any process - theydon't all have to be from the same program.</p>
<p><strong>Processes</strong> represent full, independent programs. <strong>Threads</strong> represent different parts of the same program. The difference is that threads within a process share memmory, while processes do not - all threads within a process see the same global data and code. Each process has its very own address space - they can pretend they have all of the computer's memory to themselves.</p>
<h1 id="tutorial">10/5/16 - Tutorial</h1>
<p>In this course, we have apps running on top of OS/161 (our kernel), running in SYS/161 (the MIPS virtual machine for our kernel), on top of the host (our own machine). We will only really be working on the apps and OS/161. Specifically, we work mostly in the <code>os161-1.99/kern</code> folder.</p>
<p>Overview of Git. Overview of version control in general. GDB overview. Overview of the folder heirarchy in the OS/161 codebase.</p>
<h1 id="section-2">11/5/16</h1>
<p>Multi-threaded have multiple advantages over single-threaded ones:</p>
<ul>
<li>Efficiency - one thread can block on IO while another one can continue to do computation.</li>
<li>Multiprocessing - more processors can be used at a time.</li>
<li>Responsiveness - one thread can be used to keep the use interface responsive while heavy computation is going on.</li>
<li>Modularity - threads are good way to separate converns.</li>
<li>Priority scheduling - thread scheduling can be controlled in a very fine-grained way.</li>
</ul>
<p>The most important operation in threading is the thread switch - running one thread for, say, 20ms, then switching to another thread so it can run for another 20ms. When we switch threads, we need to keep track of and set the correct values of the program counter, registers, the stack pointer, the frame pointer, and so on. Each thread needs its own stack.</p>
<p>When a thread switch occurs, we need to back up the registers and other thread data on the thread's stack, and then load in the other thread's data from that thread's stack.</p>
<p>In OS/161, we have a particular register allocation when we're making function calls. This is actually a very common calling convention in MIPS, and a lot of OSs follow something that is compatible with it:</p>
<ul>
<li>R0 (<code>ZERO</code>) - is always 0</li>
<li>R1 (<code>AT</code>) - assembly temporary variable, reserved for use by the assembler</li>
<li>R2 (<code>V0</code>) - return value or system call number</li>
<li>R3 (<code>V1</code>) - return value of function call (functions can return up to two 32-bit values)</li>
<li>R4-R7 (<code>A0</code>-<code>A3</code>) - functiona call arguments 1 to 4, respectively (the rest of the arguments, if any, go on the stack)</li>
<li>R8-R15 (<code>T0</code>-<code>T7</code>) - temporary variables (might be clobbered by subroutines, but not by thread switches)</li>
<li>R24-R25 (<code>T8</code>-<code>T9</code>) - more temporary variables (might be clobbered by subroutines, but not by thread switches)</li>
<li>R16-R23 (<code>S0</code>-<code>S7</code>) - variables (preserved by thread switches and subroutines - they'll save them before changing their value, and restore them when returning if necessary)</li>
<li>R26-R27 (<code>K0</code>-<code>K1</code>) - reserved by interrupt handlers</li>
<li>R28 (<code>GP</code>) - global pointer, useful for accessing certain variables</li>
<li>R29 (<code>SP</code>) - stack pointer, pointer to the top of the stack</li>
<li>R30 (<code>FP</code>) - frame pointer, pointer to the beginning of the topmost frame (changes every function call or return)</li>
<li>R31 (<code>RA</code>) - return address, used by the JAL instruction</li>
</ul>
<p>Threads are associated with the control state of a running program, known as the thread <strong>context/state</strong>. This contains the CPU state (program counter, stack pointer, registers, and whether it's user mode or kernel mode), and the stack state (the stack lives in the address space of the process). Basically, all the information we need to pause the running program and restart it from where it left off later.</p>
<p>Note that each processor on the computer can run one thread at a time. With true concurrency, we lose program determinism - the same program run twice with the same inputs may not behave the same way each time. The hard part of concurrency is ensuring both program safety (correctness) and liveness (efficiency).</p>
<p>In OS/161, we have a built in threading library in <code>kern/include/thread.h</code>. This exposes a simple way to create a new thread given a function (through forking), kill the current thread, and yield the current thread to allow the next one to run (voluntarily causing a context switch). There are a couple of usage examples in the slides.</p>
<h1 id="section-3">13/5/16</h1>
<p>Summary of threading interface. Overview of assignment 1 and how to go about doing it: traffic simulation where we make sure that vehicles, represented by threads, are simulated in a way such that they get around without colliding. Half of the assignment is implementing the synchronization primitives, and the other half is using those primitives to build applications.</p>
<p>Pausing one thread and resuming another is called a <strong>context switch</strong>. Basically, we decide which thread to switch to (<strong>scheduling</strong>), then save the current thread's context and restore the next thread's context (dispatching). Usually, dispatching has to be done in assembly, since the way it's done varies significantly based on the architecture.</p>
<p>A simple way to do scheduling is <strong>round robin scheduling</strong>: the scheduler maintains a list of threads, and then gives each one a time slice in turn. This is often implemented as a queue, where on each context switch, the first thread in the queue is given a time slice and then moved to the back of the queue.</p>
<p>Threads can be running (currently executing), ready (waiting in the queue to execute), or blocked/sleeping (waiting for something to happen or a resource to be available, such as blocking I/O, sleeping for a timer, or waiting for a lock to be released). In some cases, a thread can also go into a special zombie state, which means that the thread should never be run again, but is still kept around because it possibly has useful information.</p>
<p>Dispatching turns a ready thread into a running thread, while yielding or other context switching converts a running thread into a ready thread. Blocking operations (namely, <code>wchan_sleep</code>) converts a running thread into a blocked/sleeping thread, and completion of that operation (namely, <code>wchan_wakeone</code> and <code>wchan_wakeall</code>) turns a blocked/sleeping thread into a ready thread.</p>
<p>A thread can voluntarily pause (by yielding - cooperatively), or involuntarily pause (by being <strong>preempted</strong>). The operating system gives each thread a certain amount of time, and if it takes too long, the operating system does a context switch (usually using interrupts). Preemption makes sure that all threads have fair access to the processor. When we're implementing the scheduler in OS/161, we can have an interrupt fire at regular intervals, check how long the current thread has been running, then force a context switch if it's longer than its assigned quantum.</p>
<p>In OS/161, <code>thread_yield</code> calls <code>thread_switch</code>, which calls <code>switchframe_switch</code>, which is written in assembly. This pushes a special kind of stack frame onto the thread's stack, known as a <strong>switch frame</strong>, that saves the thread's context (registers, etc.). When we switch back to that thread, we can just restore the context from that switchframe.</p>
<p>In OS/161, a preemptive context switch can (from the perspective of a thread) happen at any time, since it is triggered by an interrupt. When this happens, we push a special kind of stack frame into the thread's stack, known as the <strong>trap frame</strong>, that saves all of the thread registers and context. This is different from a switch frame because a thread frame is planned by the thread, so the switch frame doesn't need to save things like temporary variables, while the trap frame does.</p>
<p>The preemptive context switch is still done using <code>thread_yield</code>, just like a voluntary yield - when a thread is preempted, we push both a trap frame and a switch frame to the stack.</p>
<p>Why do we need both a trap frame and a switchframe? Well, the trap frame comes before the interrupt handler, which could call a bunch of other functions, so in general we won't know where that trap frame is relative to the stack. Plus, the <code>thread_yield</code> function probably needs to clobber things like the status register. Finally, it's possible to have a trap frame without a switchframe (when the interrupt is for non-thread-switching purposes such as PS2 keyboard input). The switchframe is convenient because we can then assume that the top of the stack s always a switchframe when we're dispatching a thread.</p>
<p>Round robin scheduling by itself is cooperative - there is no preemption. <strong>Preemptive round robin scheduling</strong> is round robin with preemption. The time slice that is allocated to a thread when it's dispatched is called a <strong>scheduling quantum</strong>. The time slice is often based on a multiple of the processor tick (a timer that regularly fires, usually at something like 1 ms, and context switching occurs at these points, usually at something like 20 ms).</p>
<p>The downside of round robin is that it doesn't work with priorities - we want important threads to be scheduled more often than less important ones.</p>
<h1 id="section-4">18/5/16</h1>
<p>To summarize: computer resources are shared using processes and threads. Threads can switch cooperatively by yielding, or by being preempted. A switch frame is pushed for any thread switch, while a trap frame only occurs for a thread preemption. Switch frames only save the usual function-call-persistent registers, while trap frames need to save all the registers.</p>
<p>A <strong>wait channel</strong> is an object that a thread can subscribe to and go to sleep. When the wait channel gets activated, any threads that are waiting on that wait channel get woken up and put in the ready or running queue. ;wip: which one? For example, if a thread wants to wait for a printing job to finish, it would wait on the printer wait channel and get woken up again when the printer is done. Wait channels have two operations: putting something on the wait channel, and taking something off the wait channel.</p>
<p>Threads need to store their name, the wait channel that it's listening on, their state, the CPU it's on, the process it belongs to, their call stack, and a few other things that the OS needs to keep track of threads.</p>
<p>The operating system will be running on each core. Each core has its own stack.</p>
<h2 id="synchronization">Synchronization</h2>
<p>Threads share access to system resources like the hard drive via the operating system, and also program data (global variables). Synchronization is a set of techniques for making concurrent operations on shared resources safe and ensures that programs remain correct.</p>
<p>If we have a global variable, and two threads read it, add 1, and then write it back (<code>x ++</code>), it's possible for one thread to get preempted after it's read the variable but before it writes back the new value - the program would only update the global once rather than twice! Since thread execution is non-deterministic, the thread could get preempted at any time, and code from different threads can be interleaved arbitrarily. Synchornization is needed to ensure that instructions are executed in the correct order.</p>
<p><strong>Mutual exclusion</strong> is the technique of only allowing one thread to access a shared resource at a time, ensuring that it doesn't get messed with by other threads.</p>
<p>The <strong>critical section</strong> is the section of code that accesses shared resources.</p>
<p>In C and C++, the <code>volatile</code> keyword applied to a variable ensures that it's always loaded and and stored directly in RAM every single time it's read or written in the code, and that this property should never be optimized away by the compiler. Among other things, this ensures that variable accesses are not lifted out of loops (loop invariant code motion), and that the variable is never assigned to a register.</p>
<p>For example, <code>int volatile x;</code>. This should be used for any variable that can be changed by anything other than our own thread's code, such as the keyboard state (the OS can change this for us) or variables shared between threads (the other threads can change this for us).</p>
<p>In the two threads example earlier with a global variable, what we want is to signal that a thread is going into a ciritcal region for a resource. If this signal is set already by the time we're trying to enter the critical section, then we want our thread to wait until the signal is stopped and the other thread is done its critical section. Synchronization is a technique for making sure different critical sections don't step on each other's toes.</p>
<p>In this course we have four synchronization primitives for this purpose:</p>
<ul>
<li>Spinlocks.
<ul>
<li>OS/161 uses only spinlocks, while user applications on top of OS/161 use the other primitives.</li>
<li>These are built into OS/161 already, and we'll be using them to implement locks, semaphores, and condition variables, by combining them with wait channels and thread sleeping.</li>
<li>When a thread tries to acquire a spinlock, it blocks until it's no longer already held (or doesn't block at all if it's already free), and then continues on.</li>
<li>When a spinlock is blocking, it basically continuously loops until the lock is released by another thread - a loop that keeps looping until a variable value changes from, say, true to false. While inefficient, it's acceptable for short waits (a few clock cycles).</li>
<li>Spinlocks make use of low-level, dedicated instructions that can ensure that no other instructions run while they're running.</li>
<li>When we're spinning in a spinlock, we generally want to disable interrupts, because if it's spinning while we do a context switch and another thread tries to acquire the same spinlock, the threads could potentially deadlock.</li>
</ul></li>
<li>Locks/mutexes.
<ul>
<li>Spinlocks and locks are used in very similar ways, but locks block the thread instead of spinning it, which is a lot more efficient.</li>
<li>We first declare locks that are used by our threads. Generally, we'd want one lock for each shared data structure or device. Each lock is almost always associated with a thing that we say it locks.</li>
<li>As soon as we acquire a lock, any other threads that attempt to acquire it will block and go to sleep.</li>
<li>So while we hold the lock, we can assume we have exclusive access to the thing it locks (because if it wasn't exclusive, other threads would have to have been able to acquire a lock for it, which they can't while we hold the lock), and perform operations as if we had non-concurrent code.</li>
<li>When our operations on the locked thing are done, we can release the lock, which would notify the lock's wait channel to wake up all the other threads that are waiting to acquire this lock.</li>
<li>The thread that releases the lock must always be the same one that acquired it. It is possible to check if the current thread is the one holding the lock.</li>
<li>When we're implementing these, we'll use spinlocks to lock and unlock wait channels. These are implemented as <code>wchan_lock</code> and <code>wchan_unlock</code>.</li>
</ul></li>
<li>Semaphores.
<ul>
<li>The semaphore also enforces synchronization, but it can solve more types of problems than just locks - semaphores are generalizations of locks.</li>
<li>These are built into OS/161 already, but not completely.</li>
<li>A semaphore has an integer value, which can be incremented or decremented. This value is always non-negative, and represents the remaining capacity for the semaphore's concurrency - how many more threads can simultaneously hold the semaphore.</li>
<li>When a thread tries to procure/acquire a semaphore, it will block until the integer value is positive (or won't block at all if it's already positive). Regardless of the value, the semaphore value is decremented afterward.</li>
<li>When a thread tries to vacate/release a semaphore, it increments the semaphore value.</li>
<li>Basically, a semaphore allows <span class="math inline">\(n\)</span> threads to hold on to a resource at a time, and other threads get blocked when they try to acquire it. It acts like a fixed-size buffer for a resource.</li>
<li>A binary semaphore always has a value between 0 and 1. A counting semaphore can have any valid semaphore value. A binary semaphore is basically a lock, except it blocks when its value is 0, whereas locks block when their value is 1.</li>
<li>The starting value of a semaphore determines the total number of threads that can acquire it at the same time.</li>
<li>A good analogy for semaphores is the grocery store checkout line, where the semaphore starting value is the number of cashiers. Each person in the line attempts to procure a cashier, blocks (waits in line) if there's no cashiers available, and then when one does become available, goes to the cashier, decreasing the number of available cashiers by 1.</li>
</ul></li>
<li>Condition variables.
<ul>
<li>The condition variable also enforces synchronization, but condition variables are generalizations of semaphores. Note that condition variables can be implemented using semaphores.</li>
<li>When a thread tries to wait/acquire a condition variable, it will block the thread until the condition variable gets signalled.</li>
<li>When a thread tries to signal a condition variable, it will block, and anything that was blocked on this condition variable at that time will be woken up.</li>
<li>Basically, waiting on a condition variable means blocking until a signal comes on it. This lets us wait until a condition comes true.</li>
<li>We always need to lock condition variables when we're waiting (or deciding whether to wait) or signalling. This is because there can be subtle race conditions that occur when threads get interleaved between checking whether to wait and actually waiting.</li>
</ul></li>
</ul>
<h1 id="section-5">20/5/16</h1>
<p>;wip: get materials from LEARN</p>
<p>Blocked threads can only be woken up by a signal - since they never execute themselves, they can't wake themselves up.</p>
<p>In modern OSs, when the CPU is idle, almost all of the hundreds or thousands of threads are blocked.</p>
<p>There is one wait channel for each lock instance - every critical section. Wait channels can have multiple things waiting on them. In OS/161, we can either wake up one thread waiting on the wait channel, or all of them. To implement thread priority, we could unblock threads on wait channels in order of their priority.</p>
<p>Every wait channel can be locked using a spinlock that's associated with it. Most wait channel functions require spinlocking the wait channel while they're actually updating things like lists of threads.</p>
<p>Best practices for locking is to be as granular as possible - ideally, there should be as little code that runs while locked as possible, and as few locks as possible. The usual pattern is to copy data structures that need to be shared into local, non-shared memory, then release the lock before processing the local copy - this is often useful for ensuring that the data structure stays locked for the minimum time possible.</p>
<p><code>wchan_sleep</code> is very similar to <code>thread_yield</code> in that it gives up the thread's reamining quantum. However, <code>thread_yield</code> makes the thread become ready, while <code>wchan_sleep</code> makes the thread become blocked.</p>
<p>When we attempt to procure a semaphore, we spinlock the semaphore instance, and while the semaphore value is 0, we'll temporarily release the spinlock and block the current thread.</p>
<p>Semaphores are often used when we're trying to synchronize producers and consumers, to make sure that there is at least one item queued up before consumers try to read them. Producers push items to the queue and vacate a spot for a consumer, while consumers procure a spot and take an item from the queue. This ensures that consumers will properly block if there are no spots available.</p>
<p>If the queue must be a bounded size, we could also use a second semaphore to enforce that only so many spots (spaces for items) are available on the queue. Basically producers would procure a spot in the queue before adding it to the queue, and consumers would vacate a spot on the queue after it takes an item from the queue.</p>
<h1 id="section-6">25/5/16</h1>
<p>;wip: missed due to interviews</p>
<h1 id="section-7">27/5/16</h1>
<p>The spinlock holder (<code>lk_holder</code>) is a CPU rather than a thread, because the spinlock only runs on a single CPU.</p>
<p>To acquire a spinlock, we first disable interrupts on this CPU, to prevent other threads from running (at least, on this CPU), then enter an infinite loop.</p>
<p>Then, we try to atomically test and set the spinlock's locked field to true using special assembly instructions. This has to be atomic because we don't want to risk the possibility of another thread acquiring the spinlock in between us testing the value of the locked field and setting it to true.</p>
<p>Basically, in the infinite loop, we repeatedly check if the spinlock's locked flag is false, and if it is, we set it to true. If we do set it to true, we break out of the loop because we've managed to acquire the lock.</p>
<p>In MIPS, we can implement the atomic test and set using two ASM instructions <code>ll</code> (load linked) and <code>sc</code> (store conditional). The basic strategy is to check whether another thread came in and changed things between the <code>ll</code> and <code>sc</code>, and if it did, we assume the locked flag is already set to true by the other thread and don't set.</p>
<p>To release a spinlock, we set the spinlock's locked flag to false, and then re-enable interrupts on the CPU.</p>
<p>Spinlocks are nice for short waits, because they only take a few instructions to set up. However, it wastes CPU time for longer waits, and it's also possible for some threads to starve - if multiple threads are trying to get a lock, it's possible for some of them to consistently acquire the lock, while others never get the chance to. With locks in OS/161, we ensure that locks are acquired in FIFO order, so that if a thread tries to acquire a lock, it is guaranteed that it eventually will.</p>
<h3 id="synchronization-pitfalls">Synchronization Pitfalls</h3>
<p><strong>Starvation</strong> can occur whenever some threads have a higher chance of getting a lock, while some other threads do not. The threads that are less likely to acquire the lock might never get it at all, in certain cases - those threads starve. For locks in OS/161, we ensure that starvation never occurs by using FIFO order for lock acquiring - if a thread attempts to acquire a lock, it is guaranteed that eventually it will manage to if all locks are eventually released.</p>
<p>A <strong>deadlock</strong> is a cycle of two or more locks, and results in indefinite waiting. For example, thread A waits for thread B to release something, while thread B is waiting for thread A to release something else - they depend on each other, and are both stuck.</p>
<p>One example of deadlock is two threads that each allocate 40% of the computer's memory, and then each try to allocate another 30%, but both get blocked by the kernel since there isn't enough memory. Each thread is waiting for the other thread to release its original 40% of the memory so it can allocate its additional 30%.</p>
<p>To prevent deadlocks, we can force all threads to request resoruces all at once (the &quot;no hold and wait&quot; strategy), take away resources from some of the threads (the &quot;preemption strategy&quot;, which isn't usually feasible), or assign an order to resources, and force all threads to request resources in ascending order, which ensures that a cycle can't occur.</p>
<p>To detect deadlock, we can treat the locks as vertices of a directed graph, and a lock B acquired while lock A is held is a directed edge from A to B. Every so often, we can do a depth-first search to detect cycles, and usually, terminate one or more of the threads that are in the deadlock to break the cycle.</p>
<p>A <strong>race condition</strong> is when a computation's result unexpectedly depends on the order that we execute the threads in.</p>
<p>Most architectures have a few atomic operations to help us implement synchronization: disabling interrupts, test-and-set (test if a flag is false, and if it is set it to true and return whether we actually set it to true), and compare-and-swap (compare two values, and swap them if they're different and return whether we swapped them).</p>
<p>A <strong>process</strong>, like a thread, also abstracts program execution, but they are somewhat more isolated from each other. Notably, different processes have different address spaces, so processes don't share global variables. Processes can contain multiple threads. Additionally, many different OS resources are often associated with the current process rather than the current thread, like files and sockets.</p>
<p>A process is essentially a collection of one or more threads, an address space, and a collection of associated resources. A process is <strong>concurrent</strong> if it has multiple threads, and is <strong>sequential</strong> otherwise.</p>
<p>While multiprocessing is the concept of programs using multiple processes/cores, multiprogramming is the concept of programs using multiple processes. Multiprogramming allows the OS to be robust against out of control programs - if the address spaces are isolated, programs can't mess up the OS's memory, and if resources are associated with processes, the OS can control and multiplex access to the actual resources to ensure no program can hog all of it.</p>
<p>For our purposes, the OS is the kernel plus a bunch of utility functions and libraries that use usable by user programs. The kernel alone runs in privileged mode, as a single program with possibly multiple threads, in order to control hardware and protect itself. Privileged mode can run additional, special instructions (like <code>halt</code> and the ability to access certain restricted regions of memory) that can't be run in user mode. The kernel exposes some of these through controlled, secure interfaces called <strong>system calls/syscalls</strong>.</p>
<p>Some examples of system calls are fork/exec, fopen/fclose, and mkdir/rm/mv.</p>
<p>A syscall works by switching to kernel mode, saving the thread context (like the program counter) on the kernel's stack, and then jumping to a specific, fixed memory address specified in hardware in the kernel memory space. At this location we have a <strong>system call handler</strong>, which can read the cause register <code>C0</code> to figure out which system call to perform. After performing the system call, the handler restores the thread context and switches the processor back into user mode.</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>