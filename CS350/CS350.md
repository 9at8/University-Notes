CS350
=====

Operating systems.

    Kevin Lanctot
    Section 003
    Email: klanctot@uwaterloo.ca
    Office Hours: Wednesdays at 2:00 PM-3:30 PM in DC 2131
    Website: https://www.student.cs.uwaterloo.ca/~cs350/S16/
    Wednesdays/Fridays 4:00 PM in MC 4040

**Note:** I made a Docker image containing a full [development environment](https://github.com/Uberi/uw-cs350-development-environment#readme), current as of Sprint 2016. This is basically all of the tools you need to complete the CS350 coursework, in a format that will work without any fuss. Notably, this image fixes several issues with setting up compiler versions and ncurses incompatibilities.

# 4/5/16

Assignments and questions will be distributed via Piazza.

There are 5 assignments, each one building on top of the previous ones. Assignments will be software that runs on top of the OS/161 operating system, on top of a MIPS VM, SYS/161. There are 5 slip days that can be used to extend assignment deadlines, and 3 or fewer can be used for any single assignment.

The role of the operating system is to both help applications do what they need to do, and to stop applications from doing what they shouldn't. For example, an operating system might expose a nice abstract filesystem for storing data in, but it might prevent applications from, say, writing to a file that was being written to by another application.

To applications, the OS provides an execution environment, with interfaces to various resources, like networking, storage, and graphics (most of the code in a modern operating system lives in the various drivers). The OS is also responsible for isolating different applications from each other, making sure they don't step on each other's toes. This is the **application view** of operating systems.

The OS is responsible for keeping all of the hardware in line, making sure that these resources get allocated among the programs in a fair way. This is the **system view** of operating systems.

The OS is also a concurrent, real-time program (at least, most modern OSs). It needs to be able to support multiple things going on at once, but also satisfy timing constraints, such as feeding audio data to the sound card to output uninterrupted sound. This is the **implementation view** of the operating system.

# 6/5/16

The OS **kernel** is the part of the OS that responds to system calls, interrupts, and exceptions. The rest of the operating system includes all of the utilities, shells, and programming libraries.

Applications run on top of the OS, which in turn runs on top of the hardware. Applications interact with the OS kernel via system calls, and the OS interacts with the hardware through commands, data, and interrupts. The user space is all of the parts above the kernel, and is isolated by the kernel from the actual resources - in the user space, you cannot interact directly with resources, only through the OS system calls. Note that system calls are very different from procedure calls.

Some examples of abstractions include:

* Files/filesystems abstract secondary storage.
* Address spaces abstract primary memory.
* Processes/threads abstract CPU and other executor resources.
* Sockets/pipes abstract network and other messaging channels.

In the 1940's, computers didn't use OSs - programmers would work directly on the hardware. By the 1950's, these huge, time-shared computers started getting monitoring and scheduling software, and critically, shared libraries and software - this was called batch processing. UNIX followed naturally from these shared libraries and utilities, first written in assembly, then C. Since UNIX was very accessible, and the source code was often available, improvements came rapidly and soon became dominant in the computing industry..

Five things make it possible for OSs to robustly manage software and hardware:

* Timers - to prevent infinite loops in applications from hanging the netire computer, we can give each application a certain number of time slices at a time, where control is returned to the OS once applications run out of time. This ensures that other software also gets a chance to run, and also that we can kill any runawway software.
* Interrupts: to allow devices to notify the computer of events, like for keyboard input or timer triggers, we can use special CPU inputs that can immediately modify the control flow.
* Memory protection - to stop one program from being able to modify others, we can use virtual mmeory and memory protection, to ensure that each application can work as if it has the entire address space to itself.
* Kernel/user separation - to prevent bugs in user programs from messing up the entire system, there are special, privileged instructions that can only be accessed in kernel mode. In user mode applications, it is not possible to directly access resources, like secondary storage or networks. This ensures that the OS can enforce access restrictions.
* Device independence - things like I/O buffering and asynchronous I/O mean that programs don't block on slow resource operations.

Concurrency
-----------

**Concurrency** is the ability for multiple jobs to be running at the same time. Generally, this happens through multitasking (quickly switching between different jobs on a single processor) and multiprocessing (running different jobs on different processors). Jobs are simply threads from any process - theydon't all have to be from the same program.

**Processes** represent full, independent programs. **Threads** represent different parts of the same program. The difference is that threads within a process share memmory, while processes do not - all threads within a process see the same global data and code. Each process has its very own address space - they can pretend they have all of the computer's memory to themselves.

# 10/5/16 - Tutorial

In this course, we have apps running on top of OS/161 (our kernel), running in SYS/161 (the MIPS virtual machine for our kernel), on top of the host (our own machine). We will only really be working on the apps and OS/161. Specifically, we work mostly in the `os161-1.99/kern` folder.

Overview of Git. Overview of version control in general. GDB overview. Overview of the folder heirarchy in the OS/161 codebase.

# 11/5/16

Multi-threaded have multiple advantages over single-threaded ones:

* Efficiency - one thread can block on IO while another one can continue to do computation.
* Multiprocessing - more processors can be used at a time.
* Responsiveness - one thread can be used to keep the use interface responsive while heavy computation is going on.
* Modularity - threads are good way to separate converns.
* Priority scheduling - thread scheduling can be controlled in a very fine-grained way.

The most important operation in threading is the thread switch - running one thread for, say, 20ms, then switching to another thread so it can run for another 20ms. When we switch threads, we need to keep track of and set the correct values of the program counter, registers, the stack pointer, the frame pointer, and so on. Each thread needs its own stack.

When a thread switch occurs, we need to back up the registers and other thread data on the thread's stack, and then load in the other thread's data from that thread's stack.

In OS/161, we have a particular register allocation when we're making function calls. This is actually a very common calling convention in MIPS, and a lot of OSs follow something that is compatible with it:

* R0 (`ZERO`) - is always 0
* R1 (`AT`) - assembly temporary variable, reserved for use by the assembler
* R2 (`V0`) - return value or system call number
* R3 (`V1`) - return value of function call (functions can return up to two 32-bit values)
* R4-R7 (`A0`-`A3`) - functiona call arguments 1 to 4, respectively (the rest of the arguments, if any, go on the stack)
* R8-R15 (`T0`-`T7`) - temporary variables (might be clobbered by subroutines, but not by thread switches)
* R24-R25 (`T8`-`T9`) - more temporary variables (might be clobbered by subroutines, but not by thread switches)
* R16-R23 (`S0`-`S7`) - variables (preserved by thread switches and subroutines - they'll save them before changing their value, and restore them when returning if necessary)
* R26-R27 (`K0`-`K1`) - reserved by interrupt handlers
* R28 (`GP`) - global pointer, useful for accessing certain variables
* R29 (`SP`) - stack pointer, pointer to the top of the stack
* R30 (`FP`) - frame pointer, pointer to the beginning of the topmost frame (changes every function call or return)
* R31 (`RA`) - return address, used by the JAL instruction

Threads are associated with the control state of a running program, known as the thread **context/state**. This contains the CPU state (program counter, stack pointer, registers, and whether it's user mode or kernel mode), and the stack state (the stack lives in the address space of the process). Basically, all the information we need to pause the running program and restart it from where it left off later.

Note that each processor on the computer can run one thread at a time. With true concurrency, we lose program determinism - the same program run twice with the same inputs may not behave the same way each time. The hard part of concurrency is ensuring both program safety (correctness) and liveness (efficiency).

In OS/161, we have a built in threading library in `kern/include/thread.h`. This exposes a simple way to create a new thread given a function (through forking), kill the current thread, and yield the current thread to allow the next one to run (voluntarily causing a context switch). There are a couple of usage examples in the slides.

# 13/5/16

Summary of threading interface. Overview of assignment 1 and how to go about doing it: traffic simulation where we make sure that vehicles, represented by threads, are simulated in a way such that they get around without colliding. Half of the assignment is implementing the synchronization primitives, and the other half is using those primitives to build applications.

Pausing one thread and resuming another is called a **context switch**. Basically, we decide which thread to switch to (**scheduling**), then save the current thread's context and restore the next thread's context (dispatching). Usually, dispatching has to be done in assembly, since the way it's done varies significantly based on the architecture.

A simple way to do scheduling is **round robin scheduling**: the scheduler maintains a list of threads, and then gives each one a time slice in turn. This is often implemented as a queue, where on each context switch, the first thread in the queue is given a time slice and then moved to the back of the queue.

Threads can be running (currently executing), ready (waiting in the queue to execute), or blocked/sleeping (waiting for something to happen or a resource to be available, such as blocking I/O, sleeping for a timer, or waiting for a lock to be released). In some cases, a thread can also go into a special zombie state, which means that the thread should never be run again, but is still kept around because it possibly has useful information.

Dispatching turns a ready thread into a running thread, while yielding or other context switching converts a running thread into a ready thread. Blocking operations (namely, `wchan_sleep`) converts a running thread into a blocked/sleeping thread, and completion of that operation (namely, `wchan_wakeone` and `wchan_wakeall`) turns a blocked/sleeping thread into a ready thread.

A thread can voluntarily pause (by yielding - cooperatively), or involuntarily pause (by being **preempted**). The operating system gives each thread a certain amount of time, and if it takes too long, the operating system does a context switch (usually using interrupts). Preemption makes sure that all threads have fair access to the processor. When we're implementing the scheduler in OS/161, we can have an interrupt fire at regular intervals, check how long the current thread has been running, then force a context switch if it's longer than its assigned quantum.

In OS/161, `thread_yield` calls `thread_switch`, which calls `switchframe_switch`, which is written in assembly. This pushes a special kind of stack frame onto the thread's stack, known as a **switch frame**, that saves the thread's context (registers, etc.). When we switch back to that thread, we can just restore the context from that switchframe.

In OS/161, a preemptive context switch can (from the perspective of a thread) happen at any time, since it is triggered by an interrupt. When this happens, we push a special kind of stack frame into the thread's stack, known as the **trap frame**, that saves all of the thread registers and context. This is different from a switch frame because a thread frame is planned by the thread, so the switch frame doesn't need to save things like temporary variables, while the trap frame does.

The preemptive context switch is still done using `thread_yield`, just like a voluntary yield - when a thread is preempted, we push both a trap frame and a switch frame to the stack. This is because the trap frame comes before the interrupt handler and thread switching stack frame, so in general, we won't know where that trap frame is relative to the stack. Plus, the `thread_yield` function probably needs to clobber things like the status register. Doing it this way allows us to assume that the top of the stack is always a switch frame when we're dispatching a thread.

Round robin scheduling by itself is cooperative - there is no preemption. **Preemptive round robin scheduling** is round robin with preemption. The time slice that is allocated to a thread when it's dispatched is called a **scheduling quantum**. The time slice is often based on a multiple of the processor tick (a timer that regularly fires, usually at something like 1 ms, and context switching occurs at these points, usually at something like 20 ms).

The downside of round robin is that it doesn't work with priorities - we want important threads to be scheduled more often than less important ones.

# 18/5/16

To summarize: computer resources are shared using processes and threads. Threads can switch cooperatively by yielding, or by being preempted. A switch frame is pushed for any thread switch, while a trap frame only occurs for a thread preemption. Switch frames only save the usual function-call-persistent registers, while trap frames need to save all the registers.

A **wait channel** is an object that a thread can subscribe to and go to sleep. When the wait channel gets activated, any threads that are waiting on that wait channel get woken up and put in the ready or running queue. ;wip: which one? For example, if a thread wants to wait for a printing job to finish, it would wait on the printer wait channel and get woken up again when the printer is done.

Threads need to store their name, the wait channel that it's listening on, their state, the CPU it's on, the process it belongs to, their call stack, and a few other things that the OS needs to keep track of threads.

The operating system will be running on each core. Each core has its own stack.

Synchronization
---------------

Threads share access to system resources like the hard drive via the operating system, and also program data (global variables). Synchronization is a set of techniques for making concurrent operations on shared resources safe and ensures that programs remain correct.

If we have a global variable, and two threads read it, add 1, and then write it back (`x ++`), it's possible for one thread to get preempted after it's read the variable but before it writes back the new value - the program would only update the global once rather than twice! Since thread execution is non-deterministic, the thread could get preempted at any time, and code from different threads can be interleaved arbitrarily. Synchornization is needed to ensure that instructions are executed in the correct order.

**Mutual exclusion** is the technique of only allowing one thread to access a shared resource at a time, ensuring that it doesn't get messed with by other threads.

The **critical section** is the section of code that accesses shared resources.

In C and C++, the `volatile` keyword applied to a variable ensures that it's always loaded and and stored directly in RAM every single time it's read or written in the code, and that this property should never be optimized away by the compiler. Among other things, this ensures that variable accesses are not lifted out of loops, and that the variable is never assigned to a register.

For example, `int volatile x;`. This should be used for any variable that can be changed by anything other than our own thread's code, such as the keyboard state (the OS can change this for us) or variables shared between threads (the other threads can change this for us).

In the two threads example earlier with a global variable, what we want is to signal that a thread is going into a ciritcal region for a resource. If this signal is set already by the time we're trying to enter the critical section, then we want our thread to wait until the signal is stopped and the other thread is done its critical section. The lock is a synchronization primitive that does this.

In this course we have four synchronization primitives for this purpose:

* Spinlocks.
    * OS/161 uses only spinlocks, while user applications on top of OS/161 use the other primitives.
    * These are built into OS/161 already, and we'll be using them to implement locks, semaphores, and condition variables, by combining them with wait channels and thread sleeping.
    * This basically spins a thread in place until the lock is released - a loop that keeps looping until a variable value changes from, say, true to false. While very inefficient, it's acceptable for short waits (a few clock cycles).
    * Spinlocks make use of low-level, dedicated instructions that can ensure that no other instructions run while they're running.
* Locks.
    * Spinlocks and locks are used in very similar ways.
    * We first declare locks that are used by our threads. Generally, we'd want one lock for each shared data structure or device. Each lock is almost always associated with a thing that we say it locks.
    * As soon as we acquire a lock, any other threads that attempt to acquire it will block and go to sleep.
    * So while we hold the lock, we can assume we have exclusive access to the thing it locks (because if it wasn't exclusive, other threads would have to have been able to acquire a lock for it, which they can't while we hold the lock), and perform operations as if we had non-concurrent code.
    * When our operations on the locked thing are done, we can release the lock, which would notify the lock's wait channel to wake up all the other threads that are waiting to acquire this lock.
    * The thread that releases the lock must always be the same one that acquired it. It is possible to check if the current thread is the one holding the lock.
    * When we're implementing these, we'll use spinlocks to lock and unlock wait channels. These are implemented as `wchan_lock` and `wchan_unlock`.
* Semaphores.
    * These are built into OS/161 already, but not completely.
* Condition variables.

A **deadlock** is a cycle of locks, and results in indefinite waiting. For example, thread A waits for thread B to release something, while thread B is waiting for thread A to release something else.