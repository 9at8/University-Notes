CS240
=====

Data Structures and Data Management.

    Therese Beidl
    Section 002
    Email: beidl@uwaterloo.ca
    Web: http://www.student.cs.uwaterloo.ca/~cs240
    ISA: Joseph (Danny) Sitter
    ISA Email: cs240@studennt.cs.uwaterloo.ca

$$
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
$$

Seciton 1 and 2 are the regular classes, and section 3 is the enhanced section and has very different content.

Midterm at Thurs. Feb. 26 at 4:30 PM, worth 25%. Final exam worth 50%. Must pass weighted average of exam to pass the course. 5 assignments, each worth 5%.

# 6/1/15

Assignment 0 is due on Tuesdays. Assignment 0 is due on Jan. 13. ;wip: do it

Suppose we have a lot of data to keep track of. We could store it in an array/list, but depending on the type of the data, this might not be the best choice. Data structures should make it easy and efficient to perform the operations we need. For example, an English dictionary probably needs to be efficiently searched, but we don't really need to care about insertion and deletion since they're so rare.

The best data structure for something depends on the type of data we want to store. Our goal is to have a short running time and little memory.

In this course we will be performing theoretical analysis, developing ideas and pseudocode (and sometimes, implementations), and analyzing them using tools like big-O notation.

An Abstract Data Type is the idea of describing something by what it can do, not how it does it.

Required background includes arrays, linked lists, strings, stacks, queues, ADTs, recursive algorithms, trees, sorting algorithms (insertion, selection, quick, merge), binary search/BSTs, arithmetic series, geometric series, harmonic series ($\frac 1 1 + \ldots + \frac 1 n = \ln n$).

In this course, $\log n$ is implicitly $\log_2 n$ - all logarithms are base 2 unless otherwise stated.

A **problem** is a description of a general situation and the desired outcome. For example, the sorting problem is "given comparable values, put them in sorted order".

A **problem instance** is a particular input to a problem, like a particular array of numbers to sort for the sorting problem.

A **problem solution** is a change/process that, given the situation of a problem instance, results in the desired outcome. For example, the sorted array of numbers for the sorting problem.

We **solve** a problem by giving the correct algorithm for it. A solution is **correct** if it finds a solution for every possible input that can be given.

An **algorithm** is a finite description of a process that gives an answer (a solution that is not necessarily correct) for all possible instances of a problem.

**Efficiency** usually refers to an algorithm's runtime, and sometimes its memory usage. It may also refer to things specific to the problem domain, like the number of comparisons done.

To solve a problem:

1. Design an algorithm.
  1. Write down the main idea of the algorithm in plain prose. For example, "for increasing $i$, make $A[0 \ldots i]$ sorted by inserting $A[i]$ into a sorted $A[0 \ldots i - 1]$" for the sorting problem.
  2. Optionally, write pseudocode - code that might not be a real language, and is something like a mix between prose and code. This is a more precise way of specifying an algorithm. Any consistent, precise, and clearly understandable language will be accepted as pseudocode.

        Preconditions: an array `A[0 ... n - 1]`$.
        Postconditions: `A` is sorted
        for i in 1 ... n - 1:
          value = A[i]
          j = i - 1
          while j >= 0 and A[j] > key:
            A[j + 1] = A[j]
            j = j - 1
          A[j + 1] = value

2. Argue/prove that it is correct.
  1. We can use formal correctness proofs, but this is often excessive to convince someone an algorithm is correct. Instead, we can simply give lots of invariants and prove that the algorithm terminates.
  2. For example, for the above, we would say that for the inner loop, "`A[j + 1 ... i]` contains items that are bigger than `value`, and is in sorted order" is an invariant.
  3. It is very important to prove that it terminates, especially recursive algorithms. We simply use the standard methods specified in CS245, but we can usually just write that each call or iteration gets some smaller input, and there is some small input for which the algorithm terminates.
3. Analyze how good the algorithm is, in terms of efficiency and sometimes lower bounds.
  1. For this we use time complexity/running-time analysis, memory usage analysis, and so on.
  2. For example, the above code is known as Insertion Sort, which we already know to have a worst case time complexity of $O(n^2)$ and a best case time complexity of $O(n)$.
  3. Recall that formally, an algorithm being in $O(f(n))$ means that there exists a $k > 0$ and $m \ge 0$ such that for all $n > m$, $k \cdot f(n) > T(n)$ where $T(n)$ represents the number of constant time steps or amount of time that the algorithm needs to give an answer for an input of size $n$ - the running time function. This is written as $T(n) \in O(f(n))$.
4. Repeat above steps until satisfactory algorithm is found.

Only after this point would we implement/experiment the algorithm.

# 8/1/15

A **timing function** is a function $S_p: \mathrm{Input} \to \mb{R}^+$ where $p$ is the program we are running. For example, we can compare quicksort and mergesort by comparing $S_{\text{mergesort}}(\tup{5, 6, 2, 3})$ and $S_{\text{quicksort}}(\tup{5, 6, 2, 3})$.

A timing function measures the amount of time a program takes for a given input.

# elvin pls

Since computers are constantly changing and improving, comparing timing functions is not very meaningful since they will not stay consistent over differnt machines and over time as computers change. We want a machine-agnostic way to measure the amount of time a given program takes.

For example, Intel's core instruction set is RISC, with tons of extensions such as MMX, SSE, and FPU and GPU stuff. These add instructions such as `fsqrt` and similar. We assume that every instruction takes a finite, constant amount of time (interesting note).

We can instead measure the number of elementary operations, but it is difficult to define what an elementary operation actually is, and determining the exact number of elementary operations is a lot of work. For example, we might measure the number of clock cycles needed to run a program, but this would not make sense across things like RISC vs CISC machines. For example, if an architecture doesn't have a constant time division operation, we would probably need to implement one in software that would not have constant time.

In other words, we write our algorithms in pseudo-code, then count the number of primitive operations.

We now want to plot our timing function with respect to the size of the input $\abs{\mathrm{Input}} \subseteq \mb{N}$. This allows us to figure out the behaviour of the function as the size of the input gets bigger. This is difficult since finding the timing function for arbitrary inputs is often difficult. Additionally, we don't necessarily care about the constants in the function output, just its behaviour on large inputs.

Usually we care about the worst case behaviour, to determine the worst possible behaviour in all cases. We may also be interested in the average case to see how things will perform in practice. Occasionally, we may also be interested in the best case - for example, in cryptography, we want to make sure the problem cannot be solved in less than some given amount of time.

The worst case behaviour is $T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}$. The best case behaviour is $T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}$. We can now plot these functions to get the time bounds on our program.

The big-O notation is used to make life easier for computer scientists trying to calculate timing functions and to make comparing functions much easier. Refer to the CS136 notes for background. The $O$ in $O(f(x))$ is actually a capital omicron, bu nowadays the O is more common.

Comparing Big O is analogous to ordering in $\mb{R}$. Just like how $x \le y$, we might write $f(x) \in O(g(x))$. What we care about is the trends of the function, not the actual numbers.

Big-O gives an upper bound behaviour on a function (analogous to $x \le y$). Big-$\Omega$ gives a lower bound (analogous to $x \ge y$). Big-$\Theta$ us the exact bounds - when the Big-$O$ is the same as the Big-$\Omega$ (analogous to $x = y$).

There is also Little-$o$, which is a non-inclusive upper bound (analogous to x < y), and Little-$\omega$, which is a non-inclusive lower bound (analogous to $x > y$).

There are also incomparable functions, like one that goes up and down arbitrarily. As a result, functions can only be partially ordered.

# 13/1/15

Big-O does not care about the values of the functions - only about how it grow as input values get large.

Prove that $2n^2 + 3n + 11 \in O(n^2)$:

> We want to prove that $2n^2 + 3n + 11 \le cn^2$ for all $n > n_0$.  
> Let $n_0 = 10$ and $n \ge n_0$. Clearly $2n^2 + 3n + 11 \le 2n^2 + 3n^2 + 11n^2$ when $n \ge 1$ and $2n^2 + 3n^2 + 11n^2 = 16n^2$.  
> Then 16 is a possible value for $c$ and $2n^2 + 3n + 11 \in O(n^2)$.  

$O(n^2 + \log n)$ is not correct since it is not fully simplified.

Common time complexities include $\Theta(1)$ (constant), $\Theta(\log n)$ (logarithmic), $\Theta(n)$ (linear), $\Theta(n \log n)$ (pseudo-linear), $\Theta(n^2)$ (quadratic), $\Theta(n^3)$ (cubic), $\Theta(n^k)$ (polynomial), $\Theta(2^n)$ (exponential). In the real world, everything above $O(n \log n)$ tends to be rather bad performance.

It is not always the case that an algorithm with better time complexity than another is always better in all circumstances. For example, despite insertion sort being $O(n^2)$ and merge sort being $O(n \log n)$, we often use insertion sort when sorting smaller lists since it is faster in practice when the input is small. Many practical sorting algorithms therefore drop down into insertion sort when the input is small. This is also why most people use insertion sort when sorting things in real life - it is very fast for real-world inputs, which are usually small.

Let $L = \lim_{n \to \infty} \rfac{f(n)}{g(n)}$. If $L = 0$, $f(n) \in o(g(n))$. If $0 < L < \infty$, $f(n) \in \Theta(g(n))$. Otherwise, $L = \infty$ and $f(n) \in \omega(g(n))$. This is a useful way to prove orders for functions that would otherwise be difficult to prove by first principles.

For example, if we want to prove that $n^2 \log n \in o(n^3)$, we can do $\lim_{n \to \infty} \frac{n^2 \log n}{n^3} = \lim_{n \to \infty} \frac{\log n}{n} \lH \lim_{n \to \infty} \frac{\frac 1 n}{1} = 0$, and use the fact that $L = 0$ to conclude that $n^2 \log n \in o(n^3)$.

We can aso use this to prove that $(\log n)^a \in o(n^b)$ for any $a, b > 0$.

Also, $f(n) \in \Omega(g(n)) \iff g(n) \in \Omega(f(n))$, $f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))$, $f(n) \in o(g(n)) \iff g(n) \in \omega(f(n))$, $f(n) \in \Theta(g(n)) \iff g(n) \in O(f(n)) \wedge g(n) \in \Omega(f(n))$, $f(n) \in o(g(n)) \implies f(n) \in O(g(n))$, $f(n) \in o(g(n)) \implies f(n) \notin \Omega(g(n))$, $f(n) \in \omega(g(n)) \implies f(n) \in \Omega(g(n))$, and $f(n) \in \omega(g(n)) \implies f(n) \notin O(g(n))$. We can use these to simplify order notation expressions.

Also, $O(f(n) + g(n)) = O(\max(f(n), g(n))) = \max(O(f(n)), O(g(n)))$. Also, orders are transitive - $f(n \in O(g(n))) \wedge g(n) \in O(h(n)) \implies f(n) \in O(h(n))$.

In our pseudocoe, we often use $x \left 5$ or `x := 5` to denote assignment, since `=` is ambiguous - it could potentially mean equality or assignment.

Math with orders works a lot like normal math: $\Theta(1) \cdot (1 + \ldots + n) = \Theta(1) \cdot \frac{n^2 + n}{2} = \Theta(n^2 + n) = \Theta(n^2)$.

$\sum_{i = 1}^n i^k = \Theta(n^{k + 1})$

