CS240
=====

Data Structures and Data Management.

    Therese Beidl
    Section 002
    Email: beidl@uwaterloo.ca
    Web: http://www.student.cs.uwaterloo.ca/~cs240
    ISA: Joseph (Danny) Sitter
    ISA Email: cs240@studennt.cs.uwaterloo.ca

$$
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
$$

Seciton 1 and 2 are the regular classes, and section 3 is the enhanced section and has very different content.

Midterm at Thurs. Feb. 26 at 4:30 PM, worth 25%. Final exam worth 50%. Must pass weighted average of exam to pass the course. 5 assignments, each worth 5%.

# 6/1/15

Assignment 0 is due on Tuesdays. Assignment 0 is due on Jan. 13. ;wip: do it

Suppose we have a lot of data to keep track of. We could store it in an array/list, but depending on the type of the data, this might not be the best choice. Data structures should make it easy and efficient to perform the operations we need. For example, an English dictionary probably needs to be efficiently searched, but we don't really need to care about insertion and deletion since they're so rare.

The best data structure for something depends on the type of data we want to store. Our goal is to have a short running time and little memory.

In this course we will be performing theoretical analysis, developing ideas and pseudocode (and sometimes, implementations), and analyzing them using tools like big-O notation.

An Abstract Data Type is the idea of describing something by what it can do, not how it does it.

Required background includes arrays, linked lists, strings, stacks, queues, ADTs, recursive algorithms, trees, sorting algorithms (insertion, selection, quick, merge), binary search/BSTs, arithmetic series, geometric series, harmonic series ($\frac 1 1 + \ldots + \frac 1 n = \ln n$).

In this course, $\log n$ is implicitly $\log_2 n$ - all logarithms are base 2 unless otherwise stated.

A **problem** is a description of a general situation and the desired outcome. For example, the sorting problem is "given comparable values, put them in sorted order".

A **problem instance** is a particular input to a problem, like a particular array of numbers to sort for the sorting problem.

A **problem solution** is a change/process that, given the situation of a problem instance, results in the desired outcome. For example, the sorted array of numbers for the sorting problem.

We **solve** a problem by giving the correct algorithm for it. A solution is **correct** if it finds a solution for every possible input that can be given.

An **algorithm** is a finite description of a process that gives an answer (a solution that is not necessarily correct) for all possible instances of a problem.

**Efficiency** usually refers to an algorithm's runtime, and sometimes its memory usage. It may also refer to things specific to the problem domain, like the number of comparisons done.

To solve a problem:

1. Design an algorithm.
  1. Write down the main idea of the algorithm in plain prose. For example, "for increasing $i$, make $A[0 \ldots i]$ sorted by inserting $A[i]$ into a sorted $A[0 \ldots i - 1]$" for the sorting problem.
  2. Optionally, write pseudocode - code that might not be a real language, and is something like a mix between prose and code. This is a more precise way of specifying an algorithm. Any consistent, precise, and clearly understandable language will be accepted as pseudocode.

        Preconditions: an array `A[0 ... n - 1]`$.
        Postconditions: `A` is sorted
        for i in 1 ... n - 1:
          value = A[i]
          j = i - 1
          while j >= 0 and A[j] > key:
            A[j + 1] = A[j]
            j = j - 1
          A[j + 1] = value

2. Argue/prove that it is correct.
  1. We can use formal correctness proofs, but this is often excessive to convince someone an algorithm is correct. Instead, we can simply give lots of invariants and prove that the algorithm terminates.
  2. For example, for the above, we would say that for the inner loop, "`A[j + 1 ... i]` contains items that are bigger than `value`, and is in sorted order" is an invariant.
  3. It is very important to prove that it terminates, especially recursive algorithms. We simply use the standard methods specified in CS245, but we can usually just write that each call or iteration gets some smaller input, and there is some small input for which the algorithm terminates.
3. Analyze how good the algorithm is, in terms of efficiency and sometimes lower bounds.
  1. For this we use time complexity/running-time analysis, memory usage analysis, and so on.
  2. For example, the above code is known as Insertion Sort, which we already know to have a worst case time complexity of $O(n^2)$ and a best case time complexity of $O(n)$.
  3. Recall that formally, an algorithm being in $O(f(n))$ means that there exists a $k > 0$ and $m \ge 0$ such that for all $n > m$, $k \cdot f(n) > T(n)$ where $T(n)$ represents the number of constant time steps or amount of time that the algorithm needs to give an answer for an input of size $n$ - the running time function. This is written as $T(n) \in O(f(n))$.
4. Repeat above steps until satisfactory algorithm is found.

Only after this point would we implement/experiment the algorithm.

# 8/1/15

A **timing function** is a function $S_p: \mathrm{Input} \to \mb{R}^+$ where $p$ is the program we are running. For example, we can compare quicksort and mergesort by comparing $S_{\text{mergesort}}(\tup{5, 6, 2, 3})$ and $S_{\text{quicksort}}(\tup{5, 6, 2, 3})$.

A timing function measures the amount of time a program takes for a given input.

Since computers are constantly changing and improving, comparing timing functions is not very meaningful since they will not stay consistent over differnt machines and over time as computers change. We want a machine-agnostic way to measure the amount of time a given program takes.

For example, Intel's core instruction set is RISC, with tons of extensions such as MMX, SSE, and FPU and GPU stuff. These add instructions such as `fsqrt` and similar. We assume that every instruction takes a finite, constant amount of time (interesting note).

We can instead measure the number of elementary operations, but it is difficult to define what an elementary operation actually is, and determining the exact number of elementary operations is a lot of work. For example, we might measure the number of clock cycles needed to run a program, but this would not make sense across things like RISC vs CISC machines. For example, if an architecture doesn't have a constant time division operation, we would probably need to implement one in software that would not have constant time.

In other words, we write our algorithms in pseudo-code, then count the number of primitive operations.

We now want to plot our timing function with respect to the size of the input $\abs{\mathrm{Input}} \subseteq \mb{N}$. This allows us to figure out the behaviour of the function as the size of the input gets bigger. This is difficult since finding the timing function for arbitrary inputs is often difficult. Additionally, we don't necessarily care about the constants in the function output, just its behaviour on large inputs.

Usually we care about the worst case behaviour, to determine the worst possible behaviour in all cases. We may also be interested in the average case to see how things will perform in practice. Occasionally, we may also be interested in the best case - for example, in cryptography, we want to make sure the problem cannot be solved in less than some given amount of time.

The worst case behaviour is $T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}$. The best case behaviour is $T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}$. The average case behaviour is $T_p(n) = \frac{\sum_{e \in  R} i}{\abs{R}}$ where $R = \set{S_p(I) \middle| \abs{I} = n}$. We can now plot these functions to get the time bounds on our program.

The big-O notation is used to make life easier for computer scientists trying to calculate timing functions and to make comparing functions much easier. Refer to the CS136 notes for background. The $O$ in $O(f(x))$ is actually a capital omicron, bu nowadays the O is more common.

Comparing Big O is analogous to ordering in $\mb{R}$. Just like how $x \le y$, we might write $f(x) \in O(g(x))$. What we care about is the trends of the function, not the actual numbers.

Big-O gives an upper bound behaviour on a function (analogous to $x \le y$). Big-$\Omega$ gives a lower bound (analogous to $x \ge y$). Big-$\Theta$ us the exact bounds - when the Big-$O$ is the same as the Big-$\Omega$ (analogous to $x = y$).

There is also Little-$o$, which is a non-inclusive upper bound (analogous to x < y), and Little-$\omega$, which is a non-inclusive lower bound (analogous to $x > y$). For little-o, instead of $\exists c > 0, \exists n_0 > 0, n > n_0 \implies f(n) \le cg(x)$, we have $\forall c > 0, \exists n_0 > 0, n > n_0 \implies f(n) \le cg(x)$.

There are also incomparable functions, like one that goes up and down arbitrarily. As a result, functions can only be partially ordered.

# 13/1/15

Big-O does not care about the values of the functions - only about how it grow as input values get large.

Prove that $2n^2 + 3n + 11 \in O(n^2)$:

> We want to prove that $2n^2 + 3n + 11 \le cn^2$ for all $n > n_0$.  
> Let $n_0 = 10$ and $n \ge n_0$. Clearly $2n^2 + 3n + 11 \le 2n^2 + 3n^2 + 11n^2$ when $n \ge 1$ and $2n^2 + 3n^2 + 11n^2 = 16n^2$.  
> Then 16 is a possible value for $c$ and $2n^2 + 3n + 11 \in O(n^2)$.  

$O(n^2 + \log n)$ is not correct since it is not fully simplified.

Common time complexities include $\Theta(1)$ (constant), $\Theta(\log n)$ (logarithmic), $\Theta(n)$ (linear), $\Theta(n \log n)$ (pseudo-linear), $\Theta(n^2)$ (quadratic), $\Theta(n^3)$ (cubic), $\Theta(n^k)$ (polynomial), $\Theta(2^n)$ (exponential). In the real world, everything above $O(n \log n)$ tends to be rather bad performance.

It is not always the case that an algorithm with better time complexity than another is always better in all circumstances. For example, despite insertion sort being $O(n^2)$ and merge sort being $O(n \log n)$, we often use insertion sort when sorting smaller lists since it is faster in practice when the input is small. Many practical sorting algorithms therefore drop down into insertion sort when the input is small. This is also why most people use insertion sort when sorting things in real life - it is very fast for real-world inputs, which are usually small.

Let $L = \lim_{n \to \infty} \frac{f(n)}{g(n)}$. If $L = 0$, $f(n) \in o(g(n))$. If $0 < L < \infty$, $f(n) \in \Theta(g(n))$. Otherwise, $L = \infty$ and $f(n) \in \omega(g(n))$. This is a useful way to prove orders for functions that would otherwise be difficult to prove by first principles.

For example, if we want to prove that $n^2 \log n \in o(n^3)$, we can do $\lim_{n \to \infty} \frac{n^2 \log n}{n^3} = \lim_{n \to \infty} \frac{\log n}{n} \lH \lim_{n \to \infty} \frac{\frac 1 n}{1} = 0$, and use the fact that $L = 0$ to conclude that $n^2 \log n \in o(n^3)$.

We can aso use this to prove that $(\log n)^a \in o(n^b)$ for any $a, b > 0$.

Also, $f(n) \in \Omega(g(n)) \iff g(n) \in \Omega(f(n))$, $f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))$, $f(n) \in o(g(n)) \iff g(n) \in \omega(f(n))$, $f(n) \in \Theta(g(n)) \iff g(n) \in O(f(n)) \wedge g(n) \in \Omega(f(n))$, $f(n) \in o(g(n)) \implies f(n) \in O(g(n))$, $f(n) \in o(g(n)) \implies f(n) \notin \Omega(g(n))$, $f(n) \in \omega(g(n)) \implies f(n) \in \Omega(g(n))$, and $f(n) \in \omega(g(n)) \implies f(n) \notin O(g(n))$. We can use these to simplify order notation expressions.

Also, $O(f(n) + g(n)) = O(\max(f(n), g(n))) = \max(O(f(n)), O(g(n)))$. Also, orders are transitive - $f(n \in O(g(n))) \wedge g(n) \in O(h(n)) \implies f(n) \in O(h(n))$.

In our pseudocoe, we often use $x \left 5$ or `x := 5` to denote assignment, since `=` is ambiguous - it could potentially mean equality or assignment.

Math with orders works a lot like normal math: $\Theta(1) \cdot (1 + \ldots + n) = \Theta(1) \cdot \frac{n^2 + n}{2} = \Theta(n^2 + n) = \Theta(n^2)$.

$\sum_{i = 1}^n i^k = \Theta(n^{k + 1})$

# 15/1/15

A priority queue is a data structure that stores items with keys that represent the priorities that the values have. It has two operations - insertion with a given key and deletion of the item with the maximum priority.

There are a number of ways we could implement a priority queue - an unsorted dynamic array (amortized $O(1)$ insert, $O(n)$ deletion), a sorted dynamic array, and something known as a heap. We don't want to use sorted order, since that makes insertion slow. We also dont't want to use unsorted order, since that makes deletion slow.

A **heap** is a data structure that allows us to implement a priority queue in a very efficient way. The main idea is that we would like to know very easily what the maximum element is, so we make all the other elements follow from it.

The basic idea behind the heap is that it is a tree where each element keeps track of candidates that can replace them when they're deleted, as children. The maximum item would therefore be the root. When we insert, we simply insert 

Most commonly, we use binary heaps, which is a binary tree. Recall that this is a data structure consisting of nodes and links between node, such that each node has 2 or fewer children, and 1 or fewer parents.

Formally, a **binary max-heap** is a binary tree that satisfies the structural property and the max-heap property. There are no rules about the order of children, unlike a binary search tree.

The **structural property** is that all levels except the lowest level must be completely filled. Each node must have two children, except the lowest level, where nodes can have fewer than two children but **must fill up from the left** - in the lowest level, the left sibling of a node must have 2 children before we can insert a child into it.

The **max-heap property** is that for any node $x$ in the heap, the key of $x$ is less than or equal to the key of the parent of $x$. There is also a min-heap property where the key of $x$ is greater than or equal to the key of the parent of $x$, used for min-heaps. In other words, the parent of any node in a heap must be bigger or equal to that node. As a result, the maximum node of a heap is always the root node.

Insertion into a heap is easy because we can simply do a standard tree insertion, with certain constraints to ensure it satisfies the structural property and the heap property. This operation is $\Theta(\log n)$ because the structural property ensures that the height of the tree is $\Theta(\log n)$.

Also, the height of a binary heap is $\log_2 n - \omega(\log n)$. The height of a tree is 1 less than the number of levels it has (1 less than the maximum depth). Levels start at 1 and increase with depth.

Proof:

> Let $x$ be a heap with $n$ nodes and height $h$.  
> Clearly, any binary tree of height $h$ must have $n \le 2^{h + 1} - 1$, since each level $i$ has less than or equal to $2^{i - 1}$ nodes, and the total is $n \le \sum_{i = 1}^{h + 1} 2^{i - 1} \le \frac{2^{h + 1} - 1}{2 - 1}$.  
> Since $n + 1 \le 2^{h + 1}$, $h \ge \log(n + 1) - 1$.  
> Clearly, $n \ge 2^h$, by the structural property, since the lowest level is always $h + 1$ so each level $i$ before it has exactly $2^{i - 1}$ nodes, and the total for all levels except the lowest is $2^h - 1$. Since the lowest level has 1 or more nodes, the total is $n \ge (2^h - 1) + 1$, or $2^h$.  

# 20/1/15

Binary heaps are trees, so we can store them as a tree structure with references and nodes. However, a more efficient way to store binary trees is as an array:

* $A$ is an array representing the heap.
* $A[0]$ contains the root node.
* $A[1], \ldots, A[2]$ store the children of the root node.
* $A[3], \ldots, A[6]$ store the children of the children of the root nodes, and so on.

Essentially, we first have the first level, then the second level, and so on. Formally, the left and right children of a node stored in the array at index $i$ are $lc(i) = 2i + 1$ and $rc(i) = 2i + 2$ (in a 0-indexed array). Likewise, the parent of any node is $p(i) = \floor{\frac{i - 1} 2}$. This works because the leftmost node in level $l$ has index $\sum_{k = 1}^{l - 1} 2^{k - 1}$.

In fact, this left and right parent system works for any binary tree, and even for $n$-ary trees with a simple extension. However, it is not space efficient unless the tree obeys something like the structural property. The structural property ensures that the array is mostly filled, possibly except for part that stores the last level.

### Heaps as Priority Queues

We will represent our heap as an array $A$ containing $n$ values, automatically expanded or shrunk as needed.

To **insert** into a heap, we first find the only location that the structural property allows us to insert a new node $i$ into. This could possibly violate the heap property since it is possible that $i > p(i)$, so if $i > p(i)$, we swap $i$ and $p(i)$ to get $p(i)'$ and $i'$, where $p(p(i)') = i'$.

Now the heap property is satisfied for $i'$ and $p(i)'$, and it isn't possible for $p(i)'$'s sibling to be greater than $i'$ since it was less than the original parent $p(i)$. However, it is possible that $i'$ is now greater than it's new parent $p(i')$, so we swap again if $i' > p(i)'$.

We repeat swapping the originally inserted node with its parents until either it is less than its parent, or it is the new root node. This is called "bubbling up until it holds".

In pseudocode:

    def insert(key):
        # wip: we resize the array here if necessary
        
        A[n] = key # $n$ is the index of the next free space in the heap - we keep track of this in the heap structure
        n ++
        j = N - 1
        while j > 0 and A[p(j)] < A[j]:
            swap A[j] and A[p(j)]
            j = p(j)

Clearly, this takes $O(\log n)$ time worst case since every run of the loop runs on a lower height on the tree. The bubbling up process would potentially need to visit every level in the tree if inserting an element greater than the root element of the heap.

To **delete the maximum** of a heap, our goal is to remove the root element of the heap. When we do this, we now have a blank space at the root node of the heap. We fix this by moving the last node in the heap (the rightmost node on the lowest level) into the position the root originally occupied. This allows us to satisfy the structural property again.

However, now the heap property is not satisfied. We fix this now by swapping the root node (originally the last node) $i$ with the bigger of its children $lc(i)$ and $rc(i)$, which we will represent as $c(i)$, to get $c(i)'$ and $i'$ such that $p(i') = c(i)'$. Now we have satisfied the heap property between $i'$ and $c(i)'$ and between $i'$'s sibling and $c(i)'$, but it is possible that $i'$'s children could be greater than it.

Therefore, we repeatedly swap the originally last node with its larger child until it is greater (or equal to) than both of its children, or it is a leaf node.

In pseudocode:

    def delete_max():
        max = A[0]
        n -- # 1 less than the next free space is the last node in the heap, which we will consider as a free space after we move it to the root
        # if memory management is needed, free `A[0]` here
        A[0] = A[n]
        
        # the following is the bubble_down operation, which bubbles values down until they satisfy the heap property
        j = 0
        while j <= n - 1:
            max_index = j
            if lc(j) exists and A[lc(j)] > A[j]: # existance means that lc(j) <= n - 1
                max_index = lc(j)
            if rc(j) exists and A[rc(j)] > A[max_index]: # existance means that rc(j) <= n - 1
                max_index = rc(j)
            if max_index == k: break
            swap A[j] and A[max_index]
            j = max_index
        
        # wip: we shrink the array here if necessary
        return max

Clearly, this takes $O(h)$ time worst case where $h$ is the height of the heap underneath the starting element, the root, which is $O(\log n)$ since $h$ is roughly $\log n$. The worst case is when the last node of the heap is also the smallest one.

**Priority queue sort** (PQSort) is a sorting algorithm where we insert all our elements into a priority queue and then take them out again in order. PQSort needs $n$ insertions and $n$ deletions of maximums. Since each operation is $O(\log n)$, PQSort is $O(n \log n)$, like mergesort. When the priority queue is implemented with a heap, we also call it **heapsort**.

Heapsort is pretty nice because we can sort in-place (by building a heap directly) and without a lot of recursion, but it can have poor real-world behaviour because the worst case heap insertion occurs when we are inserting nodes one by one that are already in ascending order, and already-sorted data occurs very often in real-world applications.

Also, heapsort is not stable. If a sorting algorithm is **stable**, then if different nodes have equal keys, they appear in the same order in the output as they did in the input.

Also, we often measure the runtime of sorting algorithms in terms of the number of key comparisons made ($A[i] < A[j]$). This is because we often allow users to specify their own key computing functions, which could potentially be aribtrarily exxpensive. Everything else is simply proportional or a constant on top of the number of key comparisons, in terms of runtime.

We may also want to **build heaps** - given an array $D$ with $c$ values, we want to build a head from that. This is relatively trivial in theory - simply go through the array and insert it into a heap. Clearly, this is $O(n \log n)$ since insetion is $O(\log n)$ and we need $n$ of them.

Clearly, this takes roughly $\sum_{k = 0}^{c - 1} \log(k + 1)$ time since $n$ starts off at 0 and grows by 1 upon every insertion. Clearly, there are at least $\frac n 2$ terms in that sum where $k + 1 \ge \frac n 2$, so $\sum_{k = 0}^{c - 1} \log(k + 1) \ge \frac n 2 \log \frac n 2$, so the operation is $\Omega(n \log n)$, and by the above, $\Theta(n \log n)$.

A better option is to build the heap in place, to save memory. Clearly, an array always satisfies the structural property since there's no way not to, so all we have to do is enforce the heap property in the array.

We will enforce the heap property by bubbling downward on every level of the node starting from the second-to-bottom one. Our invariant will be that for all $k > i$, the subheap rooted at $k$ satisfies the heap property. We will show that this takes less than or equal to $2c$ key comparisons:

    for i in floor(n / 2) - 1 ... 0:
        bubble_down(i)

Proof:

> Without loss of generality, assume $n = 2^t - 1$ where $t$ is the number of levels - all levels are full.  
> Clearly, any node on a level $l$ needs at most $2(t - l)$ comparisons for its bubble down, since each level until the bottom needs 2 comparisons each and there $t - l$ levels between the current level and the bottom.  
> Clearly, there are $2^{l - 1}$ nodes on each level, which means that there are $\sum_{l = 1}^{t + 1} 2^{l - 1} 2(t - l)$ comparisons in total.  
> Clearly, $\sum_{l = 1}^p 2^{l - 1} 2(p - l) = t\sum_{l = 1}^p 2^l - \sum_{l = 1}^p l2^l) \le 2^t - 2t - 2 \le 2^t - 1 = 2n$ (note that $\sum_{x = 1}^n x2^x = 2^{n + 1} n - 2^{n + 1} + 2$).  
> So there are $2n$ or fewer comparisons in total.  

In other words, we can build a heap in-place from an array in $O(n)$ time.