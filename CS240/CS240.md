CS240
=====

Data Structures and Data Management.

    Therese Beidl
    Section 002
    Email: beidl@uwaterloo.ca
    Web: http://www.student.cs.uwaterloo.ca/~cs240
    ISA: Joseph (Danny) Sitter
    ISA Email: cs240@studennt.cs.uwaterloo.ca

$$
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
$$

Seciton 1 and 2 are the regular classes, and section 3 is the enhanced section and has very different content.

Midterm at Thurs. Feb. 26 at 4:30 PM, worth 25%. Final exam worth 50%. Must pass weighted average of exam to pass the course. 5 assignments, each worth 5%.

# 6/1/15

Assignment 0 is due on Tuesdays. Assignment 0 is due on Jan. 13. ;wip: do it

Suppose we have a lot of data to keep track of. We could store it in an array/list, but depending on the type of the data, this might not be the best choice. Data structures should make it easy and efficient to perform the operations we need. For example, an English dictionary probably needs to be efficiently searched, but we don't really need to care about insertion and deletion since they're so rare.

The best data structure for something depends on the type of data we want to store. Our goal is to have a short running time and little memory.

In this course we will be performing theoretical analysis, developing ideas and pseudocode (and sometimes, implementations), and analyzing them using tools like big-O notation.

An Abstract Data Type is the idea of describing something by what it can do, not how it does it.

Required background includes arrays, linked lists, strings, stacks, queues, ADTs, recursive algorithms, trees, sorting algorithms (insertion, selection, quick, merge), binary search/BSTs, arithmetic series, geometric series, harmonic series ($\frac 1 1 + \ldots + \frac 1 n = \ln n$).

In this course, $\log n$ is implicitly $\log_2 n$ - all logarithms are base 2 unless otherwise stated.

A **problem** is a description of a general situation and the desired outcome. For example, the sorting problem is "given comparable values, put them in sorted order".

A **problem instance** is a particular input to a problem, like a particular array of numbers to sort for the sorting problem.

A **problem solution** is a change/process that, given the situation of a problem instance, results in the desired outcome. For example, the sorted array of numbers for the sorting problem.

We **solve** a problem by giving the correct algorithm for it. A solution is **correct** if it finds a solution for every possible input that can be given.

An **algorithm** is a finite description of a process that gives an answer (a solution that is not necessarily correct) for all possible instances of a problem.

**Efficiency** usually refers to an algorithm's runtime, and sometimes its memory usage. It may also refer to things specific to the problem domain, like the number of comparisons done.

To solve a problem:

1. Design an algorithm.
  1. Write down the main idea of the algorithm in plain prose. For example, "for increasing $i$, make $A[0 \ldots i]$ sorted by inserting $A[i]$ into a sorted $A[0 \ldots i - 1]$" for the sorting problem.
  2. Optionally, write pseudocode - code that might not be a real language, and is something like a mix between prose and code. This is a more precise way of specifying an algorithm. Any consistent, precise, and clearly understandable language will be accepted as pseudocode.

        Preconditions: an array `A[0 ... n - 1]`$.
        Postconditions: `A` is sorted
        for i in 1 ... n - 1:
          value = A[i]
          j = i - 1
          while j >= 0 and A[j] > key:
            A[j + 1] = A[j]
            j = j - 1
          A[j + 1] = value

2. Argue/prove that it is correct.
  1. We can use formal correctness proofs, but this is often excessive to convince someone an algorithm is correct. Instead, we can simply give lots of invariants and prove that the algorithm terminates.
  2. For example, for the above, we would say that for the inner loop, "`A[j + 1 ... i]` contains items that are bigger than `value`, and is in sorted order" is an invariant.
  3. It is very important to prove that it terminates, especially recursive algorithms. We simply use the standard methods specified in CS245, but we can usually just write that each call or iteration gets some smaller input, and there is some small input for which the algorithm terminates.
3. Analyze how good the algorithm is, in terms of efficiency and sometimes lower bounds.
  1. For this we use time complexity/running-time analysis, memory usage analysis, and so on.
  2. For example, the above code is known as Insertion Sort, which we already know to have a worst case time complexity of $O(n^2)$ and a best case time complexity of $O(n)$.
  3. Recall that formally, an algorithm being in $O(f(n))$ means that there exists a $k > 0$ and $m \ge 0$ such that for all $n > m$, $k \cdot f(n) > T(n)$ where $T(n)$ represents the number of constant time steps or amount of time that the algorithm needs to give an answer for an input of size $n$ - the running time function. This is written as $T(n) \in O(f(n))$.
4. Repeat above steps until satisfactory algorithm is found.

Only after this point would we implement/experiment the algorithm.

# 8/1/15

A **timing function** is a function $S_p: \mathrm{Input} \to \mb{R}^+$ where $p$ is the program we are running. For example, we can compare quicksort and mergesort by comparing $S_{\text{mergesort}}(\tup{5, 6, 2, 3})$ and $S_{\text{quicksort}}(\tup{5, 6, 2, 3})$.

A timing function measures the amount of time a program takes for a given input.

Since computers are constantly changing and improving, comparing timing functions is not very meaningful since they will not stay consistent over differnt machines and over time as computers change. We want a machine-agnostic way to measure the amount of time a given program takes.

For example, Intel's core instruction set is RISC, with tons of extensions such as MMX, SSE, and FPU and GPU stuff. These add instructions such as `fsqrt` and similar. We assume that every instruction takes a finite, constant amount of time (interesting note).

We can instead measure the number of elementary operations, but it is difficult to define what an elementary operation actually is, and determining the exact number of elementary operations is a lot of work. For example, we might measure the number of clock cycles needed to run a program, but this would not make sense across things like RISC vs CISC machines. For example, if an architecture doesn't have a constant time division operation, we would probably need to implement one in software that would not have constant time.

In other words, we write our algorithms in pseudo-code, then count the number of primitive operations.

We now want to plot our timing function with respect to the size of the input $\abs{\mathrm{Input}} \subseteq \mb{N}$. This allows us to figure out the behaviour of the function as the size of the input gets bigger. This is difficult since finding the timing function for arbitrary inputs is often difficult. Additionally, we don't necessarily care about the constants in the function output, just its behaviour on large inputs.

Usually we care about the worst case behaviour, to determine the worst possible behaviour in all cases. We may also be interested in the average case to see how things will perform in practice. Occasionally, we may also be interested in the best case - for example, in cryptography, we want to make sure the problem cannot be solved in less than some given amount of time.

The worst case behaviour is $T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}$. The best case behaviour is $T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}$. The average case behaviour is $T_p(n) = \frac{\sum_{e \in  R} i}{\abs{R}}$ where $R = \set{S_p(I) \middle| \abs{I} = n}$. We can now plot these functions to get the time bounds on our program.

The big-O notation is used to make life easier for computer scientists trying to calculate timing functions and to make comparing functions much easier. Refer to the CS136 notes for background. The $O$ in $O(f(x))$ is actually a capital omicron, bu nowadays the O is more common.

Comparing Big O is analogous to ordering in $\mb{R}$. Just like how $x \le y$, we might write $f(x) \in O(g(x))$. What we care about is the trends of the function, not the actual numbers.

Big-O gives an upper bound behaviour on a function (analogous to $x \le y$). Big-$\Omega$ gives a lower bound (analogous to $x \ge y$). Big-$\Theta$ us the exact bounds - when the Big-$O$ is the same as the Big-$\Omega$ (analogous to $x = y$).

There is also Little-$o$, which is a non-inclusive upper bound (analogous to x < y), and Little-$\omega$, which is a non-inclusive lower bound (analogous to $x > y$). For little-o, instead of $\exists c > 0, \exists n_0 > 0, n > n_0 \implies f(n) \le cg(x)$, we have $\forall c > 0, \exists n_0 > 0, n > n_0 \implies f(n) \le cg(x)$.

There are also incomparable functions, like one that goes up and down arbitrarily. As a result, functions can only be partially ordered.

# 13/1/15

Big-O does not care about the values of the functions - only about how it grow as input values get large.

Prove that $2n^2 + 3n + 11 \in O(n^2)$:

> We want to prove that $2n^2 + 3n + 11 \le cn^2$ for all $n > n_0$.  
> Let $n_0 = 10$ and $n \ge n_0$. Clearly $2n^2 + 3n + 11 \le 2n^2 + 3n^2 + 11n^2$ when $n \ge 1$ and $2n^2 + 3n^2 + 11n^2 = 16n^2$.  
> Then 16 is a possible value for $c$ and $2n^2 + 3n + 11 \in O(n^2)$.  

$O(n^2 + \log n)$ is not correct since it is not fully simplified.

Common time complexities include $\Theta(1)$ (constant), $\Theta(\log n)$ (logarithmic), $\Theta(n)$ (linear), $\Theta(n \log n)$ (pseudo-linear), $\Theta(n^2)$ (quadratic), $\Theta(n^3)$ (cubic), $\Theta(n^k)$ (polynomial), $\Theta(2^n)$ (exponential). In the real world, everything above $O(n \log n)$ tends to be rather bad performance.

It is not always the case that an algorithm with better time complexity than another is always better in all circumstances. For example, despite insertion sort being $O(n^2)$ and merge sort being $O(n \log n)$, we often use insertion sort when sorting smaller lists since it is faster in practice when the input is small. Many practical sorting algorithms therefore drop down into insertion sort when the input is small. This is also why most people use insertion sort when sorting things in real life - it is very fast for real-world inputs, which are usually small.

Let $L = \lim_{n \to \infty} \frac{f(n)}{g(n)}$. If $L = 0$, $f(n) \in o(g(n))$. If $0 < L < \infty$, $f(n) \in \Theta(g(n))$. Otherwise, $L = \infty$ and $f(n) \in \omega(g(n))$. This is a useful way to prove orders for functions that would otherwise be difficult to prove by first principles.

For example, if we want to prove that $n^2 \log n \in o(n^3)$, we can do $\lim_{n \to \infty} \frac{n^2 \log n}{n^3} = \lim_{n \to \infty} \frac{\log n}{n} \lH \lim_{n \to \infty} \frac{\frac 1 n}{1} = 0$, and use the fact that $L = 0$ to conclude that $n^2 \log n \in o(n^3)$.

We can aso use this to prove that $(\log n)^a \in o(n^b)$ for any $a, b > 0$.

Also, $f(n) \in \Omega(g(n)) \iff g(n) \in \Omega(f(n))$, $f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))$, $f(n) \in o(g(n)) \iff g(n) \in \omega(f(n))$, $f(n) \in \Theta(g(n)) \iff g(n) \in O(f(n)) \wedge g(n) \in \Omega(f(n))$, $f(n) \in o(g(n)) \implies f(n) \in O(g(n))$, $f(n) \in o(g(n)) \implies f(n) \notin \Omega(g(n))$, $f(n) \in \omega(g(n)) \implies f(n) \in \Omega(g(n))$, and $f(n) \in \omega(g(n)) \implies f(n) \notin O(g(n))$. We can use these to simplify order notation expressions.

Also, $O(f(n) + g(n)) = O(\max(f(n), g(n))) = \max(O(f(n)), O(g(n)))$. Also, orders are transitive - $f(n \in O(g(n))) \wedge g(n) \in O(h(n)) \implies f(n) \in O(h(n))$.

In our pseudocode, we often use $x \left 5$ or `x := 5` to denote assignment, since `=` is ambiguous - it could potentially mean equality or assignment.

Math with orders works a lot like normal math: $\Theta(1) \cdot (1 + \ldots + n) = \Theta(1) \cdot \frac{n^2 + n}{2} = \Theta(n^2 + n) = \Theta(n^2)$.

$\sum_{i = 1}^n i^k = \Theta(n^{k + 1})$

# 15/1/15

A priority queue is a data structure that stores items with keys that represent the priorities that the values have. It has two operations - insertion with a given key and deletion of the item with the maximum priority.

There are a number of ways we could implement a priority queue - an unsorted dynamic array (amortized $O(1)$ insert, $O(n)$ deletion), a sorted dynamic array, and something known as a heap. We don't want to use sorted order, since that makes insertion slow. We also dont't want to use unsorted order, since that makes deletion slow.

A **heap** is a data structure that allows us to implement a priority queue in a very efficient way. The main idea is that we would like to know very easily what the maximum element is, so we make all the other elements follow from it.

The basic idea behind the heap is that it is a tree where each element keeps track of candidates that can replace them when they're deleted, as children. The maximum item would therefore be the root. When we insert, we simply insert 

Most commonly, we use binary heaps, which is a binary tree. Recall that this is a data structure consisting of nodes and links between node, such that each node has 2 or fewer children, and 1 or fewer parents.

Formally, a **binary max-heap** is a binary tree that satisfies the structural property and the max-heap property. There are no rules about the order of children, unlike a binary search tree.

The **structural property** is that all levels except the lowest level must be completely filled. Each node must have two children, except the lowest level, where nodes can have fewer than two children but **must fill up from the left** - in the lowest level, the left sibling of a node must have 2 children before we can insert a child into it.

The **max-heap property** is that for any node $x$ in the heap, the key of $x$ is less than or equal to the key of the parent of $x$. There is also a min-heap property where the key of $x$ is greater than or equal to the key of the parent of $x$, used for min-heaps. In other words, the parent of any node in a heap must be bigger or equal to that node. As a result, the maximum node of a heap is always the root node.

Insertion into a heap is easy because we can simply do a standard tree insertion, with certain constraints to ensure it satisfies the structural property and the heap property. This operation is $\Theta(\log n)$ because the structural property ensures that the height of the tree is $\Theta(\log n)$.

Also, the height of a binary heap is $\log_2 n - \omega(\log n)$. The height of a tree is 1 less than the number of levels it has (1 less than the maximum depth). Levels start at 1 and increase with depth.

Proof:

> Let $x$ be a heap with $n$ nodes and height $h$.  
> Clearly, any binary tree of height $h$ must have $n \le 2^{h + 1} - 1$, since each level $i$ has less than or equal to $2^{i - 1}$ nodes, and the total is $n \le \sum_{i = 1}^{h + 1} 2^{i - 1} \le \frac{2^{h + 1} - 1}{2 - 1}$.  
> Since $n + 1 \le 2^{h + 1}$, $h \ge \log(n + 1) - 1$.  
> Clearly, $n \ge 2^h$, by the structural property, since the lowest level is always $h + 1$ so each level $i$ before it has exactly $2^{i - 1}$ nodes, and the total for all levels except the lowest is $2^h - 1$. Since the lowest level has 1 or more nodes, the total is $n \ge (2^h - 1) + 1$, or $2^h$.  

# 20/1/15

Binary heaps are trees, so we can store them as a tree structure with references and nodes. However, a more efficient way to store binary trees is as an array:

* $A$ is an array representing the heap.
* $A[0]$ contains the root node.
* $A[1], \ldots, A[2]$ store the children of the root node.
* $A[3], \ldots, A[6]$ store the children of the children of the root nodes, and so on.

Essentially, we first have the first level, then the second level, and so on. Formally, the left and right children of a node stored in the array at index $i$ are $lc(i) = 2i + 1$ and $rc(i) = 2i + 2$ (in a 0-indexed array). Likewise, the parent of any node is $p(i) = \floor{\frac{i - 1} 2}$. This works because the leftmost node in level $l$ has index $\sum_{k = 1}^{l - 1} 2^{k - 1}$.

In fact, this left and right parent system works for any binary tree, and even for $n$-ary trees with a simple extension. However, it is not space efficient unless the tree obeys something like the structural property. The structural property ensures that the array is mostly filled, possibly except for part that stores the last level.

### Heaps as Priority Queues

We will represent our heap as an array $A$ containing $n$ values, automatically expanded or shrunk as needed.

To **insert** into a heap, we first find the only location that the structural property allows us to insert a new node $i$ into. This could possibly violate the heap property since it is possible that $i > p(i)$, so if $i > p(i)$, we swap $i$ and $p(i)$ to get $p(i)'$ and $i'$, where $p(p(i)') = i'$.

Now the heap property is satisfied for $i'$ and $p(i)'$, and it isn't possible for $p(i)'$'s sibling to be greater than $i'$ since it was less than the original parent $p(i)$. However, it is possible that $i'$ is now greater than it's new parent $p(i')$, so we swap again if $i' > p(i)'$.

We repeat swapping the originally inserted node with its parents until either it is less than its parent, or it is the new root node. This is called "bubbling up until it holds".

In pseudocode:

    def insert(key):
        # wip: we resize the array here if necessary
        
        A[n] = key # $n$ is the index of the next free space in the heap - we keep track of this in the heap structure
        n ++
        j = N - 1
        while j > 0 and A[p(j)] < A[j]:
            swap A[j] and A[p(j)]
            j = p(j)

Clearly, this takes $O(\log n)$ time worst case since every run of the loop runs on a lower height on the tree. The bubbling up process would potentially need to visit every level in the tree if inserting an element greater than the root element of the heap.

To **delete the maximum** of a heap, our goal is to remove the root element of the heap. When we do this, we now have a blank space at the root node of the heap. We fix this by moving the last node in the heap (the rightmost node on the lowest level) into the position the root originally occupied. This allows us to satisfy the structural property again.

However, now the heap property is not satisfied. We fix this now by swapping the root node (originally the last node) $i$ with the bigger of its children $lc(i)$ and $rc(i)$, which we will represent as $c(i)$, to get $c(i)'$ and $i'$ such that $p(i') = c(i)'$. Now we have satisfied the heap property between $i'$ and $c(i)'$ and between $i'$'s sibling and $c(i)'$, but it is possible that $i'$'s children could be greater than it.

Therefore, we repeatedly swap the originally last node with its larger child until it is greater (or equal to) than both of its children, or it is a leaf node.

In pseudocode:

    def delete_max():
        max = A[0]
        n -- # 1 less than the next free space is the last node in the heap, which we will consider as a free space after we move it to the root
        # if memory management is needed, free `A[0]` here
        A[0] = A[n]
        
        # the following is the bubble_down operation, which bubbles values down until they satisfy the heap property
        j = 0
        while j <= n - 1:
            max_index = j
            if lc(j) exists and A[lc(j)] > A[j]: # existance means that lc(j) <= n - 1
                max_index = lc(j)
            if rc(j) exists and A[rc(j)] > A[max_index]: # existance means that rc(j) <= n - 1
                max_index = rc(j)
            if max_index == k: break # the heap property is already satisfied, so stop bubbling
            swap A[j] and A[max_index] # swap the current node with its child, which is larger than it
            j = max_index
        
        # if necessary, shrink the array here
        return max

Clearly, this takes $O(h)$ time worst case where $h$ is the height of the heap underneath the starting element, the root, which is $O(\log n)$ since $h$ is roughly $\log n$. The worst case is when the last node of the heap is also the smallest one.

**Priority queue sort** (PQSort) is a sorting algorithm where we insert all our elements into a priority queue and then take them out again in order. PQSort needs $n$ insertions and $n$ deletions of maximums. Since each operation is $O(\log n)$, PQSort is $O(n \log n)$, like mergesort. When the priority queue is implemented with a heap, we also call it **heapsort**.

Heapsort is pretty nice because we can sort in-place (by building a heap directly) and without a lot of recursion, but it can have poor real-world behaviour because the worst case heap insertion occurs when we are inserting nodes one by one that are already in ascending order, and already-sorted data occurs very often in real-world applications.

Also, heapsort is not stable. If a sorting algorithm is **stable**, then if different nodes have equal keys, they appear in the same order in the output as they did in the input.

Also, we often measure the runtime of sorting algorithms in terms of the number of key comparisons made ($A[i] < A[j]$). This is because we often allow users to specify their own key computing functions, which could potentially be aribtrarily exxpensive. Everything else is simply proportional or a constant on top of the number of key comparisons, in terms of runtime.

We may also want to **build heaps** - given an array $D$ with $c$ values, we want to build a head from that. This is relatively trivial in theory - simply go through the array and insert it into a heap. Clearly, this is $O(n \log n)$ since insertion is $O(\log n)$ and we need $n$ of them.

Clearly, this takes roughly $\sum_{k = 0}^{c - 1} \log(k + 1)$ time since $n$ starts off at 0 and grows by 1 upon every insertion. Clearly, there are at least $\frac n 2$ terms in that sum where $k + 1 \ge \frac n 2$, so $\sum_{k = 0}^{c - 1} \log(k + 1) \ge \frac n 2 \log \frac n 2$, so the operation is $\Omega(n \log n)$, and by the above, $\Theta(n \log n)$.

A better option is to build the heap in place, to save memory. Clearly, an array always satisfies the structural property since there's no way not to, so all we have to do is enforce the heap property in the array.

We will enforce the heap property by bubbling downward on every level of the node starting from the last node that has children (the last node on the second-to-last level). Our invariant will be that for all $k > i$, the subheap rooted at $k$ satisfies the heap property. We will show that this takes less than or equal to $2c$ key comparisons:

    for i in floor(n / 2) - 1 ... 0:
        bubble_down(i)

We could bubble down for $n - 1$ down to 0, since that visits every node, but since the ones at the end are all leaf nodes and bubbling down is a no-op, we just need to bubble down for all nodes from $p(n - 1) = \floor{\frac{n - 2} 2} = \floor{\frac n 2} - 1$ down to 0.

Proof:

> Without loss of generality, assume $n = 2^t - 1$ where $t$ is the number of levels - all levels are full.  
> Clearly, any node on a level $l$ needs at most $2(t - l)$ comparisons for its bubble down, since each level until the bottom needs 2 comparisons each and there $t - l$ levels between the current level and the bottom.  
> Clearly, there are $2^{l - 1}$ nodes on each level, which means that there are $\sum_{l = 1}^{t + 1} 2^{l - 1} 2(t - l)$ comparisons in total.  
> Clearly, $\sum_{l = 1}^p 2^{l - 1} 2(p - l) = t\sum_{l = 1}^p 2^l - \sum_{l = 1}^p l2^l) \le 2^t - 2t - 2 \le 2^t - 1 = 2n$ (note that $\sum_{x = 1}^n x2^x = 2^{n + 1} n - 2^{n + 1} + 2$).  
> So there are $2n$ or fewer comparisons in total.  

In other words, we can build a heap in-place from an array in $O(n)$ time.

Also, if we prove that the running time of a function is of a certain order at infinity, then it is proven for all $n$.

# 22/1/15

Another proof that in-place heapify is linear in time would be to use induction to show that the number of comparisons done for each node that is $i$ levels above the lowest is $2i$, and that there are $\frac{N + 1}{2^{i + 1}}$ nodes in that level. Therefore, the total number of comparisons is $\sum_{i = 1}^{h + 1} \frac{N + 1}{2^{i + 1}} 2i = (N + 1)\sum_{i = 1}^{h + 1} \frac{i}{2^i} \le (N + 1)\sum_{i = 1}^\infty \frac{i}{2^i} = 2(N + 1)$, by the series rules.

Selection Problem
-----------------

Given an array $A$ of $N$ integers and an integer $k \in \set{0, \ldots, N - 1}$, find the $k$th smallest item in $A$. If $k = 0$, the result is the minimum. If $k \approxeq \frac N 2$, the result is the median. If $k = N - 1$, the result is the maximum.

We could solve this by sorting the array, which would be $O(n \log n)$, or we could build a min-heap or max-heap and remove the appropriate number of items, both $O(n \log n)$.

There is also a better solution called **quickselect**, based on the divide and conquer approach - we divide the problem into smaller subproblems, and then solve these subproblems. The basic idea is to recursively choose a pivot, partition the array into an array where every element is less than or equal to that pivot and an array greater than that pivot, then recurse on one of the smaller arrays to find the solution:

    def quick_select(A, k):
        if len(A) == 1: return A[0] # only one element, so must be the answer
        x = the pivot element # an element of the array that we choose to split on - how we choose it is up to us
        partition `A` into two sections (possibly in-place) `A[0:i - 1]` and `A[i + 1, n - 1]` such that `A[0:i - 1]` contains all elements that are less or equal to the pivot (excluding the pivot itself), and `A[i + 1, n - 1]` contains all elements that are greater, and `A[i]` is the pivot `x`
        if k == i: return x # the pivot element is the `k`th smallest element, so we found the answer
        if k < i: return quick_select(A[0:i - 1], k) # the answer is in the left partition, so recurse on the smaller problem with the same index
        if k > i: return quick_select(A[i + 1, n - 1], k - i - 1)

Partitioning can be done in-place in $O(n)$ time worst-case:

    def partition(A, i):
        swap A[0] and A[i] # we want to get `x` out of the way first, so we put it in `A[0]`
        
        i, j = 1, len(A) - 1
        while True: # this is linear since `i` can only increase and `j` can only decrease, so we have at most one comparison with `A[0]` per element>
            # find an element that belongs in the left partition, and an element that belongs in the right partition
            while i < len(A) and A[i] <= A[0]: i += 1 # go right until `A[i]` is greater than `x`
            while A[j] > A[0]: j -= 1 # go left until `A[j]` is less than or equal to `x`
            
            if j < i: # all of the left side is less or equal to `x` and all the right side is greater, so the partitioning is done
                # the place where the elements that are less or equal and the elements that are greater meet is the boundary between partitions
                swap A[0] and A[j] # put the pivot back into its place between the two partitions, since A[j] is less or equal to `x`
                break
            
            # at this point, `i` is greater than or equal to `j`, where `i` is the index of an element that is on the right but belongs on the left, and `j` represents the index of an element that is on the left but belongs on the right
            swap A[i] and A[j] # by swapping, we are moving each element into their correct partition

The worst case runtime for quickselect is $T(N) = \begin{cases}
c_1 &\text{if } N = 1 \\
T(N - 1) + c_2 N &\text{if } N > 1 \\
\end{cases} = c_2 N + c_2 (N - 1) + \ldots + c_2 2 + c_1 = O(N^2)$ where $c_1, c_2$ are constants - in the worst case, quickselect takes quadratic time. However, in the best case it takes just $O(n)$, where the pivot is the $k$th smallest element.

The average case runtime is $T_{avg}(N) = \frac{\sum_{\text{each input } i \text{ of size } N} T(i)}{\text{number of inputs with size } N}$. This is not feasible to calculate directly, but we can actually describe every input instance as a permutation $P$, since we only care about the order of numbers, not the values themselves. We will analyze the worst case of $k$, where we need the maximum amount of recursion, in order to not have to consider it in our calculations.

With these assumptions in place, we can now write it as $T_{avg}(N) = \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements}} T(i)}{N!}$. We now split the sum into those where the pivot ended up being roughly in the middle of the array after partitioning, and those where the pivot did not: $\frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \in [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!} + \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \notin [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!}$.

Note that when the pivot ends up being roughly in the middle, we are recursing on an input that is roughly half the size of the original input, which is the best case scenario for splitting input. When the pivot is not near the middle, in the worst case of $k$ the input we are recursing on could be almost as large as the original input.

Clearly, if $i \in [\frac 1 4 N, \frac 3 4 N)$, which is $\frac 1 2$ of the time, then when we recurse in quickselect, our new input size is proportionally smaller, at most $\frac 3 4$ of the original size, so $T(N) = T(\frac 3 4 N)$. If $i \notin [\frac 1 4 N, \frac 3 4 N)$, which is also $\frac 1 2$ of the time, then the largest array we might recurse on could be roughly the same size as the original array, so $T(N) = T(N - 1) + c_2 N$.

Therefore, on average $T(N) = \begin{cases}
c_1 &\text{if } N = 1 \\
\frac 1 2 T(\frac 3 4 N) + \frac 1 2 T(N - 1) + c_2 N &\text{if } N > 1 \\
\end{cases} = O(n)$ where $c_1, c_2$ are constants. This means that quickselect is, on average, a linear time operation, with a $O(n^2)$ worst case and a $O(n)$ best case.

Also, exponential runtime includes all bases, but each base is distinct: $O(4^n)$ is not the same as $O(2^n)$.

# 27/1/15

Quicksort
---------

Quicksort sorts by partitioning the input into two based on a pivot, then sorting each partition recursively. This can all be done in place:

    def quicksort(A, left, right):
        # postcondition: `A[left]`, ..., `A[right]` is sorted
        if right >= left: return # the array is already sorted since it has 1 or fewer elements
        pivot = find_pivot(A, left, right)
        i = partition(A, left, right, pivot) # `i` is the new position of the original `pivot` element
        quicksort(A, left, i - 1)
        quicksort(A, i + 1, right)

Quicksort is similar to quickselect, but we sort both sides when we recurse. In practice, quicksort is very fast, though in theory heapsort and mergesort are better.

Let $T(n)$ be the runtime for sorting $n$ elements, which is $right - left + 1$. Clearly, $T(n) = \begin{cases}
O(1) &\text{if } n \le 1 \\
T(\text{size of left partition}) + T(\text{size of right partition}) + O(n) &\text{if } n > 1 \\
\end{cases}$.

We can also write this more formally:

> Let $c_1, n_0$ exist such that everything in the function except the recursive calls takes $c_1 n_0$ time or less for all $n \ge n_0$. Let $c_2 = \max(T(0), \ldots, T(n_0))$. Let $c = \max(c_1, c_2)$.  
> Then $T(n) \le c_2 \le c_2 \le c \le cn \le T(\text{size of left partition}) + T(\text{size of right partition}) + cn$ if $2 \le n \le n_0$. So $T(n) \le \begin{cases}
c &\text{if } n \le 1 \\
T(\text{size of left partition}) + T(\text{size of right partition}) + cn &\text{if } n > 1 \\
\end{cases}$.  

In the worst case the size of one of the partitions will be $n - 1$, when the pivot is the largest or smallest element each time - sorted input. So $T(n) = T(1) + T(n - 1) + O(1) = O(n^2)$.

In the best case the size of each partition will be roughly equal, when the pivot is one of the median elements. So $T(n) = 2T\left(\frac n 2\right) + O(n) = O(n \log n)$.

The average case proof is a bit more difficult:

> Clearly, the average is simply take the average of all possible pivot combinations: $T_{avg}(n) = \frac{\sum_{i = 0}^{n - 1} \left(T_{avg}(i) + T_{avg}(n - i + 1) + cn\right)}{n} = \frac 1 n \sum_{i = 0}^{n - 1} T_{avg}(i) + \frac 1 n \sum_{i = 0}^{n - 1} T_{avg}(n - i + 1) + cn = \frac 2 n \sum_{i = 0}^{n - 1} T_{avg}(i) + cn$.  
> We claim that given $D = \max\left(T(0), T(1), \frac{T(2){2 \ln 2}}, 18c\right)$, $T_{avg}(n) \le D \max(1, n \ln n)$ for all $n$. This can be proven uing induction over $n$, and $T_{avg}(n) \le D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 n D + cn$.  
> Clearly, $D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 n (T(0) + T(1)) + cn \le D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 3 (D + D) + cn$ because $n \ge 3$.  
> So $D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 3 (D + D) + cn \le \frac 1 2 n^2 \ln n - \frac 1 4 n^2 \le \frac{2D} n \frac 1 2 n^2 \ln n - \frac{2D} n \frac 1 4 n^2 + cn \le D n \ln n - \frac D {18} n + cn = O(n \log n)$.  
> Basically, $\frac 2 n \sum_{i = 0}^{n - 1} T(i) + cn \in O(n \log n)$.  

Quicksort and quickselect are bad in the worst case but good in the average case. In real life, how good it is will depend on what kind of input we run it on. The average case assumes that the every input is equally likely to occur, but it is not necessarily true. For example, sorting almost sorted arrays is a very common use case in real life, which, given our pivot selection technique, would often give the worst case behaviour.

Instead, we can force the instances to become equally likely, to get something like average case behaviour. We do this by using randomization - eliminating the bias caused by real world input by adding a random factor to the algorithm.

For quicksort, we can do this by selecting a random pivot instead of just always taking the first element in the array as the pivot. This makes all elements equally likely, so the probability of choosing a bad pivot is always $\frac 1 n$, regardless of what the input is. The average case is now $O(n \log n)$ regardless of what kind of input we have.

# 29/1/15

We will prove that all comparison sorting algorithms are $\Omega(n \log n)$. However, there are other sorting algorithms that can also run in $O(n)$ time, which we will also look at.

A comparison sorting algorithm is one that is based purely on key comparisons - we can only compare two keys to see if one is greater, less, or equal to another, and no other operations are available.

We don't really have tools for analyzing arbitrary algorithms. However, for comparison-based algorithms we have **decision trees**:

Consider an example, sorting three elements $A[0..2]$:

                                         A[0] \le A[1]
                                        / false       \ true
                           A[1] \le A[2]               same as left side, but with A[0] and A[1] exchanged
                          / false       \ true
    A[0] \le A[1] \le A[2]               A[0] \le A[2]
                                        / false       \ true
                  A[0] \le A[2] \le A[1]               A[0] \le A[2] \le A[1]

Decision trees are binary trees that represent every possible decision path in the input, where each leaf is an output and each nde represents a decision. This decision tree represents the absolute minimum number of comparisons that we have to use in order to correctly sort three values

The worst case number of comparisons is the length of the longest path from the root to the leaf - the height of the tree. The length of a path is the number of edges in it.

Using these, we can now construct a proof:

> No matter what comparison sorting algorithm we use to sort, it is based on comparisons and is therefore representable using decision trees.  
> When we sort $n$ items, there are $n!$ possible outputs - every permutation of the input is a possible output.  
> Clearly, any binary tree of height $h$ must have at most $2^h$ leaves, so any binary tree with $l$ leaves must have height at least $\log_2 l$.  
> So the height $h$ of the decision tree is at least $\log_2 n!$.  
> Clearly, $h \ge \log_2 n! = \log_2 (n(n - 1) \cdots 1) \ge \log_2 n + \log_2 (n - 1) + \ldots + \log \frac n 2 \ge \frac n 2 \log_2 \frac n 2 = \frac n 2 \log n + \frac n 2 \log_2 \frac 1 n = \frac n 2 \log n - \frac n 2$.  
> So $h \in \Omega(n \log n)$.  

However, non-comparison sorting algorithms can be faster. **Bucket sort** is how most people will sort things by keys like names or grades. Basically, we have one pile for each possible value, make a pile for each value, and put them together:

    let $0 \le A[i] \le R - 1$ be an integer ($R$ is the radix)
    create an array $R$ of empty lists $L[0], \ldots, L[R - 1]$
    for element in A:
        append `element` t `L[element]`
    concatenate all the lists $L[0], \ldots, L[R - 1]$ together into another list `L`
    the sorted result is now `L`

This is a stable sort. Since we iterate through the array, and the lists take a constant time to create, bucket sort is $O(n + R)$ in the worst case.

This is not a comparison sorting algorithm because it uses more information than just comparisons - it also relies on assumptions about the keys all being in the finite set of possible values.

**Key-index/count sort** is similar to bucket sort (and is also stable), but doesn't need to use lists, which are something we try to avoid. The idea is that we know where the elements of $L[1]$ will be in the final result based on the number of elements in $L[0]$, where the elements of $L[2]$ based on where $L[1]$ is and the number of elements in it, and so on:

    # compute length of each list
    list_lengths = [0] * R
    for element in A:
        list_lengths[element] += 1
    
    # compute position of each array in the result
    list_positions = [0] * R
    for i in range(1, R):
        list_positions[i] = list_positions[i - 1] + list_lengths[i - 1]
    
    # put each value into the correct position
    initialize `result` as a list with the same length as `A`
    for element in A:
        result[list_positions[element]] = element
        list_positions[element] += 1

This can also be done in place, and we can combine `list_length` into `list_positions`. This is still $O(n + R)$ since we have to initalize the array of size `R` to 0.

In practice, we wouldn't want to use these sorts for any real-world inputs. This is because although it is $O(n)$, it is often still slower than a good comparison sort for real-world number values.

These sorts also work for occasions when we have too many buckets to be practical, like names. We could first sort by the first letter/digit of the name/grade, then recursively sort each of the buckets based on the second letter/digit, and so on. This technique is also called **MSD-radix-sort**. When $R = 2$, this is very similar to quicksort.

**LSD-radix-sort** is a bit better, since we don't have to keep track of all the indices by which we're sorting recursively:

    for d from the length of the largest value down to 1:
        sort `A` by the `d`th significant digit with a stable sorting algorithm

The time complexity for this is simply $O(m(n + R))$ where $m$ is the largest possible number of digits. Typically, the number of digits or letters is bounded, so the depth of recursion is bounded by a constant and the worst case time complexity is $O(n + R)$.

# 3/2/15

An empty tree has height -1. A tree with one node has height 0. The height of a tree is 1 more than the larger of the heights of the two subtrees.

A **dictionary** is an abtract datatype that associates keys to values. Essentially, it stores key-value pairs, and supports insertion of a key value pair, searching for a key-value pair by key, and deleting a key-value pair by key.

We assume that all the keys are unique and can be compared in constant time, and each pair takes up constant space.

A naive way to implement dictionaries is to use unsorted arrays/lists ($O(1)$ insert, $O(n)$ search, $O(n)$ delete) or sorted arrays/lists ($O(n)$ insert, $O(\log n)$ search, $O(n)$ delete).

A **binary search tree** is a binary tree where for every node, every key in the left subtree is smaller than it and the every key in the right subtree are larger. We don't have to care about the duplicate keys since we assume keys are unique.

Implementing dictionaries using binary search trees is possible by constructing the BST where the value of each pair is its key ($O(h)$ insertion, $O(h)$ search, $O(h)$ deletion). It is always true that $h \in \Omega(\log n)$ and $h \in O(n)$.

We want to make sure $h$ is $O(\log n)$, by **rebalancing** the binary search tree as we perform operations on it. We do this by imposing additional conditions on the tree, and then showing that we can do all operations while maintaining these conditions and taking $O(h)$ time or less, and that these conditions cause the height of the tree to be $O(\log n)$.

There are many possible sets of conditions that we can use to make sure the binary tree is always balanced. One of these are those used in **AVL-trees**, which imposes just one condition: for any node, the height of the left and right subtree differ by at most 1 - the structural condition.

AVL trees were one of the first ever self-balancing binary trees, and although they work, they are rather slow in practice.

When we store a node in an AVL tree, we will also **store the height of the tree formed by that node and its children in that node**. This allows us to efficiently do the calculations we need later for insertions and deletions.

Searching in an AVL tree is exactly the same as in a normal BST. Since we don't modify the tree, we also can't violate the strutural condition.

For insertion, we need to insert the key-value pair at its proper spot, then check if the tree is still balanced and rebalance if not. Inserting into the correct subtree is the same as in a normal BST, but we also need to update the tree heights we are storing with the nodes - the newly inseted node gets height 0, and we recalculate the height of its parent. If the height changed, we recurse and recalculate the height of its parent, and so on.

While we are recalculating heights, we also check if the heights of the children of the ancestor differ by at most 1. If they differ by more, then **we need to rebalance on that node** - to make sure it is both a BST and tree heights don't differ by more than 1. We basically have 4 possible rebalancing operations, and it was proven that at least one of them will correctly rebalance the tree.

The first is **right rotation on node Z**. Given a BST with nodes in the form $Z(Y(X(A, B), C), D)$, we can rearrange it as $Y(X(A, B), Z(C, D))$ while still maintaining the BST properties, since $A < X < B < Y < C < Z < D$. Basically, we rotate $Y$ and $Z$ clockwise/right, changing only those elements so the entire operation is constant time.

The second is **left rotation on node Z**, which is similar but we are rotating in the opposite direction. Given a BST with nodes in the form $Z(A, Y(B, X(C, D)))$, we an rearrange it as $Y(Z(A, B), X(C, D))$ while still maintaining the BST properties.

The third is **double right rotation on node Z**. Given a BST with nodes in the form $Z(Y(A, X(B, C)), D)$, we can rearrange it as $X(Y(A, B), Z(C, D))$. Basically, we are bringing $X$ up to replace $Z$. This is called a double right rotation because it can be implemented as left rotate on $Y$ and right rotate on $Z$.

The fourth is the **double left rotation on node Z**. Given a BST with nodes in the form $Z(A, Y(X(B, C), D))$, we can rearrange it as $X(Z(A, B), Y(C, D))$.

Basically, we try each operation in turn, and check if each one is balanced. If so, then we can stop trying the others.

;wip: download the modules from the website

# 5/2/15

Operations on AVL trees can now be defined:

    def insert(k, v):
        insert `k` and `v` into the tree as a new leaf, without rebalancing
        let `node` be the new node that was inserted
        while `node` is not the root node:
            p, s = parent(node), sibling(node)
            if abs(s.height - node.height) >= 2: # node unbalanced, rebalance it
                rebalance(node)
                node = p
            else: # node balanced, update the height
                new_height = max(node.height, s.height) + 1
                if (new_height == p.height): break # the height has stopped changing and we can stop rebalancing
                p.height = new_height
    
    def delete(k):
        delete the node `k`, without rebalancing
        let `node` be the node that was deleted
        while `node` is not the root node:
            p, s = parent(node), sibling(node)
            if abs(s.height - node.height) >= 2: # node unbalanced, rebalance it
                rebalance(node)
                node = p
            else: # node balanced, update the height
                new_height = max(node.height, s.height) + 1
                if (new_height == p.height): break # the height has stopped changing and we can stop rebalancing
                p.height = new_height
    
    def rebalance(node):
        let `y` be the larger child of `node`
        let `x` be the larger child of `y` # this child must exist since the height differs by more than 1, so `node` must have grandchildren
        perform the rebalancing operation according to the four rebalancing cases, based on whether `y` and `x` are left and right children
        update the heights of `node`, `y`, and `x`

The height $h$ of an AVL-tree is always at most $O(\log n)$. Proof:

> We want to prove that that $h \le \log_c n = O(\log n)$ for a constant $c$.  
> We want to find $c$ and $n$ for $n \ge c^h$ - given a fixed height, we want to find the smalles number of nodes we can have.  
> Clearly, for height 0, there must be at least 1 node. Clearly, for height 1 there must be at least 2 nodes. Clearly, for height 2 there must be at least 4 nodes. Clearly, for height 3 there must be at least 7 nodes.  
> By induction, we can show that the minimum number of $n$, represented $n(h)$ must be $n(0) = 1, n(1) = 2, n(h) = 1 + n(h - 1) + n(h - 2)$, since each minimal tree must have the smaller minimal trees as its children, and we want to use one as large as possible while the other is as small as possible.  
> Interesingly, the values of $n(h)$ is simply 1 less than each Fibonacci number. The Fibonacci numbers grow exponentially, so $n(h)$ does too.  
> Clearly, $n(h) \ge \sqrt{2}^h$, since $n(h) = n(h - 1) n(h - 2) + 1 \ge n(h - 2) + n(h - 2) = 2n(h - 2) \ge 2^2 n - 4 \ge \ldots \ge 2^i n(h - 2i), i > 1$, so $n(h) \ge 2^{\ceil{\frac h 2}}$.  
> So any AVL-tree of height $h$ has $\sqrt{s}^h$ or more nodes, so $h \le \log_{\sqrt 2} n = O(\log n)$.  

Clearly, the worst case time for insertion/deletion/search is $O(h)$, and since $h \in O(\log n)$ and each rebalancing operation takes constant time, insertion/deletion/search is always $O(\log n)$.

### 2-3-trees

A 2-3-tree is similar to AVL-trees in concept, but we no longer have to be a binary tree - a node may have 2 or 3 children. The 2-3-tree could be said to be a generalization of the AVL-tree.

Each node in a 2-3 tree is either a node with one key-value pair and two children, or a node with two key-value pairs and three children (the left child is less than key 1, the middle child is between key 1 and 2, and the right child is greater than key 2, and key 2 must be greater than key 1). Additionally, all empty subtrees must be on the same level.

Searching is done by a generalized version of the standard BST search - when we are comparing, we simply need to check how many key-value pairs the node has and check the cases accordingly.

Insertion into a 2-3 key is done by searching for the node on the lowest level that would have had the key as a child if it was inserted directly. If it has only one key-value pair, then add our key-value pair to it to make it a node with two key-value pairs. Otherwise, we add the key-value pair to it to make a node with three key-value pairs, then split it into three nodes with one key-value pair each and move the middle one (the one that is neither largest or smallest) up to the parent, so the node becomes two nodes and the parent gets an extra key-value pair. If the parent now has three nodes when we do this, we recursively perform this splitting and moving up operation on the parent, all the way up until no nodes have three key-value pairs.