<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS240 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>
function highlight() { // highlight all code blocks using HighlightJS
  var code_blocks = document.getElementsByTagName("code");
  for (var i = 0; i < code_blocks.length; i++)
    hljs.highlightBlock(code_blocks[i]);
}
</script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/Résumé.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.mesecons.net/">mesecons</a></li>
    <span class="divider"></span>
    <li><a href="http://www.autohotkey.net/~Uberi/">autohotkey.net</a></li>
  </ul>
<h1 id="cs240">CS240</h1>
<p>Data Structures and Data Management.</p>
<pre><code>Therese Beidl
Section 002
Email: beidl@uwaterloo.ca
Web: http://www.student.cs.uwaterloo.ca/~cs240
ISA: Joseph (Danny) Sitter
ISA Email: cs240@studennt.cs.uwaterloo.ca</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l&#39;H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
\]</span></p>
<p>Seciton 1 and 2 are the regular classes, and section 3 is the enhanced section and has very different content.</p>
<p>Midterm at Thurs. Feb. 26 at 4:30 PM, worth 25%. Final exam worth 50%. Must pass weighted average of exam to pass the course. 5 assignments, each worth 5%.</p>
<h1 id="section">6/1/15</h1>
<p>Assignment 0 is due on Tuesdays. Assignment 0 is due on Jan. 13. ;wip: do it</p>
<p>Suppose we have a lot of data to keep track of. We could store it in an array/list, but depending on the type of the data, this might not be the best choice. Data structures should make it easy and efficient to perform the operations we need. For example, an English dictionary probably needs to be efficiently searched, but we don't really need to care about insertion and deletion since they're so rare.</p>
<p>The best data structure for something depends on the type of data we want to store. Our goal is to have a short running time and little memory.</p>
<p>In this course we will be performing theoretical analysis, developing ideas and pseudocode (and sometimes, implementations), and analyzing them using tools like big-O notation.</p>
<p>An Abstract Data Type is the idea of describing something by what it can do, not how it does it.</p>
<p>Required background includes arrays, linked lists, strings, stacks, queues, ADTs, recursive algorithms, trees, sorting algorithms (insertion, selection, quick, merge), binary search/BSTs, arithmetic series, geometric series, harmonic series (<span class="math">\(\frac 1 1 + \ldots + \frac 1 n = \ln n\)</span>).</p>
<p>In this course, <span class="math">\(\log n\)</span> is implicitly <span class="math">\(\log_2 n\)</span> - all logarithms are base 2 unless otherwise stated.</p>
<p>A <strong>problem</strong> is a description of a general situation and the desired outcome. For example, the sorting problem is &quot;given comparable values, put them in sorted order&quot;.</p>
<p>A <strong>problem instance</strong> is a particular input to a problem, like a particular array of numbers to sort for the sorting problem.</p>
<p>A <strong>problem solution</strong> is a change/process that, given the situation of a problem instance, results in the desired outcome. For example, the sorted array of numbers for the sorting problem.</p>
<p>We <strong>solve</strong> a problem by giving the correct algorithm for it. A solution is <strong>correct</strong> if it finds a solution for every possible input that can be given.</p>
<p>An <strong>algorithm</strong> is a finite description of a process that gives an answer (a solution that is not necessarily correct) for all possible instances of a problem.</p>
<p><strong>Efficiency</strong> usually refers to an algorithm's runtime, and sometimes its memory usage. It may also refer to things specific to the problem domain, like the number of comparisons done.</p>
<p>To solve a problem:</p>
<ol style="list-style-type: decimal">
<li>Design an algorithm.</li>
<li>Write down the main idea of the algorithm in plain prose. For example, &quot;for increasing <span class="math">\(i\)</span>, make <span class="math">\(A[0 \ldots i]\)</span> sorted by inserting <span class="math">\(A[i]\)</span> into a sorted <span class="math">\(A[0 \ldots i - 1]\)</span>&quot; for the sorting problem.</li>
<li><p>Optionally, write pseudocode - code that might not be a real language, and is something like a mix between prose and code. This is a more precise way of specifying an algorithm. Any consistent, precise, and clearly understandable language will be accepted as pseudocode.</p>
<pre><code>Preconditions: an array `A[0 ... n - 1]`$.
Postconditions: `A` is sorted
for i in 1 ... n - 1:
  value = A[i]
  j = i - 1
  while j &gt;= 0 and A[j] &gt; key:
    A[j + 1] = A[j]
    j = j - 1
  A[j + 1] = value</code></pre></li>
<li>Argue/prove that it is correct.</li>
<li>We can use formal correctness proofs, but this is often excessive to convince someone an algorithm is correct. Instead, we can simply give lots of invariants and prove that the algorithm terminates.</li>
<li>For example, for the above, we would say that for the inner loop, &quot;<code>A[j + 1 ... i]</code> contains items that are bigger than <code>value</code>, and is in sorted order&quot; is an invariant.</li>
<li>It is very important to prove that it terminates, especially recursive algorithms. We simply use the standard methods specified in CS245, but we can usually just write that each call or iteration gets some smaller input, and there is some small input for which the algorithm terminates.</li>
<li>Analyze how good the algorithm is, in terms of efficiency and sometimes lower bounds.</li>
<li>For this we use time complexity/running-time analysis, memory usage analysis, and so on.</li>
<li>For example, the above code is known as Insertion Sort, which we already know to have a worst case time complexity of <span class="math">\(O(n^2)\)</span> and a best case time complexity of <span class="math">\(O(n)\)</span>.</li>
<li>Recall that formally, an algorithm being in <span class="math">\(O(f(n))\)</span> means that there exists a <span class="math">\(k &gt; 0\)</span> and <span class="math">\(m \ge 0\)</span> such that for all <span class="math">\(n &gt; m\)</span>, <span class="math">\(k \cdot f(n) &gt; T(n)\)</span> where <span class="math">\(T(n)\)</span> represents the number of constant time steps or amount of time that the algorithm needs to give an answer for an input of size <span class="math">\(n\)</span> - the running time function. This is written as <span class="math">\(T(n) \in O(f(n))\)</span>.</li>
<li><p>Repeat above steps until satisfactory algorithm is found.</p></li>
</ol>
<p>Only after this point would we implement/experiment the algorithm.</p>
<h1 id="section-1">8/1/15</h1>
<p>A <strong>timing function</strong> is a function <span class="math">\(S_p: \mathrm{Input} \to \mb{R}^+\)</span> where <span class="math">\(p\)</span> is the program we are running. For example, we can compare quicksort and mergesort by comparing <span class="math">\(S_{\text{mergesort}}(\tup{5, 6, 2, 3})\)</span> and <span class="math">\(S_{\text{quicksort}}(\tup{5, 6, 2, 3})\)</span>.</p>
<p>A timing function measures the amount of time a program takes for a given input.</p>
<p>Since computers are constantly changing and improving, comparing timing functions is not very meaningful since they will not stay consistent over differnt machines and over time as computers change. We want a machine-agnostic way to measure the amount of time a given program takes.</p>
<p>For example, Intel's core instruction set is RISC, with tons of extensions such as MMX, SSE, and FPU and GPU stuff. These add instructions such as <code>fsqrt</code> and similar. We assume that every instruction takes a finite, constant amount of time (interesting note).</p>
<p>We can instead measure the number of elementary operations, but it is difficult to define what an elementary operation actually is, and determining the exact number of elementary operations is a lot of work. For example, we might measure the number of clock cycles needed to run a program, but this would not make sense across things like RISC vs CISC machines. For example, if an architecture doesn't have a constant time division operation, we would probably need to implement one in software that would not have constant time.</p>
<p>In other words, we write our algorithms in pseudo-code, then count the number of primitive operations.</p>
<p>We now want to plot our timing function with respect to the size of the input <span class="math">\(\abs{\mathrm{Input}} \subseteq \mb{N}\)</span>. This allows us to figure out the behaviour of the function as the size of the input gets bigger. This is difficult since finding the timing function for arbitrary inputs is often difficult. Additionally, we don't necessarily care about the constants in the function output, just its behaviour on large inputs.</p>
<p>Usually we care about the worst case behaviour, to determine the worst possible behaviour in all cases. We may also be interested in the average case to see how things will perform in practice. Occasionally, we may also be interested in the best case - for example, in cryptography, we want to make sure the problem cannot be solved in less than some given amount of time.</p>
<p>The worst case behaviour is <span class="math">\(T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}\)</span>. The best case behaviour is <span class="math">\(T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}\)</span>. The average case behaviour is <span class="math">\(T_p(n) = \frac{\sum_{e \in  R} i}{\abs{R}}\)</span> where <span class="math">\(R = \set{S_p(I) \middle| \abs{I} = n}\)</span>. We can now plot these functions to get the time bounds on our program.</p>
<p>The big-O notation is used to make life easier for computer scientists trying to calculate timing functions and to make comparing functions much easier. Refer to the CS136 notes for background. The <span class="math">\(O\)</span> in <span class="math">\(O(f(x))\)</span> is actually a capital omicron, bu nowadays the O is more common.</p>
<p>Comparing Big O is analogous to ordering in <span class="math">\(\mb{R}\)</span>. Just like how <span class="math">\(x \le y\)</span>, we might write <span class="math">\(f(x) \in O(g(x))\)</span>. What we care about is the trends of the function, not the actual numbers.</p>
<p>Big-O gives an upper bound behaviour on a function (analogous to <span class="math">\(x \le y\)</span>). Big-<span class="math">\(\Omega\)</span> gives a lower bound (analogous to <span class="math">\(x \ge y\)</span>). Big-<span class="math">\(\Theta\)</span> us the exact bounds - when the Big-<span class="math">\(O\)</span> is the same as the Big-<span class="math">\(\Omega\)</span> (analogous to <span class="math">\(x = y\)</span>).</p>
<p>There is also Little-<span class="math">\(o\)</span>, which is a non-inclusive upper bound (analogous to x &lt; y), and Little-<span class="math">\(\omega\)</span>, which is a non-inclusive lower bound (analogous to <span class="math">\(x &gt; y\)</span>). For little-o, instead of <span class="math">\(\exists c &gt; 0, \exists n_0 &gt; 0, n &gt; n_0 \implies f(n) \le cg(x)\)</span>, we have <span class="math">\(\forall c &gt; 0, \exists n_0 &gt; 0, n &gt; n_0 \implies f(n) \le cg(x)\)</span>.</p>
<p>There are also incomparable functions, like one that goes up and down arbitrarily. As a result, functions can only be partially ordered.</p>
<h1 id="section-2">13/1/15</h1>
<p>Big-O does not care about the values of the functions - only about how it grow as input values get large.</p>
<p>Prove that <span class="math">\(2n^2 + 3n + 11 \in O(n^2)\)</span>:</p>
<blockquote>
<p>We want to prove that <span class="math">\(2n^2 + 3n + 11 \le cn^2\)</span> for all <span class="math">\(n &gt; n_0\)</span>.<br />Let <span class="math">\(n_0 = 10\)</span> and <span class="math">\(n \ge n_0\)</span>. Clearly <span class="math">\(2n^2 + 3n + 11 \le 2n^2 + 3n^2 + 11n^2\)</span> when <span class="math">\(n \ge 1\)</span> and <span class="math">\(2n^2 + 3n^2 + 11n^2 = 16n^2\)</span>.<br />Then 16 is a possible value for <span class="math">\(c\)</span> and <span class="math">\(2n^2 + 3n + 11 \in O(n^2)\)</span>.</p>
</blockquote>
<p><span class="math">\(O(n^2 + \log n)\)</span> is not correct since it is not fully simplified.</p>
<p>Common time complexities include <span class="math">\(\Theta(1)\)</span> (constant), <span class="math">\(\Theta(\log n)\)</span> (logarithmic), <span class="math">\(\Theta(n)\)</span> (linear), <span class="math">\(\Theta(n \log n)\)</span> (pseudo-linear), <span class="math">\(\Theta(n^2)\)</span> (quadratic), <span class="math">\(\Theta(n^3)\)</span> (cubic), <span class="math">\(\Theta(n^k)\)</span> (polynomial), <span class="math">\(\Theta(2^n)\)</span> (exponential). In the real world, everything above <span class="math">\(O(n \log n)\)</span> tends to be rather bad performance.</p>
<p>It is not always the case that an algorithm with better time complexity than another is always better in all circumstances. For example, despite insertion sort being <span class="math">\(O(n^2)\)</span> and merge sort being <span class="math">\(O(n \log n)\)</span>, we often use insertion sort when sorting smaller lists since it is faster in practice when the input is small. Many practical sorting algorithms therefore drop down into insertion sort when the input is small. This is also why most people use insertion sort when sorting things in real life - it is very fast for real-world inputs, which are usually small.</p>
<p>Let <span class="math">\(L = \lim_{n \to \infty} \frac{f(n)}{g(n)}\)</span>. If <span class="math">\(L = 0\)</span>, <span class="math">\(f(n) \in o(g(n))\)</span>. If <span class="math">\(0 &lt; L &lt; \infty\)</span>, <span class="math">\(f(n) \in \Theta(g(n))\)</span>. Otherwise, <span class="math">\(L = \infty\)</span> and <span class="math">\(f(n) \in \omega(g(n))\)</span>. This is a useful way to prove orders for functions that would otherwise be difficult to prove by first principles.</p>
<p>For example, if we want to prove that <span class="math">\(n^2 \log n \in o(n^3)\)</span>, we can do <span class="math">\(\lim_{n \to \infty} \frac{n^2 \log n}{n^3} = \lim_{n \to \infty} \frac{\log n}{n} \lH \lim_{n \to \infty} \frac{\frac 1 n}{1} = 0\)</span>, and use the fact that <span class="math">\(L = 0\)</span> to conclude that <span class="math">\(n^2 \log n \in o(n^3)\)</span>.</p>
<p>We can aso use this to prove that <span class="math">\((\log n)^a \in o(n^b)\)</span> for any <span class="math">\(a, b &gt; 0\)</span>.</p>
<p>Also, <span class="math">\(f(n) \in \Omega(g(n)) \iff g(n) \in \Omega(f(n))\)</span>, <span class="math">\(f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))\)</span>, <span class="math">\(f(n) \in o(g(n)) \iff g(n) \in \omega(f(n))\)</span>, <span class="math">\(f(n) \in \Theta(g(n)) \iff g(n) \in O(f(n)) \wedge g(n) \in \Omega(f(n))\)</span>, <span class="math">\(f(n) \in o(g(n)) \implies f(n) \in O(g(n))\)</span>, <span class="math">\(f(n) \in o(g(n)) \implies f(n) \notin \Omega(g(n))\)</span>, <span class="math">\(f(n) \in \omega(g(n)) \implies f(n) \in \Omega(g(n))\)</span>, and <span class="math">\(f(n) \in \omega(g(n)) \implies f(n) \notin O(g(n))\)</span>. We can use these to simplify order notation expressions.</p>
<p>Also, <span class="math">\(O(f(n) + g(n)) = O(\max(f(n), g(n))) = \max(O(f(n)), O(g(n)))\)</span>. Also, orders are transitive - <span class="math">\(f(n \in O(g(n))) \wedge g(n) \in O(h(n)) \implies f(n) \in O(h(n))\)</span>.</p>
<p>In our pseudocoe, we often use <span class="math">\(x \left 5\)</span> or <code>x := 5</code> to denote assignment, since <code>=</code> is ambiguous - it could potentially mean equality or assignment.</p>
<p>Math with orders works a lot like normal math: <span class="math">\(\Theta(1) \cdot (1 + \ldots + n) = \Theta(1) \cdot \frac{n^2 + n}{2} = \Theta(n^2 + n) = \Theta(n^2)\)</span>.</p>
<p><span class="math">\(\sum_{i = 1}^n i^k = \Theta(n^{k + 1})\)</span></p>
<h1 id="section-3">15/1/15</h1>
<p>A priority queue is a data structure that stores items with keys that represent the priorities that the values have. It has two operations - insertion with a given key and deletion of the item with the maximum priority.</p>
<p>There are a number of ways we could implement a priority queue - an unsorted dynamic array (amortized <span class="math">\(O(1)\)</span> insert, <span class="math">\(O(n)\)</span> deletion), a sorted dynamic array, and something known as a heap. We don't want to use sorted order, since that makes insertion slow. We also dont't want to use unsorted order, since that makes deletion slow.</p>
<p>A <strong>heap</strong> is a data structure that allows us to implement a priority queue in a very efficient way. The main idea is that we would like to know very easily what the maximum element is, so we make all the other elements follow from it.</p>
<p>The basic idea behind the heap is that it is a tree where each element keeps track of candidates that can replace them when they're deleted, as children. The maximum item would therefore be the root. When we insert, we simply insert</p>
<p>Most commonly, we use binary heaps, which is a binary tree. Recall that this is a data structure consisting of nodes and links between node, such that each node has 2 or fewer children, and 1 or fewer parents.</p>
<p>Formally, a <strong>binary max-heap</strong> is a binary tree that satisfies the structural property and the max-heap property. There are no rules about the order of children, unlike a binary search tree.</p>
<p>The <strong>structural property</strong> is that all levels except the lowest level must be completely filled. Each node must have two children, except the lowest level, where nodes can have fewer than two children but <strong>must fill up from the left</strong> - in the lowest level, the left sibling of a node must have 2 children before we can insert a child into it.</p>
<p>The <strong>max-heap property</strong> is that for any node <span class="math">\(x\)</span> in the heap, the key of <span class="math">\(x\)</span> is less than or equal to the key of the parent of <span class="math">\(x\)</span>. There is also a min-heap property where the key of <span class="math">\(x\)</span> is greater than or equal to the key of the parent of <span class="math">\(x\)</span>, used for min-heaps. In other words, the parent of any node in a heap must be bigger or equal to that node. As a result, the maximum node of a heap is always the root node.</p>
<p>Insertion into a heap is easy because we can simply do a standard tree insertion, with certain constraints to ensure it satisfies the structural property and the heap property. This operation is <span class="math">\(\Theta(\log n)\)</span> because the structural property ensures that the height of the tree is <span class="math">\(\Theta(\log n)\)</span>.</p>
<p>Also, the height of a binary heap is <span class="math">\(\log_2 n - \omega(\log n)\)</span>. The height of a tree is 1 less than the number of levels it has (1 less than the maximum depth). Levels start at 1 and increase with depth.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(x\)</span> be a heap with <span class="math">\(n\)</span> nodes and height <span class="math">\(h\)</span>.<br />Clearly, any binary tree of height <span class="math">\(h\)</span> must have <span class="math">\(n \le 2^{h + 1} - 1\)</span>, since each level <span class="math">\(i\)</span> has less than or equal to <span class="math">\(2^{i - 1}\)</span> nodes, and the total is <span class="math">\(n \le \sum_{i = 1}^{h + 1} 2^{i - 1} \le \frac{2^{h + 1} - 1}{2 - 1}\)</span>.<br />Since <span class="math">\(n + 1 \le 2^{h + 1}\)</span>, <span class="math">\(h \ge \log(n + 1) - 1\)</span>.<br />Clearly, <span class="math">\(n \ge 2^h\)</span>, by the structural property, since the lowest level is always <span class="math">\(h + 1\)</span> so each level <span class="math">\(i\)</span> before it has exactly <span class="math">\(2^{i - 1}\)</span> nodes, and the total for all levels except the lowest is <span class="math">\(2^h - 1\)</span>. Since the lowest level has 1 or more nodes, the total is <span class="math">\(n \ge (2^h - 1) + 1\)</span>, or <span class="math">\(2^h\)</span>.</p>
</blockquote>
<h1 id="section-4">20/1/15</h1>
<p>Binary heaps are trees, so we can store them as a tree structure with references and nodes. However, a more efficient way to store binary trees is as an array:</p>
<ul>
<li><span class="math">\(A\)</span> is an array representing the heap.</li>
<li><span class="math">\(A[0]\)</span> contains the root node.</li>
<li><span class="math">\(A[1], \ldots, A[2]\)</span> store the children of the root node.</li>
<li><span class="math">\(A[3], \ldots, A[6]\)</span> store the children of the children of the root nodes, and so on.</li>
</ul>
<p>Essentially, we first have the first level, then the second level, and so on. Formally, the left and right children of a node stored in the array at index <span class="math">\(i\)</span> are <span class="math">\(lc(i) = 2i + 1\)</span> and <span class="math">\(rc(i) = 2i + 2\)</span> (in a 0-indexed array). Likewise, the parent of any node is <span class="math">\(p(i) = \floor{\frac{i - 1} 2}\)</span>. This works because the leftmost node in level <span class="math">\(l\)</span> has index <span class="math">\(\sum_{k = 1}^{l - 1} 2^{k - 1}\)</span>.</p>
<p>In fact, this left and right parent system works for any binary tree, and even for <span class="math">\(n\)</span>-ary trees with a simple extension. However, it is not space efficient unless the tree obeys something like the structural property. The structural property ensures that the array is mostly filled, possibly except for part that stores the last level.</p>
<h3 id="heaps-as-priority-queues">Heaps as Priority Queues</h3>
<p>We will represent our heap as an array <span class="math">\(A\)</span> containing <span class="math">\(n\)</span> values, automatically expanded or shrunk as needed.</p>
<p>To <strong>insert</strong> into a heap, we first find the only location that the structural property allows us to insert a new node <span class="math">\(i\)</span> into. This could possibly violate the heap property since it is possible that <span class="math">\(i &gt; p(i)\)</span>, so if <span class="math">\(i &gt; p(i)\)</span>, we swap <span class="math">\(i\)</span> and <span class="math">\(p(i)\)</span> to get <span class="math">\(p(i)&#39;\)</span> and <span class="math">\(i&#39;\)</span>, where <span class="math">\(p(p(i)&#39;) = i&#39;\)</span>.</p>
<p>Now the heap property is satisfied for <span class="math">\(i&#39;\)</span> and <span class="math">\(p(i)&#39;\)</span>, and it isn't possible for <span class="math">\(p(i)&#39;\)</span>'s sibling to be greater than <span class="math">\(i&#39;\)</span> since it was less than the original parent <span class="math">\(p(i)\)</span>. However, it is possible that <span class="math">\(i&#39;\)</span> is now greater than it's new parent <span class="math">\(p(i&#39;)\)</span>, so we swap again if <span class="math">\(i&#39; &gt; p(i)&#39;\)</span>.</p>
<p>We repeat swapping the originally inserted node with its parents until either it is less than its parent, or it is the new root node. This is called &quot;bubbling up until it holds&quot;.</p>
<p>In pseudocode:</p>
<pre><code>def insert(key):
    # wip: we resize the array here if necessary
    
    A[n] = key # $n$ is the index of the next free space in the heap - we keep track of this in the heap structure
    n ++
    j = N - 1
    while j &gt; 0 and A[p(j)] &lt; A[j]:
        swap A[j] and A[p(j)]
        j = p(j)</code></pre>
<p>Clearly, this takes <span class="math">\(O(\log n)\)</span> time worst case since every run of the loop runs on a lower height on the tree. The bubbling up process would potentially need to visit every level in the tree if inserting an element greater than the root element of the heap.</p>
<p>To <strong>delete the maximum</strong> of a heap, our goal is to remove the root element of the heap. When we do this, we now have a blank space at the root node of the heap. We fix this by moving the last node in the heap (the rightmost node on the lowest level) into the position the root originally occupied. This allows us to satisfy the structural property again.</p>
<p>However, now the heap property is not satisfied. We fix this now by swapping the root node (originally the last node) <span class="math">\(i\)</span> with the bigger of its children <span class="math">\(lc(i)\)</span> and <span class="math">\(rc(i)\)</span>, which we will represent as <span class="math">\(c(i)\)</span>, to get <span class="math">\(c(i)&#39;\)</span> and <span class="math">\(i&#39;\)</span> such that <span class="math">\(p(i&#39;) = c(i)&#39;\)</span>. Now we have satisfied the heap property between <span class="math">\(i&#39;\)</span> and <span class="math">\(c(i)&#39;\)</span> and between <span class="math">\(i&#39;\)</span>'s sibling and <span class="math">\(c(i)&#39;\)</span>, but it is possible that <span class="math">\(i&#39;\)</span>'s children could be greater than it.</p>
<p>Therefore, we repeatedly swap the originally last node with its larger child until it is greater (or equal to) than both of its children, or it is a leaf node.</p>
<p>In pseudocode:</p>
<pre><code>def delete_max():
    max = A[0]
    n -- # 1 less than the next free space is the last node in the heap, which we will consider as a free space after we move it to the root
    # if memory management is needed, free `A[0]` here
    A[0] = A[n]
    
    # the following is the bubble_down operation, which bubbles values down until they satisfy the heap property
    j = 0
    while j &lt;= n - 1:
        max_index = j
        if lc(j) exists and A[lc(j)] &gt; A[j]: # existance means that lc(j) &lt;= n - 1
            max_index = lc(j)
        if rc(j) exists and A[rc(j)] &gt; A[max_index]: # existance means that rc(j) &lt;= n - 1
            max_index = rc(j)
        if max_index == k: break
        swap A[j] and A[max_index]
        j = max_index
    
    # wip: we shrink the array here if necessary
    return max</code></pre>
<p>Clearly, this takes <span class="math">\(O(h)\)</span> time worst case where <span class="math">\(h\)</span> is the height of the heap underneath the starting element, the root, which is <span class="math">\(O(\log n)\)</span> since <span class="math">\(h\)</span> is roughly <span class="math">\(\log n\)</span>. The worst case is when the last node of the heap is also the smallest one.</p>
<p><strong>Priority queue sort</strong> (PQSort) is a sorting algorithm where we insert all our elements into a priority queue and then take them out again in order. PQSort needs <span class="math">\(n\)</span> insertions and <span class="math">\(n\)</span> deletions of maximums. Since each operation is <span class="math">\(O(\log n)\)</span>, PQSort is <span class="math">\(O(n \log n)\)</span>, like mergesort. When the priority queue is implemented with a heap, we also call it <strong>heapsort</strong>.</p>
<p>Heapsort is pretty nice because we can sort in-place (by building a heap directly) and without a lot of recursion, but it can have poor real-world behaviour because the worst case heap insertion occurs when we are inserting nodes one by one that are already in ascending order, and already-sorted data occurs very often in real-world applications.</p>
<p>Also, heapsort is not stable. If a sorting algorithm is <strong>stable</strong>, then if different nodes have equal keys, they appear in the same order in the output as they did in the input.</p>
<p>Also, we often measure the runtime of sorting algorithms in terms of the number of key comparisons made (<span class="math">\(A[i] &lt; A[j]\)</span>). This is because we often allow users to specify their own key computing functions, which could potentially be aribtrarily exxpensive. Everything else is simply proportional or a constant on top of the number of key comparisons, in terms of runtime.</p>
<p>We may also want to <strong>build heaps</strong> - given an array <span class="math">\(D\)</span> with <span class="math">\(c\)</span> values, we want to build a head from that. This is relatively trivial in theory - simply go through the array and insert it into a heap. Clearly, this is <span class="math">\(O(n \log n)\)</span> since insetion is <span class="math">\(O(\log n)\)</span> and we need <span class="math">\(n\)</span> of them.</p>
<p>Clearly, this takes roughly <span class="math">\(\sum_{k = 0}^{c - 1} \log(k + 1)\)</span> time since <span class="math">\(n\)</span> starts off at 0 and grows by 1 upon every insertion. Clearly, there are at least <span class="math">\(\frac n 2\)</span> terms in that sum where <span class="math">\(k + 1 \ge \frac n 2\)</span>, so <span class="math">\(\sum_{k = 0}^{c - 1} \log(k + 1) \ge \frac n 2 \log \frac n 2\)</span>, so the operation is <span class="math">\(\Omega(n \log n)\)</span>, and by the above, <span class="math">\(\Theta(n \log n)\)</span>.</p>
<p>A better option is to build the heap in place, to save memory. Clearly, an array always satisfies the structural property since there's no way not to, so all we have to do is enforce the heap property in the array.</p>
<p>We will enforce the heap property by bubbling downward on every level of the node starting from the last node that has children (the last node on the second-to-last level). Our invariant will be that for all <span class="math">\(k &gt; i\)</span>, the subheap rooted at <span class="math">\(k\)</span> satisfies the heap property. We will show that this takes less than or equal to <span class="math">\(2c\)</span> key comparisons:</p>
<pre><code>for i in floor(n / 2) - 1 ... 0:
    bubble_down(i)</code></pre>
<p>We could bubble down for <span class="math">\(n - 1\)</span> down to 0, since that visits every node, but since the ones at the end are all leaf nodes and bubbling down is a no-op, we just need to bubble down for all nodes from <span class="math">\(p(n - 1) = \floor{\frac{n - 2} 2} = \floor{\frac n 2} - 1\)</span> down to 0.</p>
<p>Proof:</p>
<blockquote>
<p>Without loss of generality, assume <span class="math">\(n = 2^t - 1\)</span> where <span class="math">\(t\)</span> is the number of levels - all levels are full.<br />Clearly, any node on a level <span class="math">\(l\)</span> needs at most <span class="math">\(2(t - l)\)</span> comparisons for its bubble down, since each level until the bottom needs 2 comparisons each and there <span class="math">\(t - l\)</span> levels between the current level and the bottom.<br />Clearly, there are <span class="math">\(2^{l - 1}\)</span> nodes on each level, which means that there are <span class="math">\(\sum_{l = 1}^{t + 1} 2^{l - 1} 2(t - l)\)</span> comparisons in total.<br />Clearly, <span class="math">\(\sum_{l = 1}^p 2^{l - 1} 2(p - l) = t\sum_{l = 1}^p 2^l - \sum_{l = 1}^p l2^l) \le 2^t - 2t - 2 \le 2^t - 1 = 2n\)</span> (note that <span class="math">\(\sum_{x = 1}^n x2^x = 2^{n + 1} n - 2^{n + 1} + 2\)</span>).<br />So there are <span class="math">\(2n\)</span> or fewer comparisons in total.</p>
</blockquote>
<p>In other words, we can build a heap in-place from an array in <span class="math">\(O(n)\)</span> time.</p>
<h1 id="section-5">22/1/15</h1>
<p>Another proof that in-place heapify is linear in time would be to use induction to show that the number of comparisons done for each node that is <span class="math">\(i\)</span> levels above the lowest is <span class="math">\(2i\)</span>, and that there are <span class="math">\(\frac{N + 1}{2^{i + 1}}\)</span> nodes in that level. Therefore, the total number of comparisons is <span class="math">\(\sum_{i = 1}^{h + 1} \frac{N + 1}{2^{i + 1}} 2i = (N + 1)\sum_{i = 1}^{h + 1} \frac{i}{2^i} \le (N + 1)\sum_{i = 1}^\infty \frac{i}{2^i} = 2(N + 1)\)</span>, by the series rules.</p>
<h2 id="selection-problem">Selection Problem</h2>
<p>Given an array <span class="math">\(A\)</span> of <span class="math">\(N\)</span> integers and an integer <span class="math">\(k \in \set{0, \ldots, N - 1}\)</span>, find the <span class="math">\(k\)</span>th smallest item in <span class="math">\(A\)</span>. If <span class="math">\(k = 0\)</span>, the result is the minimum. If <span class="math">\(k \approxeq \frac N 2\)</span>, the result is the median. If <span class="math">\(k = N - 1\)</span>, the result is the maximum.</p>
<p>We could solve this by sorting the array, which would be <span class="math">\(O(n \log n)\)</span>, or we could build a min-heap or max-heap and remove the appropriate number of items, both <span class="math">\(O(n \log n)\)</span>.</p>
<p>There is also a better solution called <strong>quickselect</strong>, based on the divide and conquer approach - we divide the problem into smaller subproblems, and then solve these subproblems. The basic idea is to recursively choose a pivot, partition the array into an array where every element is less than or equal to that pivot and an array greater than that pivot, then recurse on one of the smaller arrays to find the solution:</p>
<pre><code>def quick_select(A, k):
    if len(A) == 1: return A[0] # only one element, so must be the answer
    x = the pivot element # an element of the array that we choose to split on - how we choose it is up to us
    partition `A` into two sections (possibly in-place) `A[0:i - 1]` and `A[i + 1, n - 1]` such that `A[0:i - 1]` contains all elements that are less or equal to the pivot (excluding the pivot itself), and `A[i + 1, n - 1]` contains all elements that are greater, and `A[i]` is the pivot `x`
    if k == i: return x # the pivot element is the `k`th smallest element, so we found the answer
    if k &lt; i: return quick_select(A[0:i - 1], k) # the answer is in the left partition, so recurse on the smaller problem with the same index
    if k &gt; i: return quick_select(A[i + 1, n - 1], k - i - 1)</code></pre>
<p>Partitioning can be done in-place in <span class="math">\(O(n)\)</span> time worst-case:</p>
<pre><code>def partition(A, i):
    swap A[0] and A[i] # we want to get `x` out of the way first, so we put it in `A[0]`
    
    i, j = 1, len(A) - 1
    while True: # this is linear since `i` can only increase and `j` can only decrease, so we have at most one comparison with `A[0]` per element&gt;
        # find an element that belongs in the left partition, and an element that belongs in the right partition
        while i &lt; len(A) and A[i] &lt;= A[0]: i += 1 # go right until `A[i]` is greater than `x`
        while A[j] &gt; A[0]: j -= 1 # go left until `A[j]` is less than or equal to `x`
        
        if j &lt; i: # all of the left side is less or equal to `x` and all the right side is greater, so the partitioning is done
            # the place where the elements that are less or equal and the elements that are greater meet is the boundary between partitions
            swap A[0] and A[j] # put the pivot back into its place between the two partitions, since A[j] is less or equal to `x`
            break
        
        # at this point, `i` is greater than or equal to `j`, where `i` is the index of an element that is on the right but belongs on the left, and `j` represents the index of an element that is on the left but belongs on the right
        swap A[i] and A[j] # by swapping, we are moving each element into their correct partition</code></pre>
<p>The worst case runtime for quickselect is <span class="math">\(T(N) = \begin{cases} c_1 &amp;\text{if } N = 1 \\ T(N - 1) + c_2 N &amp;\text{if } N &gt; 1 \\ \end{cases} = c_2 N + c_2 (N - 1) + \ldots + c_2 2 + c_1 = O(N^2)\)</span> where <span class="math">\(c_1, c_2\)</span> are constants - in the worst case, quickselect takes quadratic time. However, in the best case it takes just <span class="math">\(O(n)\)</span>, where the pivot is the <span class="math">\(k\)</span>th smallest element.</p>
<p>The average case runtime is <span class="math">\(T_{avg}(N) = \frac{\sum_{\text{each input } i \text{ of size } N} T(i)}{\text{number of inputs with size } N}\)</span>. This is not feasible to calculate directly, but we can actually describe every input instance as a permutation <span class="math">\(P\)</span>, since we only care about the order of numbers, not the values themselves. We will analyze the worst case of <span class="math">\(k\)</span>, where we need the maximum amount of recursion, in order to not have to consider it in our calculations.</p>
<p>With these assumptions in place, we can now write it as <span class="math">\(T_{avg}(N) = \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements}} T(i)}{N!}\)</span>. We now split the sum into those where the pivot ended up being roughly in the middle of the array after partitioning, and those where the pivot did not: <span class="math">\(\frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \in [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!} + \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \notin [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!}\)</span>.</p>
<p>Note that when the pivot ends up being roughly in the middle, we are recursing on an input that is roughly half the size of the original input, which is the best case scenario for splitting input. When the pivot is not near the middle, in the worst case of <span class="math">\(k\)</span> the input we are recursing on could be almost as large as the original input.</p>
<p>Clearly, if <span class="math">\(i \in [\frac 1 4 N, \frac 3 4 N)\)</span>, which is <span class="math">\(\frac 1 2\)</span> of the time, then when we recurse in quickselect, our new input size is proportionally smaller, at most <span class="math">\(\frac 3 4\)</span> of the original size, so <span class="math">\(T(N) = T(\frac 3 4 N)\)</span>. If <span class="math">\(i \notin [\frac 1 4 N, \frac 3 4 N)\)</span>, which is also <span class="math">\(\frac 1 2\)</span> of the time, then the largest array we might recurse on could be roughly the same size as the original array, so <span class="math">\(T(N) = T(N - 1) + c_2 N\)</span>.</p>
<p>Therefore, on average <span class="math">\(T(N) = \begin{cases} c_1 &amp;\text{if } N = 1 \\ \frac 1 2 T(\frac 3 4 N) + \frac 1 2 T(N - 1) + c_2 N &amp;\text{if } N &gt; 1 \\ \end{cases} = O(n)\)</span> where <span class="math">\(c_1, c_2\)</span> are constants. This means that quickselect is, on average, a linear time operation, with a <span class="math">\(O(n^2)\)</span> worst case and a <span class="math">\(O(n)\)</span> best case.</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>