<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>CS240 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>
function highlight() { // highlight all code blocks using HighlightJS
  var code_blocks = document.getElementsByTagName("code");
  for (var i = 0; i < code_blocks.length; i++)
    hljs.highlightBlock(code_blocks[i]);
}
</script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/Résumé.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.mesecons.net/">mesecons</a></li>
    <span class="divider"></span>
    <li><a href="http://www.autohotkey.net/~Uberi/">autohotkey.net</a></li>
  </ul>
<h1 id="cs240">CS240</h1>
<p>Data Structures and Data Management.</p>
<pre><code>Therese Beidl
Section 002
Email: beidl@uwaterloo.ca
Web: http://www.student.cs.uwaterloo.ca/~cs240
ISA: Joseph (Danny) Sitter
ISA Email: cs240@studennt.cs.uwaterloo.ca</code></pre>
<p><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l&#39;H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}_P)}
\]</span></p>
<p>Section 1 and 2 are the regular classes, and section 3 is the enhanced section and has very different content.</p>
<p>Midterm at Thurs. Feb. 26 at 4:30 PM, worth 25%. Final exam worth 50%. Must pass weighted average of exam to pass the course. 5 assignments, each worth 5%.</p>
<h1 id="section">6/1/15</h1>
<p>Assignments is due on Tuesdays. Assignment 0 is due on Jan. 13.</p>
<p>Suppose we have a lot of data to keep track of. We could store it in an array/list, but depending on the type of the data, this might not be the best choice. Data structures should make it easy and efficient to perform the operations we need. For example, an English dictionary probably needs to be efficiently searched, but we don't really need to care about insertion and deletion since they're so rare.</p>
<p>The best data structure for something depends on the type of data we want to store. Our goal is to have a short running time and little memory.</p>
<p>In this course we will be performing theoretical analysis, developing ideas and pseudocode (and sometimes, implementations), and analyzing them using tools like big-O notation.</p>
<p>An Abstract Data Type is the idea of describing something by what it can do, not how it does it.</p>
<p>Required background includes arrays, linked lists, strings, stacks, queues, ADTs, recursive algorithms, trees, sorting algorithms (insertion, selection, quick, merge), binary search/BSTs, arithmetic series, geometric series, harmonic series (<span class="math">\(\frac 1 1 + \ldots + \frac 1 n = \ln n\)</span>).</p>
<p>In this course, <span class="math">\(\log n\)</span> is implicitly <span class="math">\(\log_2 n\)</span> - all logarithms are base 2 unless otherwise stated.</p>
<p>A <strong>problem</strong> is a description of a general situation and the desired outcome. For example, the sorting problem is &quot;given comparable values, put them in sorted order&quot;.</p>
<p>A <strong>problem instance</strong> is a particular input to a problem, like a particular array of numbers to sort for the sorting problem.</p>
<p>A <strong>problem solution</strong> is a change/process that, given the situation of a problem instance, results in the desired outcome. For example, the sorted array of numbers for the sorting problem.</p>
<p>We <strong>solve</strong> a problem by giving the correct algorithm for it. A solution is <strong>correct</strong> if it finds a solution for every possible input that can be given.</p>
<p>An <strong>algorithm</strong> is a finite description of a process that gives an answer (a solution that is not necessarily correct) for all possible instances of a problem.</p>
<p><strong>Efficiency</strong> usually refers to an algorithm's runtime, and sometimes its memory usage. It may also refer to things specific to the problem domain, like the number of comparisons done.</p>
<p>To solve a problem:</p>
<ol style="list-style-type: decimal">
<li>Design an algorithm.</li>
<li>Write down the main idea of the algorithm in plain prose. For example, &quot;for increasing <span class="math">\(i\)</span>, make <span class="math">\(A[0 \ldots i]\)</span> sorted by inserting <span class="math">\(A[i]\)</span> into a sorted <span class="math">\(A[0 \ldots i - 1]\)</span>&quot; for the sorting problem.</li>
<li><p>Optionally, write pseudocode - code that might not be a real language, and is something like a mix between prose and code. This is a more precise way of specifying an algorithm. Any consistent, precise, and clearly understandable language will be accepted as pseudocode.</p>
<pre><code>Preconditions: an array `A[0 ... n - 1]`$.
Postconditions: `A` is sorted
for i in 1 ... n - 1:
  value = A[i]
  j = i - 1
  while j &gt;= 0 and A[j] &gt; key:
    A[j + 1] = A[j]
    j = j - 1
  A[j + 1] = value</code></pre></li>
<li>Argue/prove that it is correct.</li>
<li>We can use formal correctness proofs, but this is often excessive to convince someone an algorithm is correct. Instead, we can simply give lots of invariants and prove that the algorithm terminates.</li>
<li>For example, for the above, we would say that for the inner loop, &quot;<code>A[j + 1 ... i]</code> contains items that are bigger than <code>value</code>, and is in sorted order&quot; is an invariant.</li>
<li>It is very important to prove that it terminates, especially recursive algorithms. We simply use the standard methods specified in CS245, but we can usually just write that each call or iteration gets some smaller input, and that small enough inputs cause the algorithm to terminate.</li>
<li>Analyze how good the algorithm is, in terms of efficiency and sometimes lower bounds.</li>
<li>For this we use time complexity/running-time analysis, memory usage analysis, and so on.</li>
<li>For example, the above code is known as Insertion Sort, which we already know to have a worst case time complexity of <span class="math">\(O(n^2)\)</span> and a best case time complexity of <span class="math">\(O(n)\)</span>.</li>
<li>Recall that formally, an algorithm being in <span class="math">\(O(f(n))\)</span> means that there exists a <span class="math">\(k &gt; 0\)</span> and <span class="math">\(m \ge 0\)</span> such that for all <span class="math">\(n &gt; m\)</span>, <span class="math">\(k \cdot f(n) &gt; T(n)\)</span> where <span class="math">\(T(n)\)</span> represents the number of constant time steps or amount of time that the algorithm needs to give an answer for an input of size <span class="math">\(n\)</span> - the running time function. This is written as <span class="math">\(T(n) \in O(f(n))\)</span>.</li>
<li><p>Repeat above steps until satisfactory algorithm is found.</p></li>
</ol>
<p>Only after this point would we implement/experimentally test the algorithm.</p>
<h1 id="section-1">8/1/15</h1>
<p>A <strong>timing function</strong> is a function <span class="math">\(S_p: \mathrm{Input} \to \mb{R}^+\)</span> where <span class="math">\(p\)</span> is the program we are running. For example, we can compare quicksort and mergesort by comparing <span class="math">\(S_{\text{mergesort}}(\tup{5, 6, 2, 3})\)</span> and <span class="math">\(S_{\text{quicksort}}(\tup{5, 6, 2, 3})\)</span>.</p>
<p>A timing function measures the amount of time a program takes for a given input.</p>
<p>Since computers are constantly changing and improving, comparing timing functions is not very meaningful since they will not stay consistent over differnt machines and over time as computers change. We want a machine-agnostic way to measure the amount of time a given program takes.</p>
<p>For example, Intel's core instruction set is RISC, with tons of extensions such as MMX, SSE, and FPU and GPU stuff. These add instructions such as <code>fsqrt</code> and similar. We assume that every instruction takes a finite, constant amount of time (interesting note).</p>
<p>We can instead measure the number of elementary operations, but it is difficult to define what an elementary operation actually is, and determining the exact number of elementary operations is a lot of work. For example, we might measure the number of clock cycles needed to run a program, but this would not make sense across things like RISC vs CISC machines. For example, if an architecture doesn't have a constant time division operation, we would probably need to implement one in software that would not have constant time.</p>
<p>In other words, we write our algorithms in pseudo-code, then count the number of primitive operations.</p>
<p>We now want to plot our timing function with respect to the size of the input <span class="math">\(\abs{\mathrm{Input}} \subseteq \mb{N}\)</span>. This allows us to figure out the behaviour of the function as the size of the input gets bigger. This is difficult since finding the timing function for arbitrary inputs is often difficult. Additionally, we don't necessarily care about the constants in the function output, just its behaviour on large inputs.</p>
<p>Usually we care about the worst case behaviour, to determine the worst possible behaviour in all cases. We may also be interested in the average case to see how things will perform in practice. Occasionally, we may also be interested in the best case - for example, in cryptography, we want to make sure the problem cannot be solved in less than some given amount of time.</p>
<p>The worst case behaviour is <span class="math">\(T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}\)</span>. The best case behaviour is <span class="math">\(T_p(n) = \max \set{S_p(I) \middle| \abs{I} = n}\)</span>. The average case behaviour is <span class="math">\(T_p(n) = \frac{\sum_{e \in  R} i}{\abs{R}}\)</span> where <span class="math">\(R = \set{S_p(I) \middle| \abs{I} = n}\)</span>. We can now plot these functions to get the time bounds on our program.</p>
<p>The big-O notation is used to make life easier for computer scientists trying to calculate timing functions and to make comparing functions much easier. Refer to the CS136 notes for background. The <span class="math">\(O\)</span> in <span class="math">\(O(f(x))\)</span> is actually a capital omicron, bu nowadays the O is more common.</p>
<p>Comparing Big O is analogous to ordering in <span class="math">\(\mb{R}\)</span>. Just like how <span class="math">\(x \le y\)</span>, we might write <span class="math">\(f(x) \in O(g(x))\)</span>. What we care about is the trends of the function, not the actual numbers.</p>
<p>Big-O gives an upper bound behaviour on a function (analogous to <span class="math">\(x \le y\)</span>). Big-<span class="math">\(\Omega\)</span> gives a lower bound (analogous to <span class="math">\(x \ge y\)</span>). Big-<span class="math">\(\Theta\)</span> us the exact bounds - when the Big-<span class="math">\(O\)</span> is the same as the Big-<span class="math">\(\Omega\)</span> (analogous to <span class="math">\(x = y\)</span>).</p>
<p>There is also Little-<span class="math">\(o\)</span>, which is a non-inclusive upper bound (analogous to x &lt; y), and Little-<span class="math">\(\omega\)</span>, which is a non-inclusive lower bound (analogous to <span class="math">\(x &gt; y\)</span>). For little-o, instead of <span class="math">\(\exists c &gt; 0, \exists n_0 &gt; 0, n &gt; n_0 \implies f(n) \le cg(x)\)</span>, we have <span class="math">\(\forall c &gt; 0, \exists n_0 &gt; 0, n &gt; n_0 \implies f(n) &lt; cg(x)\)</span>.</p>
<p>There are also incomparable functions, like one that goes up and down arbitrarily. As a result, functions can only be partially ordered.</p>
<h1 id="section-2">13/1/15</h1>
<p>Big-O does not care about the values of the functions - only about how it grow as input values get large.</p>
<p>Prove that <span class="math">\(2n^2 + 3n + 11 \in O(n^2)\)</span>:</p>
<blockquote>
<p>We want to prove that <span class="math">\(2n^2 + 3n + 11 \le cn^2\)</span> for all <span class="math">\(n &gt; n_0\)</span>.<br />Let <span class="math">\(n_0 = 10\)</span> and <span class="math">\(n \ge n_0\)</span>. Clearly <span class="math">\(2n^2 + 3n + 11 \le 2n^2 + 3n^2 + 11n^2\)</span> when <span class="math">\(n \ge 1\)</span> and <span class="math">\(2n^2 + 3n^2 + 11n^2 = 16n^2\)</span>.<br />Then 16 is a possible value for <span class="math">\(c\)</span> and <span class="math">\(2n^2 + 3n + 11 \in O(n^2)\)</span>.</p>
</blockquote>
<p><span class="math">\(O(n^2 + \log n)\)</span> is not correct since it is not fully simplified.</p>
<p>Common time complexities include <span class="math">\(\Theta(1)\)</span> (constant), <span class="math">\(\Theta(\log n)\)</span> (logarithmic), <span class="math">\(\Theta(n)\)</span> (linear), <span class="math">\(\Theta(n \log n)\)</span> (pseudo-linear), <span class="math">\(\Theta(n^2)\)</span> (quadratic), <span class="math">\(\Theta(n^3)\)</span> (cubic), <span class="math">\(\Theta(n^k)\)</span> (polynomial), <span class="math">\(\Theta(2^n)\)</span> (exponential). In the real world, everything above <span class="math">\(O(n \log n)\)</span> tends to be rather bad performance.</p>
<p>It is not always the case that an algorithm with better time complexity than another is always better in all circumstances. For example, despite insertion sort being <span class="math">\(O(n^2)\)</span> and merge sort being <span class="math">\(O(n \log n)\)</span>, we often use insertion sort when sorting smaller lists since it is faster in practice when the input is small. Many practical sorting algorithms therefore drop down into insertion sort when the input is small. This is also why most people use insertion sort when sorting things in real life - it is very fast for real-world inputs, which are usually small.</p>
<p>Let <span class="math">\(L = \lim_{n \to \infty} \frac{f(n)}{g(n)}\)</span>. If <span class="math">\(L = 0\)</span>, <span class="math">\(f(n) \in o(g(n))\)</span>. If <span class="math">\(0 &lt; L &lt; \infty\)</span>, <span class="math">\(f(n) \in \Theta(g(n))\)</span>. Otherwise, <span class="math">\(L = \infty\)</span> and <span class="math">\(f(n) \in \omega(g(n))\)</span>. This is a useful way to prove orders for functions that would otherwise be difficult to prove by first principles.</p>
<p>For example, if we want to prove that <span class="math">\(n^2 \log n \in o(n^3)\)</span>, we can do <span class="math">\(\lim_{n \to \infty} \frac{n^2 \log n}{n^3} = \lim_{n \to \infty} \frac{\log n}{n} \lH \lim_{n \to \infty} \frac{\frac 1 n}{1} = 0\)</span>, and use the fact that <span class="math">\(L = 0\)</span> to conclude that <span class="math">\(n^2 \log n \in o(n^3)\)</span>.</p>
<p>We can aso use this to prove that <span class="math">\((\log n)^a \in o(n^b)\)</span> for any <span class="math">\(a, b &gt; 0\)</span>.</p>
<p>Also, <span class="math">\(f(n) \in \Omega(g(n)) \iff g(n) \in \Omega(f(n))\)</span>, <span class="math">\(f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))\)</span>, <span class="math">\(f(n) \in o(g(n)) \iff g(n) \in \omega(f(n))\)</span>, <span class="math">\(f(n) \in \Theta(g(n)) \iff g(n) \in O(f(n)) \wedge g(n) \in \Omega(f(n))\)</span>, <span class="math">\(f(n) \in o(g(n)) \implies f(n) \in O(g(n))\)</span>, <span class="math">\(f(n) \in o(g(n)) \implies f(n) \notin \Omega(g(n))\)</span>, <span class="math">\(f(n) \in \omega(g(n)) \implies f(n) \in \Omega(g(n))\)</span>, and <span class="math">\(f(n) \in \omega(g(n)) \implies f(n) \notin O(g(n))\)</span>. We can use these to simplify order notation expressions.</p>
<p>Also, <span class="math">\(O(f(n) + g(n)) = O(\max(f(n), g(n))) = \max(O(f(n)), O(g(n)))\)</span>. Also, orders are transitive - <span class="math">\(f(n \in O(g(n))) \wedge g(n) \in O(h(n)) \implies f(n) \in O(h(n))\)</span>.</p>
<p>In our pseudocode, we often use <span class="math">\(x \leftarrow 5\)</span> or <code>x := 5</code> to denote assignment, since <code>=</code> is ambiguous - it could potentially mean equality or assignment.</p>
<p>Math with orders works a lot like normal math: <span class="math">\(\Theta(1) \cdot (1 + \ldots + n) = \Theta(1) \cdot \frac{n^2 + n}{2} = \Theta(n^2 + n) = \Theta(n^2)\)</span>.</p>
<h1 id="section-3">15/1/15</h1>
<p>A priority queue is a data structure that stores items with keys that represent the priorities that the values have. It has two operations - insertion with a given key and deletion of the item with the maximum priority.</p>
<p>There are a number of ways we could implement a priority queue - an unsorted dynamic array (amortized <span class="math">\(O(1)\)</span> insert, <span class="math">\(O(n)\)</span> deletion), a sorted dynamic array, and something known as a heap. We don't want to use sorted order, since that makes insertion slow. We also dont't want to use unsorted order, since that makes deletion slow.</p>
<p>A <strong>heap</strong> is a data structure that allows us to implement a priority queue in a very efficient way. The main idea is that we would like to know very easily what the maximum element is, so we make all the other elements follow from it.</p>
<p>The basic idea behind the heap is that it is a tree where each element keeps track of candidates that can replace them when they're deleted, as children. The maximum item would therefore be the root.</p>
<p>Most commonly, we use binary heaps, which is a binary tree. Recall that this is a data structure consisting of nodes and links between node, such that each node has 2 or fewer children, and 1 or fewer parents.</p>
<p>Formally, a <strong>binary max-heap</strong> is a binary tree that satisfies the structural property and the max-heap property. There are no rules about the order of children, unlike a binary search tree.</p>
<p>The <strong>structural property</strong> is that all levels except the lowest level must be completely filled. Each node must have two children, except the lowest level, where nodes can have fewer than two children but <strong>must fill up from the left</strong> - in the lowest level, the left sibling of a node must have 2 children before we can insert a child into it.</p>
<p>The <strong>max-heap property</strong> is that for any node <span class="math">\(x\)</span> in the heap, the key of <span class="math">\(x\)</span> is less than or equal to the key of the parent of <span class="math">\(x\)</span>. As a result, the maximum node of a heap is always the root node. There is also a min-heap property where the key of <span class="math">\(x\)</span> is greater than or equal to the key of the parent of <span class="math">\(x\)</span>, used for min-heaps. In other words, the parent of any node in a heap must be bigger or equal to that node.</p>
<p>Insertion into a heap is easy because we can simply do a standard tree insertion, with certain constraints to ensure it satisfies the structural property and the heap property. This operation is <span class="math">\(\Theta(\log n)\)</span> because the structural property ensures that the height of the tree is <span class="math">\(\Theta(\log n)\)</span>.</p>
<p>Also, the height of a binary heap is <span class="math">\(\ceil{\log_2 n}\)</span>. The height of a tree is 1 less than the number of levels it has (1 less than the maximum depth). Levels start at 1 and increase with depth.</p>
<p>Proof:</p>
<blockquote>
<p>Let <span class="math">\(x\)</span> be a heap with <span class="math">\(n\)</span> nodes and height <span class="math">\(h\)</span>.<br />Clearly, any binary tree of height <span class="math">\(h\)</span> must have <span class="math">\(n \le 2^{h + 1} - 1\)</span>, since each level <span class="math">\(i\)</span> has less than or equal to <span class="math">\(2^{i - 1}\)</span> nodes, and the total is <span class="math">\(n \le \sum_{i = 1}^{h + 1} 2^{i - 1} \le \frac{2^{h + 1} - 1}{2 - 1}\)</span>.<br />Since <span class="math">\(n + 1 \le 2^{h + 1}\)</span>, <span class="math">\(h \ge \log(n + 1) - 1\)</span>.<br />Clearly, <span class="math">\(n \ge 2^h\)</span>, by the structural property, since the lowest level is always <span class="math">\(h + 1\)</span> so each level <span class="math">\(i\)</span> before it has exactly <span class="math">\(2^{i - 1}\)</span> nodes, and the total for all levels except the lowest is <span class="math">\(2^h - 1\)</span>. Since the lowest level has 1 or more nodes, the total is <span class="math">\(n \ge (2^h - 1) + 1\)</span>, or <span class="math">\(2^h\)</span>.</p>
</blockquote>
<h1 id="section-4">20/1/15</h1>
<p>Binary heaps are trees, so we can store them as a tree structure with references and nodes. However, a more efficient way to store binary trees is as an array:</p>
<ul>
<li><span class="math">\(A\)</span> is an array representing the heap.</li>
<li><span class="math">\(A[0]\)</span> contains the root node.</li>
<li><span class="math">\(A[1], \ldots, A[2]\)</span> store the children of the root node.</li>
<li><span class="math">\(A[3], \ldots, A[6]\)</span> store the children of the children of the root nodes, and so on.</li>
</ul>
<p>Essentially, we first have the first level, then the second level, and so on. Formally, the left and right children of a node stored in the array at index <span class="math">\(i\)</span> are <span class="math">\(lc(i) = 2i + 1\)</span> and <span class="math">\(rc(i) = 2i + 2\)</span> (in a 0-indexed array). Likewise, the parent of any node is <span class="math">\(p(i) = \floor{\frac{i - 1} 2}\)</span>. This works because the leftmost node in level <span class="math">\(l\)</span> has index <span class="math">\(\sum_{k = 1}^{l - 1} 2^{k - 1}\)</span>.</p>
<p>In fact, this left and right parent system works for any binary tree, and even for <span class="math">\(n\)</span>-ary trees with a simple extension. However, it is not space efficient unless the tree obeys something like the structural property. The structural property ensures that the array is mostly filled, possibly except for part that stores the last level.</p>
<h3 id="heaps-as-priority-queues">Heaps as Priority Queues</h3>
<p>We will represent our heap as an array <span class="math">\(A\)</span> containing <span class="math">\(n\)</span> values, automatically expanded or shrunk as needed.</p>
<p>To <strong>insert</strong> into a heap, we first find the only location that the structural property allows us to insert a new node <span class="math">\(i\)</span> into. This could possibly violate the heap property since it is possible that <span class="math">\(i &gt; p(i)\)</span>, so if <span class="math">\(i &gt; p(i)\)</span>, we swap <span class="math">\(i\)</span> and <span class="math">\(p(i)\)</span> to get <span class="math">\(p(i)&#39;\)</span> and <span class="math">\(i&#39;\)</span>, where <span class="math">\(p(p(i)&#39;) = i&#39;\)</span>.</p>
<p>Now the heap property is satisfied for <span class="math">\(i&#39;\)</span> and <span class="math">\(p(i)&#39;\)</span>, and it isn't possible for <span class="math">\(p(i)&#39;\)</span>'s sibling to be greater than <span class="math">\(i&#39;\)</span> since it was less than the original parent <span class="math">\(p(i)\)</span>. However, it is possible that <span class="math">\(i&#39;\)</span> is now greater than it's new parent <span class="math">\(p(i&#39;)\)</span>, so we swap again if <span class="math">\(i&#39; &gt; p(i)&#39;\)</span>.</p>
<p>We repeat swapping the originally inserted node with its parents until either it is less than its parent, or it is the new root node. This is called &quot;bubbling up until it holds&quot;.</p>
<p>In pseudocode:</p>
<pre><code>def insert(key):
    # wip: we resize the array here if necessary
    
    A[n] = key # $n$ is the index of the next free space in the heap - we keep track of this in the heap structure
    j = n
    n += 1 # set $n$ to the next empty space in the array
    while j &gt; 0 and A[p(j)] &lt; A[j]:
        swap A[j] and A[p(j)]
        j = p(j)</code></pre>
<p>Clearly, this takes <span class="math">\(O(\log n)\)</span> time worst case since every run of the loop runs on a lower height on the tree. The bubbling up process would potentially need to visit every level in the tree if inserting an element greater than the root element of the heap.</p>
<p>To <strong>delete the maximum</strong> of a heap, our goal is to remove the root element of the heap. When we do this, we now have a blank space at the root node of the heap. We fix this by moving the last node in the heap (the rightmost node on the lowest level) into the position the root originally occupied. This allows us to satisfy the structural property again.</p>
<p>However, now the heap property is not satisfied. We fix this now by swapping the root node (originally the last node) <span class="math">\(i\)</span> with the bigger of its children <span class="math">\(lc(i)\)</span> and <span class="math">\(rc(i)\)</span>, which we will represent as <span class="math">\(c(i)\)</span>, to get <span class="math">\(c(i)&#39;\)</span> and <span class="math">\(i&#39;\)</span> such that <span class="math">\(p(i&#39;) = c(i)&#39;\)</span>. Now we have satisfied the heap property between <span class="math">\(i&#39;\)</span> and <span class="math">\(c(i)&#39;\)</span> and between <span class="math">\(i&#39;\)</span>'s sibling and <span class="math">\(c(i)&#39;\)</span>, but it is possible that <span class="math">\(i&#39;\)</span>'s children could be greater than it.</p>
<p>Therefore, we repeatedly swap the originally last node with its larger child until it is greater (or equal to) than both of its children, or it is a leaf node.</p>
<p>In pseudocode:</p>
<pre><code>def delete_max():
    max = A[0]
    n -- # 1 less than the next free space is the last node in the heap, which we will consider as a free space after we move it to the root
    # if memory management is needed, free `A[0]` here
    A[0] = A[n]
    
    # the following is the bubble_down operation, which bubbles values down until they satisfy the heap property
    j = 0
    while j &lt;= n - 1:
        max_index = j
        if lc(j) exists and A[lc(j)] &gt; A[j]: # existance means that lc(j) &lt;= n - 1
            max_index = lc(j)
        if rc(j) exists and A[rc(j)] &gt; A[max_index]: # existance means that rc(j) &lt;= n - 1
            max_index = rc(j)
        if max_index == k: break # the heap property is already satisfied, so stop bubbling
        swap A[j] and A[max_index] # swap the current node with its child, which is larger than it
        j = max_index
    
    # if necessary, shrink the array here
    return max</code></pre>
<p>Clearly, this takes <span class="math">\(O(h)\)</span> time worst case where <span class="math">\(h\)</span> is the height of the heap underneath the starting element, the root, which is <span class="math">\(O(\log n)\)</span> since <span class="math">\(h\)</span> is roughly <span class="math">\(\log n\)</span>. The worst case is when the last node of the heap is also the smallest one.</p>
<p><strong>Priority queue sort</strong> (PQSort) is a sorting algorithm where we insert all our elements into a priority queue and then take them out again in order. PQSort needs <span class="math">\(n\)</span> insertions and <span class="math">\(n\)</span> deletions of maximums. Since each operation is <span class="math">\(O(\log n)\)</span>, PQSort is <span class="math">\(O(n \log n)\)</span>, like mergesort. When the priority queue is implemented with a heap, we also call it <strong>heapsort</strong>.</p>
<p>Heapsort is pretty nice because we can sort in-place (by building a heap directly) and without a lot of recursion, but it can have poor real-world behaviour because the worst case heap insertion occurs when we are inserting nodes one by one that are already in ascending order, and already-sorted data occurs very often in real-world applications.</p>
<p>Also, heapsort is not stable. If a sorting algorithm is <strong>stable</strong>, then if different nodes have equal keys, they appear in the same order in the output as they did in the input.</p>
<p>Also, we often measure the runtime of sorting algorithms in terms of the number of key comparisons made (<span class="math">\(A[i] &lt; A[j]\)</span>). This is because we often allow users to specify their own key computing functions, which could potentially be aribtrarily exxpensive. Everything else is simply proportional or a constant on top of the number of key comparisons, in terms of runtime.</p>
<p>Bubbling downward is basically the operation of repeatedly swapping an element with its largest child until it is greater or equal to both. Bubbling up is basically repeatedly swapping an element with its parent until it is less than or equal to its parent.</p>
<p>We may also want to <strong>build heaps</strong> - given an array <span class="math">\(D\)</span> with <span class="math">\(n\)</span> values, we want to build a head from that. This is relatively trivial in theory - simply go through the array and insert it into a heap. Clearly, this is <span class="math">\(O(n \log n)\)</span> since insertion is <span class="math">\(O(\log n)\)</span> and we need <span class="math">\(n\)</span> of them.</p>
<p>Clearly, this takes roughly <span class="math">\(\sum_{k = 0}^{n - 1} \log(k + 1)\)</span> time since <span class="math">\(k\)</span> starts off at 0 and grows by 1 upon every insertion. Clearly, there are at least <span class="math">\(\frac n 2\)</span> terms in that sum where <span class="math">\(k + 1 \ge \frac n 2\)</span>, so <span class="math">\(\sum_{k = 0}^{c - 1} \log(k + 1) \ge \frac n 2 \log \frac n 2\)</span>, so the operation is <span class="math">\(\Omega(n \log n)\)</span>, and by the above, <span class="math">\(\Theta(n \log n)\)</span>.</p>
<p>A better option is to build the heap in place, to save memory. Clearly, an array always satisfies the structural property since there's no way not to, so all we have to do is enforce the heap property in the array.</p>
<p>We will enforce the heap property by bubbling downward on every level of the node starting from the last node that has children (the last node on the second-to-last level). Our invariant will be that for all <span class="math">\(k &gt; i\)</span>, the subheap rooted at <span class="math">\(k\)</span> satisfies the heap property (we can say this because we bubble down from right to left). We will show that this takes less than or equal to <span class="math">\(2c\)</span> key comparisons:</p>
<pre><code>for i in floor(n / 2) - 1 ... 0:
    bubble_down(i)</code></pre>
<p>We could bubble down for <span class="math">\(n - 1\)</span> down to 0, since that visits every node, but since the ones at the end are all leaf nodes and bubbling down is a no-op, we just need to bubble down for all nodes from <span class="math">\(p(n - 1) = \floor{\frac{n - 2} 2} = \floor{\frac n 2} - 1\)</span> down to 0.</p>
<p>Proof:</p>
<blockquote>
<p>Without loss of generality, assume <span class="math">\(n = 2^t - 1\)</span> where <span class="math">\(t\)</span> is the number of levels - all levels are full.<br />Clearly, any node on a level <span class="math">\(l\)</span> needs at most <span class="math">\(2(t - l)\)</span> comparisons for its bubble down, since each level until the bottom needs 2 comparisons each and there <span class="math">\(t - l\)</span> levels between the current level and the bottom.<br />Clearly, there are <span class="math">\(2^{l - 1}\)</span> nodes on each level, which means that there are <span class="math">\(\sum_{l = 1}^{t + 1} 2^{l - 1} 2(t - l)\)</span> comparisons in total.<br />Clearly, <span class="math">\(\sum_{l = 1}^p 2^{l - 1} 2(p - l) = t\sum_{l = 1}^p 2^l - \sum_{l = 1}^p l2^l) \le 2^t - 2t - 2 \le 2^t - 1 = 2n\)</span> (note that <span class="math">\(\sum_{x = 1}^n x2^x = 2^{n + 1} n - 2^{n + 1} + 2\)</span>). ;wip: wat So there are <span class="math">\(2n\)</span> or fewer comparisons in total.</p>
</blockquote>
<p>In other words, we can build a heap in-place from an array in <span class="math">\(O(n)\)</span> time.</p>
<p>Also, if we prove that the running time of a function is of a certain order at infinity, then it is proven for all <span class="math">\(n\)</span>.</p>
<h1 id="section-5">22/1/15</h1>
<p>Another proof that in-place heapify is linear in time would be to use induction to show that the number of comparisons done for each node that is <span class="math">\(i\)</span> levels above the lowest is <span class="math">\(2i\)</span>, and that there are <span class="math">\(\frac{N + 1}{2^{i + 1}}\)</span> nodes in that level. Therefore, the total number of comparisons is <span class="math">\(\sum_{i = 1}^{h + 1} \frac{N + 1}{2^{i + 1}} 2i = (N + 1)\sum_{i = 1}^{h + 1} \frac{i}{2^i} \le (N + 1)\sum_{i = 1}^\infty \frac{i}{2^i} = 2(N + 1)\)</span>, by the series rules.</p>
<h2 id="selection-problem">Selection Problem</h2>
<p>Given an array <span class="math">\(A\)</span> of <span class="math">\(N\)</span> integers and an integer <span class="math">\(k \in \set{0, \ldots, N - 1}\)</span>, find the <span class="math">\(k\)</span>th smallest item in <span class="math">\(A\)</span>. If <span class="math">\(k = 0\)</span>, the result is the minimum. If <span class="math">\(k \approxeq \frac N 2\)</span>, the result is the median. If <span class="math">\(k = N - 1\)</span>, the result is the maximum.</p>
<p>We could solve this by sorting the array, which would be <span class="math">\(O(n \log n)\)</span>, or we could build a min-heap or max-heap and remove the appropriate number of items, both <span class="math">\(O(n \log n)\)</span>.</p>
<p>There is also a better solution called <strong>quickselect</strong>, based on the divide and conquer approach - we divide the problem into smaller subproblems, and then solve these subproblems. The basic idea is to recursively choose a pivot, partition the array into an array where every element is less than or equal to that pivot and an array greater than that pivot, then recurse on one of the smaller arrays to find the solution:</p>
<pre><code>def quick_select(A, k):
    if len(A) == 1: return A[0] # only one element, so must be the answer
    x = the pivot element # an element of the array that we choose to split on - how we choose it is up to us
    partition `A` into two sections (possibly in-place) `A[0:i - 1]` and `A[i + 1, n - 1]` such that `A[0:i - 1]` contains all elements that are less or equal to the pivot (excluding the pivot itself), and `A[i + 1, n - 1]` contains all elements that are greater, and `A[i]` is the pivot `x`
    if k == i: return x # the pivot element is the `k`th smallest element, so we found the answer
    if k &lt; i: return quick_select(A[0:i - 1], k) # the answer is in the left partition, so recurse on the smaller problem with the same index
    if k &gt; i: return quick_select(A[i + 1, n - 1], k - i - 1)</code></pre>
<p>Partitioning can be done in-place in <span class="math">\(O(n)\)</span> time worst-case:</p>
<pre><code>def partition(A, i):
    swap A[0] and A[i] # we want to get `x` out of the way first, so we put it in `A[0]`
    
    i, j = 1, len(A) - 1
    while True: # this is linear since `i` can only increase and `j` can only decrease, so we have at most one comparison with `A[0]` per element&gt;
        # find an element that belongs in the left partition, and an element that belongs in the right partition
        while i &lt; len(A) and A[i] &lt;= A[0]: i += 1 # go right until `A[i]` is greater than `x`
        while A[j] &gt; A[0]: j -= 1 # go left until `A[j]` is less than or equal to `x`
        
        if j &lt; i: # all of the left side is less or equal to `x` and all the right side is greater, so the partitioning is done
            # the place where the elements that are less or equal and the elements that are greater meet is the boundary between partitions
            swap A[0] and A[j] # put the pivot back into its place between the two partitions, since A[j] is less or equal to `x`
            break
        
        # at this point, `i` is greater than or equal to `j`, where `i` is the index of an element that is on the right but belongs on the left, and `j` represents the index of an element that is on the left but belongs on the right
        swap A[i] and A[j] # by swapping, we are moving each element into their correct partition</code></pre>
<p>The worst case runtime for quickselect is <span class="math">\(T(N) = \begin{cases} c_1 &amp;\text{if } N = 1 \\ T(N - 1) + c_2 N &amp;\text{if } N &gt; 1 \\ \end{cases} = c_2 N + c_2 (N - 1) + \ldots + c_2 2 + c_1 = O(N^2)\)</span> where <span class="math">\(c_1, c_2\)</span> are constants - in the worst case, quickselect takes quadratic time. However, in the best case it takes just <span class="math">\(O(n)\)</span>, where the pivot is the <span class="math">\(k\)</span>th smallest element.</p>
<p>The average case runtime is <span class="math">\(T_{avg}(N) = \frac{\sum_{\text{each input } i \text{ of size } N} T(i)}{\text{number of inputs with size } N}\)</span>. This is not feasible to calculate directly, but we can actually describe every input instance as a permutation <span class="math">\(P\)</span>, since we only care about the order of numbers, not the values themselves. We will analyze the worst case of <span class="math">\(k\)</span>, where we need the maximum amount of recursion, in order to not have to consider it in our calculations.</p>
<p>With these assumptions in place, we can now write it as <span class="math">\(T_{avg}(N) = \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements}} T(i)}{N!}\)</span>. We now split the sum into those where the pivot ended up being roughly in the middle of the array after partitioning, and those where the pivot did not: <span class="math">\(\frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \in [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!} + \frac{\sum_{\text{each permutation } i \text{ of } N \text{ elements where } i \notin [\frac 1 4 N, \frac 3 4 N)} T(i)}{N!}\)</span>.</p>
<p>Note that when the pivot ends up being roughly in the middle, we are recursing on an input that is roughly half the size of the original input, which is the best case scenario for splitting input. When the pivot is not near the middle, in the worst case of <span class="math">\(k\)</span> the input we are recursing on could be almost as large as the original input.</p>
<p>Clearly, if <span class="math">\(i \in [\frac 1 4 N, \frac 3 4 N)\)</span>, which is <span class="math">\(\frac 1 2\)</span> of the time, then when we recurse in quickselect, our new input size is proportionally smaller, at most <span class="math">\(\frac 3 4\)</span> of the original size, so <span class="math">\(T(N) = T(\frac 3 4 N)\)</span>. If <span class="math">\(i \notin [\frac 1 4 N, \frac 3 4 N)\)</span>, which is also <span class="math">\(\frac 1 2\)</span> of the time, then the largest array we might recurse on could be roughly the same size as the original array, so <span class="math">\(T(N) = T(N - 1) + c_2 N\)</span>.</p>
<p>Therefore, on average <span class="math">\(T(N) = \begin{cases} c_1 &amp;\text{if } N = 1 \\ \frac 1 2 T(\frac 3 4 N) + \frac 1 2 T(N - 1) + c_2 N &amp;\text{if } N &gt; 1 \\ \end{cases} = O(n)\)</span> where <span class="math">\(c_1, c_2\)</span> are constants. This means that quickselect is, on average, a linear time operation, with a <span class="math">\(O(n^2)\)</span> worst case and a <span class="math">\(O(n)\)</span> best case.</p>
<p>Also, exponential runtime includes all bases, but each base is distinct: <span class="math">\(O(4^n)\)</span> is not the same as <span class="math">\(O(2^n)\)</span>.</p>
<h1 id="section-6">27/1/15</h1>
<h2 id="quicksort">Quicksort</h2>
<p>Quicksort sorts by partitioning the input into two based on a pivot, then sorting each partition recursively. This can all be done in place:</p>
<pre><code>def quicksort(A, left, right):
    # postcondition: `A[left]`, ..., `A[right]` is sorted
    if right &gt;= left: return # the array is already sorted since it has 1 or fewer elements
    pivot = find_pivot(A, left, right)
    i = partition(A, left, right, pivot) # `i` is the new position of the original `pivot` element
    quicksort(A, left, i - 1)
    quicksort(A, i + 1, right)</code></pre>
<p>Quicksort is similar to quickselect, but we sort both sides when we recurse. In practice, quicksort is very fast, though in theory heapsort and mergesort are better.</p>
<p>Let <span class="math">\(T(n)\)</span> be the runtime for sorting <span class="math">\(n\)</span> elements, which is <span class="math">\(right - left + 1\)</span>. Clearly, <span class="math">\(T(n) = \begin{cases} O(1) &amp;\text{if } n \le 1 \\ T(\text{size of left partition}) + T(\text{size of right partition}) + O(n) &amp;\text{if } n &gt; 1 \\ \end{cases}\)</span>.</p>
<p>We can also write this more formally:</p>
<blockquote>
<p>Let <span class="math">\(c_1, n_0\)</span> exist such that everything in the function except the recursive calls takes <span class="math">\(c_1 n_0\)</span> time or less for all <span class="math">\(n \ge n_0\)</span>. Let <span class="math">\(c_2 = \max(T(0), \ldots, T(n_0))\)</span>. Let <span class="math">\(c = \max(c_1, c_2)\)</span>.<br />Then <span class="math">\(T(n) \le c_2 \le c \le cn \le T(\text{size of left partition}) + T(\text{size of right partition}) + cn\)</span> if <span class="math">\(2 \le n \le n_0\)</span>. So <span class="math">\(T(n) \le \begin{cases} c &amp;\text{if } n \le 1 \\ T(\text{size of left partition}) + T(\text{size of right partition}) + cn &amp;\text{if } n &gt; 1 \\ \end{cases}\)</span>.</p>
</blockquote>
<p>In the worst case the size of one of the partitions will be <span class="math">\(n - 1\)</span>, when the pivot is the largest or smallest element each time - sorted input. So <span class="math">\(T(n) = T(1) + T(n - 1) + O(1) = O(n^2)\)</span>.</p>
<p>In the best case the size of each partition will be roughly equal, when the pivot is one of the median elements. So <span class="math">\(T(n) = 2T\left(\frac n 2\right) + O(n) = O(n \log n)\)</span>.</p>
<p>The average case proof is a bit more difficult:</p>
<blockquote>
<p>Clearly, the average is simply take the average of all possible pivot combinations: <span class="math">\(T_{avg}(n) = \frac{\sum_{i = 0}^{n - 1} \left(T_{avg}(i) + T_{avg}(n - i + 1) + cn\right)}{n} = \frac 1 n \sum_{i = 0}^{n - 1} T_{avg}(i) + \frac 1 n \sum_{i = 0}^{n - 1} T_{avg}(n - i + 1) + cn = \frac 2 n \sum_{i = 0}^{n - 1} T_{avg}(i) + cn\)</span>.<br />We claim that given <span class="math">\(D = \max\left(T(0), T(1), \frac{T(2){2 \ln 2}}, 18c\right)\)</span>, <span class="math">\(T_{avg}(n) \le D \max(1, n \ln n)\)</span> for all <span class="math">\(n\)</span>. This can be proven uing induction over <span class="math">\(n\)</span>, and <span class="math">\(T_{avg}(n) \le D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 n D + cn\)</span>.<br />Clearly, <span class="math">\(D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 n (T(0) + T(1)) + cn \le D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 3 (D + D) + cn\)</span> because <span class="math">\(n \ge 3\)</span>.<br />So <span class="math">\(D\frac 2 n \sum_{i = 2}^{n - 1} i \ln i + \frac 2 3 (D + D) + cn \le \frac 1 2 n^2 \ln n - \frac 1 4 n^2 \le \frac{2D} n \frac 1 2 n^2 \ln n - \frac{2D} n \frac 1 4 n^2 + cn \le D n \ln n - \frac D {18} n + cn = O(n \log n)\)</span>.<br />Basically, <span class="math">\(\frac 2 n \sum_{i = 0}^{n - 1} T(i) + cn \in O(n \log n)\)</span>.</p>
</blockquote>
<p>Quicksort and quickselect are bad in the worst case but good in the average case. In real life, how good it is will depend on what kind of input we run it on. The average case assumes that the every input is equally likely to occur, but it is not necessarily true. For example, sorting almost sorted arrays is a very common use case in real life, which, given our pivot selection technique, would often give the worst case behaviour.</p>
<p>Instead, we can force the instances to become equally likely, to get something like average case behaviour. We do this by using randomization - eliminating the bias caused by real world input by adding a random factor to the algorithm.</p>
<p>For quicksort, we can do this by selecting a random pivot instead of just always taking the first element in the array as the pivot. This makes all elements equally likely, so the probability of choosing a bad pivot is always <span class="math">\(\frac 1 n\)</span>, regardless of what the input is. The average case is now <span class="math">\(O(n \log n)\)</span> regardless of what kind of input we have.</p>
<h1 id="section-7">29/1/15</h1>
<p>We will prove that all comparison sorting algorithms are <span class="math">\(\Omega(n \log n)\)</span>. However, there are other sorting algorithms that can also run in <span class="math">\(O(n)\)</span> time, which we will also look at.</p>
<p>A comparison sorting algorithm is one that is based purely on key comparisons - we can only compare two keys to see if one is greater, less, or equal to another, and no other operations are available.</p>
<p>We don't really have tools for analyzing arbitrary algorithms. However, for comparison-based algorithms we have <strong>decision trees</strong>:</p>
<p>Consider an example, sorting three elements <span class="math">\(A[0..2]\)</span>:</p>
<pre><code>                                     A[0] \le A[1]
                                    / false       \ true
                       A[1] \le A[2]               same as left side, but with A[0] and A[1] exchanged
                      / false       \ true
A[0] \le A[1] \le A[2]               A[0] \le A[2]
                                    / false       \ true
              A[0] \le A[2] \le A[1]               A[0] \le A[2] \le A[1]</code></pre>
<p>Decision trees are binary trees that represent every possible decision path in the input, where each leaf is an output and each nde represents a decision. This decision tree represents the absolute minimum number of comparisons that we have to use in order to correctly sort three values</p>
<p>The worst case number of comparisons is the length of the longest path from the root to the leaf - the height of the tree. The length of a path is the number of edges in it.</p>
<p>Using these, we can now construct a proof:</p>
<blockquote>
<p>No matter what comparison sorting algorithm we use to sort, it is based on comparisons and is therefore representable using decision trees.<br />When we sort <span class="math">\(n\)</span> items, there are <span class="math">\(n!\)</span> possible outputs - every permutation of the input is a possible output.<br />Clearly, any binary tree of height <span class="math">\(h\)</span> must have at most <span class="math">\(2^h\)</span> leaves, so any binary tree with <span class="math">\(l\)</span> leaves must have height at least <span class="math">\(\log_2 l\)</span>.<br />So the height <span class="math">\(h\)</span> of the decision tree is at least <span class="math">\(\log_2 n!\)</span>.<br />Clearly, <span class="math">\(h \ge \log_2 n! = \log_2 (n(n - 1) \cdots 1) \ge \log_2 n + \log_2 (n - 1) + \ldots + \log_2 \frac n 2 \ge \frac n 2 \log_2 \frac n 2 = \frac n 2 \log_2 n + \frac n 2 \log_2 \frac 1 n = \frac n 2 \log_2 n - \frac n 2\)</span>.<br />So <span class="math">\(h \in \Omega(n \log n)\)</span>.</p>
</blockquote>
<p>However, non-comparison sorting algorithms can be faster. <strong>Bucket sort</strong> is how most people will sort things by keys like names or grades. Basically, we have one pile for each possible value, make a pile for each value, and put them together:</p>
<pre><code>let $0 \le A[i] \le R - 1$ be an integer ($R$ is the radix)
create an array $R$ of empty lists $L[0], \ldots, L[R - 1]$
for element in A:
    append `element` t `L[element]`
concatenate all the lists $L[0], \ldots, L[R - 1]$ together into another list `L`
the sorted result is now `L`</code></pre>
<p>This is a stable sort. Since we iterate through the array, and the lists take a constant time to create, bucket sort is <span class="math">\(O(n + R)\)</span> in the worst case.</p>
<p>This is not a comparison sorting algorithm because it uses more information than just comparisons - it also relies on assumptions about the keys all being in the finite set of possible values.</p>
<p><strong>Key-index/count sort</strong> is similar to bucket sort (and is also stable), but doesn't need to use lists, which are something we try to avoid. The idea is that we know where the elements of <span class="math">\(L[1]\)</span> will be in the final result based on the number of elements in <span class="math">\(L[0]\)</span>, where the elements of <span class="math">\(L[2]\)</span> based on where <span class="math">\(L[1]\)</span> is and the number of elements in it, and so on:</p>
<pre><code># compute length of each list
list_lengths = [0] * R
for element in A:
    list_lengths[element] += 1

# compute position of each array in the result
list_positions = [0] * R
for i in range(1, R):
    list_positions[i] = list_positions[i - 1] + list_lengths[i - 1]

# put each value into the correct position
initialize `result` as a list with the same length as `A`
for element in A:
    result[list_positions[element]] = element
    list_positions[element] += 1</code></pre>
<p>This can also be done in place, and we can combine <code>list_length</code> into <code>list_positions</code>. This is still <span class="math">\(O(n + R)\)</span> since we have to initalize the array of size <code>R</code> to 0.</p>
<p>In practice, we wouldn't want to use these sorts for any real-world inputs. This is because although it is <span class="math">\(O(n)\)</span>, it is often still slower than a good comparison sort for real-world number values.</p>
<p>These sorts also work for occasions when we have too many buckets to be practical, like names. We could first sort by the first letter/digit of the name/grade, then recursively sort each of the buckets based on the second letter/digit, and so on. This technique is also called <strong>MSD-radix-sort</strong>. When <span class="math">\(R = 2\)</span>, this is very similar to quicksort.</p>
<p><strong>LSD-radix-sort</strong> is a bit better, since we don't have to keep track of all the indices by which we're sorting recursively:</p>
<pre><code>for d from the length of the largest value down to 1:
    sort `A` by the `d`th significant digit with a stable sorting algorithm</code></pre>
<p>The time complexity for this is simply <span class="math">\(O(m(n + R))\)</span> where <span class="math">\(m\)</span> is the largest possible number of digits. Typically, the number of digits or letters is bounded, so the depth of recursion is bounded by a constant and the worst case time complexity is <span class="math">\(O(n + R)\)</span>.</p>
<h1 id="section-8">3/2/15</h1>
<p>An empty tree has height -1. A tree with one node has height 0. The height of a tree is 1 more than the larger of the heights of the two subtrees.</p>
<p>A <strong>dictionary</strong> is an abtract datatype that associates keys to values. Essentially, it stores key-value pairs, and supports insertion of a key value pair, searching for a key-value pair by key, and deleting a key-value pair by key.</p>
<p>We assume that all the keys are unique and can be compared in constant time, and each pair takes up constant space.</p>
<p>A naive way to implement dictionaries is to use unsorted arrays/lists (<span class="math">\(O(1)\)</span> insert, <span class="math">\(O(n)\)</span> search, <span class="math">\(O(n)\)</span> delete) or sorted arrays/lists (<span class="math">\(O(n)\)</span> insert, <span class="math">\(O(\log n)\)</span> search, <span class="math">\(O(n)\)</span> delete).</p>
<p>A <strong>binary search tree</strong> is a binary tree where for every node, every key in the left subtree is smaller than it and the every key in the right subtree are larger. We don't have to care about the duplicate keys since we assume keys are unique.</p>
<p>Implementing dictionaries using binary search trees is possible by constructing the BST where the value of each pair is its key (<span class="math">\(O(h)\)</span> insertion, <span class="math">\(O(h)\)</span> search, <span class="math">\(O(h)\)</span> deletion). It is always true that <span class="math">\(h \in \Omega(\log n)\)</span> and <span class="math">\(h \in O(n)\)</span>.</p>
<p>We want to make sure <span class="math">\(h\)</span> is <span class="math">\(O(\log n)\)</span>, by <strong>rebalancing</strong> the binary search tree as we perform operations on it. We do this by imposing additional conditions on the tree, and then showing that we can do all operations while maintaining these conditions and taking <span class="math">\(O(h)\)</span> time or less, and that these conditions ensure that <span class="math">\(h \in O(\log n)\)</span>.</p>
<p>There are many possible sets of conditions that we can use to make sure the binary tree is always balanced. One of these are those used in <strong>AVL-trees</strong>, which imposes just one condition: for any node, the height of the left and right subtree differ by at most 1 - the structural condition.</p>
<p>AVL trees were one of the first ever self-balancing binary trees, and although they work, they are rather slow in practice.</p>
<p>When we store a node in an AVL tree, we will also <strong>store the height of the tree formed by that node and its children in that node</strong>. This allows us to efficiently do the calculations we need later for insertions and deletions.</p>
<p>Searching in an AVL tree is exactly the same as in a normal BST. Since we don't modify the tree, we also can't violate the strutural condition.</p>
<p>For insertion, we need to insert the key-value pair at its proper spot, then check if the tree is still balanced and rebalance if not. Inserting into the correct subtree is the same as in a normal BST, but we also need to update the tree heights we are storing with the nodes - the newly inseted node gets height 0, and we recalculate the height of its parent. If the height changed, we recurse and recalculate the height of its parent, and so on.</p>
<p>While we are recalculating heights, we also check if the heights of the children of the ancestor differ by at most 1. If they differ by more, then <strong>we need to rebalance on that node</strong> - to make sure it is both a BST and tree heights don't differ by more than 1. We basically have 4 possible rebalancing operations, and it has been proven that at least one of them will correctly rebalance the tree.</p>
<p>The first is <strong>right rotation on node Z</strong>. Given a BST with nodes in the form <span class="math">\(Z(Y(X(A, B), C), D)\)</span>, we can rearrange it as <span class="math">\(Y(X(A, B), Z(C, D))\)</span> while still maintaining the BST properties, since <span class="math">\(A &lt; X &lt; B &lt; Y &lt; C &lt; Z &lt; D\)</span>. Basically, we rotate <span class="math">\(Y\)</span> and <span class="math">\(Z\)</span> clockwise/right, changing only those elements so the entire operation is constant time.</p>
<p>The second is <strong>left rotation on node Z</strong>, which is similar but we are rotating in the opposite direction. Given a BST with nodes in the form <span class="math">\(Z(A, Y(B, X(C, D)))\)</span>, we an rearrange it as <span class="math">\(Y(Z(A, B), X(C, D))\)</span> while still maintaining the BST properties.</p>
<p>The third is <strong>double right rotation on node Z</strong>. Given a BST with nodes in the form <span class="math">\(Z(Y(A, X(B, C)), D)\)</span>, we can rearrange it as <span class="math">\(X(Y(A, B), Z(C, D))\)</span>. Basically, we are bringing <span class="math">\(X\)</span> up to replace <span class="math">\(Z\)</span>. This is called a double right rotation because it can be implemented as left rotate on <span class="math">\(Y\)</span> and right rotate on <span class="math">\(Z\)</span>.</p>
<p>The fourth is the <strong>double left rotation on node Z</strong>. Given a BST with nodes in the form <span class="math">\(Z(A, Y(X(B, C), D))\)</span>, we can rearrange it as <span class="math">\(X(Z(A, B), Y(C, D))\)</span>.</p>
<p>Basically, we try each operation in turn, and check if each one is balanced. If so, then we can stop trying the others.</p>
<p>;wip: download the modules from the website</p>
<h1 id="section-9">5/2/15</h1>
<p>Operations on AVL trees can now be defined:</p>
<pre><code>def insert(k, v):
    insert `k` and `v` into the tree as a new leaf, without rebalancing
    let `node` be the new node that was inserted
    while `node` is not the root node:
        p, s = parent(node), sibling(node)
        if abs(s.height - node.height) &gt;= 2: # node unbalanced, rebalance it
            rebalance(node)
            node = p
        else: # node balanced, update the height
            new_height = max(node.height, s.height) + 1
            if (new_height == p.height): break # the height has stopped changing and we can stop rebalancing
            p.height = new_height

def delete(k):
    delete the node `k`, without rebalancing
    let `node` be the node that was deleted
    while `node` is not the root node:
        p, s = parent(node), sibling(node)
        if abs(s.height - node.height) &gt;= 2: # node unbalanced, rebalance it
            rebalance(node)
            node = p
        else: # node balanced, update the height
            new_height = max(node.height, s.height) + 1
            if new_height == p.height: break # the height has stopped changing and we can stop rebalancing
            p.height = new_height

def rebalance(node):
    let `y` be the larger child of `node`
    let `x` be the larger child of `y` # this child must exist since the height differs by more than 1, so `node` must have grandchildren
    perform the rebalancing operation according to the four rebalancing cases, based on whether `y` and `x` are left and right children
    update the heights of `node`, `y`, and `x`</code></pre>
<p>The height <span class="math">\(h\)</span> of an AVL-tree is always at most <span class="math">\(O(\log n)\)</span>. Proof:</p>
<blockquote>
<p>We want to prove that that <span class="math">\(h \le \log_c n = O(\log n)\)</span> for a constant <span class="math">\(c\)</span>.<br />We want to find <span class="math">\(c\)</span> and <span class="math">\(n\)</span> for <span class="math">\(n \ge c^h\)</span> - given a fixed height, we want to find the smalles number of nodes we can have.<br />Clearly, for height 0, there must be at least 1 node. Clearly, for height 1 there must be at least 2 nodes. Clearly, for height 2 there must be at least 4 nodes. Clearly, for height 3 there must be at least 7 nodes.<br />By induction, we can show that the minimum number of <span class="math">\(n\)</span>, represented <span class="math">\(n(h)\)</span> must be <span class="math">\(n(0) = 1, n(1) = 2, n(h) = 1 + n(h - 1) + n(h - 2)\)</span>, since each minimal tree must have the smaller minimal trees as its children, and we want to use one as large as possible while the other is as small as possible.<br />Interesingly, the values of <span class="math">\(n(h)\)</span> is simply 1 less than each Fibonacci number. The Fibonacci numbers grow exponentially, so <span class="math">\(n(h)\)</span> does too.<br />Clearly, <span class="math">\(n(h) \ge \sqrt{2}^h\)</span>, since <span class="math">\(n(h) = n(h - 1) n(h - 2) + 1 \ge n(h - 2) + n(h - 2) = 2n(h - 2) \ge 2^2 n - 4 \ge \ldots \ge 2^i n(h - 2i), i &gt; 1\)</span>, so <span class="math">\(n(h) \ge 2^{\ceil{\frac h 2}}\)</span>.<br />So any AVL-tree of height <span class="math">\(h\)</span> has <span class="math">\(\sqrt{s}^h\)</span> or more nodes, so <span class="math">\(h \le \log_{\sqrt 2} n = O(\log n)\)</span>.</p>
</blockquote>
<p>Clearly, the worst case time for insertion/deletion/search is <span class="math">\(O(h)\)</span>, and since <span class="math">\(h \in O(\log n)\)</span> and each rebalancing operation takes constant time, insertion/deletion/search is always <span class="math">\(O(\log n)\)</span>.</p>
<h3 id="trees">2-3-trees</h3>
<p>A 2-3-tree is similar to AVL-trees in concept, but we no longer have to be a binary tree - a node may have 2 or 3 children. The 2-3-tree could be said to be a generalization of the AVL-tree.</p>
<p>Each node in a 2-3 tree is either a node with one key-value pair and two children, or a node with two key-value pairs and three children (the left child is less than key 1, the middle child is between key 1 and 2, and the right child is greater than key 2, and key 2 must be greater than key 1). Additionally, all empty subtrees must be on the same level.</p>
<p>Searching is done by a generalized version of the standard BST search - when we are comparing, we simply need to check how many key-value pairs the node has and check the cases accordingly.</p>
<p>Insertion into a 2-3 tree is done by searching for the node on the lowest level that would have had the key as a child if it was inserted directly. If it has only one key-value pair, then add our key-value pair to it to make it a node with two key-value pairs. Otherwise, we add the key-value pair to it to make a node with three key-value pairs (an overflow), then split it into three nodes with one key-value pair each and move the middle one (the one that is neither largest or smallest) up to the parent, so the node becomes two nodes and the parent gets an extra key-value pair. If the parent now has three nodes when we do this, we recursively perform this splitting and moving up operation on the parent, all the way up until no nodes have three key-value pairs.</p>
<p>Deletion in a 2-3 tree is done by searching for the key-value pair that we want to delete. We then swap the key-value pair with its successor/predecessor (the descendant key-value pairs that are closest to the current node's value), like with deletion in a normal BST. If the successor/predecessor's node has 2 keys, then we just delete the successor/predecessor to get a node with a single key-value pair. Otherwise, we have an underflow. If possible, we <strong>transfer</strong> by moving a key-value pair from the parent down into the leaf, and move a key-value pair from a sibling up into the parent to complete the deletion. Otherwise, we <strong>fuse</strong> by moving a key-value pair from the parent down into a sibling. If we do this, it might be necessary to recurse and fuse on the parent, and so on.</p>
<h1 id="section-10">10/2/15</h1>
<p>Insertion, deletion, and searching in a 2-3 tree are all <span class="math">\(O(h)\)</span> where <span class="math">\(h\)</span> is the height of the tree. The height of a 2-3 tree is always <span class="math">\(\Theta(\log n)\)</span>. Proof:</p>
<blockquote>
<p>Let <span class="math">\(n\)</span> be the number of key-value pairs in the tree. We want to find the smallest possible <span class="math">\(n\)</span> for any given <span class="math">\(h\)</span>, in order to get an upper bound on <span class="math">\(h\)</span> given <span class="math">\(n\)</span>.<br />Clearly, the first level has at least 1 key-value pair and at least 2 children. Clearly, the second level has at least 2 key-value pairs and at least 4 children. By induction, it can be proved that we have at least <span class="math">\(n \ge 2^{i - 1}\)</span> key-value pairs on each level <span class="math">\(i\)</span>, with at least <span class="math">\(2^i\)</span> children.<br />So the total number of key-value pairs is <span class="math">\(n \ge 2^{h + 1} - 1\)</span>, and <span class="math">\(h \le \log_2(n + 1) - 1\)</span>. So <span class="math">\(h \in O(\log n)\)</span>.<br />Clearly, the first level has at most 1 key-value pair and at most 3 children. Clearly, the second level has at most 6 key-value pairs and at most 9 children. By induction, it can be proved that we have at most <span class="math">\(n \le 2 \cdot 3^{i - 1}\)</span> key-value pairs on each level <span class="math">\(i\)</span> (2 key-value pairs per node, <span class="math">\(3^{i - 1}\)</span> nodes per level), with at most <span class="math">\(3^i\)</span> children.<br />So the total number of key-value pairs is <span class="math">\(n \le 3^{h + 1} - 1\)</span>, and <span class="math">\(h \ge \log_3(n + 1) - 1\)</span>. So <span class="math">\(h \in \Omega(\log n)\)</span>.<br />So <span class="math">\(h \in \Theta(\log n)\)</span>.</p>
</blockquote>
<p>We can also generalize 2-3 trees into <span class="math">\(a\)</span>-<span class="math">\(b\)</span> trees. These are trees where the nodes can have between <span class="math">\(a\)</span> and <span class="math">\(b\)</span> children, except the root node, which can have between 2 and <span class="math">\(b\)</span> children. The number of children for each node is 1 more than the number of key-value pairs it has. Children are subtrees that are between two neighboring keys. All empty subtrees must be on the same level.</p>
<p>The height of an <span class="math">\(a\)</span>-<span class="math">\(b\)</span> tree is <span class="math">\(h \in \Omega(\log_b(n)), h \in O(\log_a(n))\)</span>. Searching takes <span class="math">\(O(\log_2 b \log_a n)\)</span>.</p>
<h2 id="external-memory">External Memory</h2>
<p>It is quite common that we have more data than we can fit into the main memory, and we store it in external memory. Accessing external memory is generally far slower.</p>
<p>We now use a new computation model in which we only <strong>count how many times we access external memory</strong> - the number of <strong>disk transfers</strong>. We will also often consider sequential access to be faster than random access access.</p>
<p>A <strong>B-tree</strong> of order <span class="math">\(M\)</span> is a <span class="math">\(\ceil{\frac M 2}\)</span>-<span class="math">\(M\)</span> tree - a tree with between <span class="math">\(\ceil{\frac M 2}\)</span> and <span class="math">\(M\)</span> children per node (except the root). We choose an <span class="math">\(M\)</span> so that one node of the B-tree will fit into main memory. Clearly, <span class="math">\(h \in O(\log_{\frac M 2} n)\)</span>. Storing our data as a B-tree is an excellent way to minimise disk transfers, since searching takes <span class="math">\(h\)</span> or fewer disk transfers, and insertion/deletion takes <span class="math">\(2h\)</span> or fewer.</p>
<h1 id="section-11">12/2/15</h1>
<p>As a result, we can use balanced binary trees to implement a dictionary with <span class="math">\(\Theta(\log n)\)</span> runtime for all supported operations.</p>
<p>Any comparison-based implementation of searching for a key <span class="math">\(k\)</span> is <span class="math">\(\Omega(\log n)\)</span>. We can prove this as follows:</p>
<blockquote>
<p>Clearly, we can only use comparison between <span class="math">\(k\)</span> and existing keys <span class="math">\(a_0, \ldots, a_n\)</span>.<br />Clearly, in the decision tree for the algorithm there must be at least one leaf per key, since any key might be looked up. Clearly, there must also be a leaf for the case where <span class="math">\(k\)</span> is not in the dictionary.<br />So the number of leaves is at least <span class="math">\(n + 1\)</span>. So the height of the decision tree must be <span class="math">\(h \ge \log(n + 1)\)</span>.<br />Clearly, the height of the decision tree determines the worst-case number of comparisons that need to be made.<br />So the worst case comparisons for the algorithm is <span class="math">\(\Omega(\log n)\)</span>.</p>
</blockquote>
<p>So logarithmic time for searching is the best we can do if we just have comparisons. However, if we exploit other properties of keys, we can do better.</p>
<p>If we assume keys of our dictionary are integers, and that they have a small lower and upper bound <span class="math">\(l \le k &lt; h\)</span>, then we can use <strong>key-indexed search</strong>. Basically, we create an array of size <span class="math">\(h - l\)</span> and map every key <span class="math">\(k\)</span> to the index <span class="math">\(k - l\)</span> in the array. This means we have <span class="math">\(\Theta(1)\)</span> time for all operations. However, it can be very wasteful of space for sparse dictionaries, and the assumption doesn't occur very often in practice.</p>
<h2 id="hash-tables">Hash Tables</h2>
<p>What we could potentially do is to map keys to the indices of a much smaller table - hashing the keys. We keep an unordered array <span class="math">\(T\)</span> (a <strong>hash table</strong>), where each entry is known as a <strong>bucket</strong> or <strong>slot</strong>. <span class="math">\(T\)</span> has size <span class="math">\(M\)</span>, which we can choose ourselves.</p>
<p>We now want to map our keys to the indices <span class="math">\(0, \ldots, M - 1\)</span> - a <strong>hash function</strong> <span class="math">\(h(k): \text{keys} \to \set{0, \ldots, M - 1}\)</span>, that can be computed in <span class="math">\(O(1)\)</span>. So now any key <span class="math">\(k\)</span> is stored at <span class="math">\(T[h(k)]\)</span>. An example of a hashing function over integer keys is <span class="math">\(h(k) = k \pmod{M}\)</span>.</p>
<p>However, it is possible for multiple keys to hash to the same value (a <strong>collision</strong>). There are multiple ways to resolve this. One way is to use a linked list or something similar at each slot, to store all the colliding key-value pairs. Unfortunately, this means that in the worst case, when all the keys collide, we have <span class="math">\(O(n)\)</span> search and deletion, or <span class="math">\(O(\log n)\)</span> if we used balanced binary trees at each slot instead of linked lists.</p>
<p>Our goal is therefore to minimise collisions on average. An ideal hash function must therefore be <strong>uniform</strong> (all slots are equally likely to be chosen for any key). We will assume that we have such a function, although in practice they are very hard to do correctly.</p>
<p>The <strong>load factor</strong> is <span class="math">\(\alpha = \frac{n}{M}\)</span> - the ratio of keys to slots. We control <span class="math">\(\alpha\)</span> because we control <span class="math">\(M\)</span>. Essentially, we can resize the array wheen we need to to reduce the load factor. We generally want to keep the load factor small to avoid collisions, but not so small that we waste too much memory.</p>
<p>Alternatively, we can find an different slot for <span class="math">\(k\)</span> in the same array rather than using a linked list for each slot (this is known as <strong>open addressing</strong>). This is more efficient because we avoid the overhead of linked lists, but is still has the poor <span class="math">\(O(n)\)</span> worst case behaviour for all operations.</p>
<p>Basically, we now have a sequence of slots <span class="math">\(h(k, 0), h(k, 1), \ldots, h(k, M - 1)\)</span> using our slightly modified hash function (this is a <strong>probe sequence</strong>), and we search for an empty slot in the sequence <span class="math">\(T[h(k, 0)], \ldots, T[h(k, n)]\)</span> when we want to insert or delete.</p>
<p>When we delete we need to make sure we fill in the gap in the sequence that opened up when we deleted a key-value pair, or else we will break the sequence. What we could do is after deletion, insert a sentinel value that represents a deleted element into that slot. This sentinel value is interpreted as an element by search and delete (but not insert), and is not considered when deciding whether to shrink the array (that means we might need to keep track of two load factors).</p>
<p>A common way to define the probe sequence is <strong>linear probing</strong>, when <span class="math">\(h(k, i) = h(k) + i \pmod{M}\)</span>. Ideally, a probe sequence is a permutation of <span class="math">\(0, \ldots, M - 1\)</span> - we want to ensure that we try all slots and don't try each one more than once.</p>
<p>For open addressing, we must ensure that the load factor is less than 1 (it can't be 1 since otherwise it will break search). It is generally a good idea to increase <span class="math">\(M\)</span> when the load factor gets up to 0.5 or so.</p>
<p>Linear hashing is inefficient because it ends up making big runs of non-empty slots - it clusters a lot of values together, and any key that hashes innto the cluster increases its size, which means that searching will often encounter long runs that it will need to go through. A good probe sequence should hop around the array more to avoid further collisions. <strong>Quadratic probing</strong> does this a little better using <span class="math">\(h(k, i) = h(k) + c_1 i + c_2 i^2\)</span>, but it is hard to choose the constants such that it makes a permutation. Another technique is to use double hashing, where <span class="math">\(h(k, i) = h_1(k) + i h_2(k)\)</span> and <span class="math">\(h_2(k) \ne 0\)</span>, but getting a hash function that is a permutation is difficult.</p>
<h1 id="section-12">24/2/15</h1>
<p><strong>Cuckoo hashing</strong> is more or less the best hashing strategy around. It uses a very different insertion strategy: we definitely insert a key at slot <span class="math">\(h(k)\)</span>. If that slot was occupied prior to insertion, we kick the old one out and insert the new one.</p>
<p>This strategy has two independent hash functions, <span class="math">\(h_1(k)\)</span> and <span class="math">\(h_2(k)\)</span>:</p>
<pre><code>def insert(key, value):
    original_index = h_1(key) # the index of the new key-value pair
    index = original_index
    while T[index] is not None: # slot is not already empty, kick out old value
        old_key, old_value = T[index]
        T[index] = (key, value) # store the current key-value pair, displacing the original key-value pair in there
        index = h_1(old_key) if index == h_2(old_key) else h_2(old_key) # switch the index to the alternate slot for the newly displaced pair
        key, value = old_key, old_value # repeat the whole kicking out and inserting process for the newly displaced key-value pair
    T[original_index] = (key, value) # insert the new key-value pair in case it didn&#39;t kick any other pairs out earlier</code></pre>
<p>It is possible for insertion to go into a cycle, such as if the hash functions were equal. To avoid an infinite loop, we check if the length of the displacement sequence (the number of times the while loop runs) and check if it equals the table size. If this is the case, we rebuild the table with a larger size, and new hashing functions.</p>
<p>Insertion into a hash table using cuckoo hashing has an expected runtime of <span class="math">\(O\left(\frac \alpha {(1 - 2\alpha)^2}\right)\)</span>, where <span class="math">\(\alpha\)</span> is the load factor and assuming the hashing functions are uniformly distributed. So when the load factor starts going above 0.5, the expected runtime goes to infinity. We should always rebuild the table before this.</p>
<p>For all the hashing strategies we've seen so far, insertion, deletion, and search has an expected runtime of <span class="math">\(O(1)\)</span>, as long as we keep the load factor below a constant (1 for open addressing, 0.5 for cuckoo hashing).</p>
<p>;wip: midterm covers everything up to here, includes identities sheet, questions like run algorithm on some input, compare algorithms by things like memory or time, develop new algorithms, proving runtimes.</p>
<h3 id="rebuilding-tableshashing">Rebuilding tables/Hashing</h3>
<p>We want to occasionally rebuild the table when the load factor gets too big or too small, to avoid collisions and save memory. Basically, we monitor <span class="math">\(\alpha\)</span> during insertions and deletions. If <span class="math">\(\alpha\)</span> gets too big, we make a new, larger array (usually about double the size of the previous one, or less), then insert all the elements of the old one into the new one, and creating new hash functions. If <span class="math">\(\alpha\)</span> gets too small, we do the same thing, but with a smaller array (usually about half the size of the previous table).</p>
<p>If we do both the growing and shrinking operations, we can ensure that the hash table also uses <span class="math">\(\Theta(n)\)</span> space.</p>
<p>Obviously, this is very slow, but it only happens very rarely. These rebuilding operations are amortized so that insertion and deletion are <span class="math">\(O(1)\)</span> on average.</p>
<p>In practice, open addressing is the most commonly used collision resolution strategy. We also want to use cheaply computable functions, unlike some theoretical ones that are very expensive to compute. A hash function should not ignore any part of the key, and should mix up patterns in data.</p>
<p>A common hash function for integer keys is <span class="math">\(h(k) = k \pmod M, M \in \mb{N}\)</span>. This function is the division method. However, this works very badly if the data is human-readable and <span class="math">\(M\)</span> is a power of 10 (since many real world numbers end with 0), and if <span class="math">\(M\)</span> is a power of 2, since powers of two are very commonly used in computer arithmetic. Ideally, <span class="math">\(M\)</span> would be a prime number. A common value is <span class="math">\(2^i - 1\)</span>, but this also is bad for certain patterns of numbers.</p>
<p>Another common hash function for integer keys is <span class="math">\(h(k) = \floor{(Ak \mod 1) \times M}, A \in \mb{R}\)</span>. This function is the <strong>multiplication method</strong>. For this, we want to choose an <span class="math">\(A\)</span> that destroys patterns in the input, though good values are hard (<span class="math">\(\phi = \frac{\sqrt{5} - 1} 2\)</span> is a good value in practice). This technique is good because the table size doesn't affect the quality of hashing.</p>
<p>Keys are not always integers. To hash strings, we often use the ascii values, and then use each of them in a certain base. For example, &quot;APPLE&quot; becomes <span class="math">\(65, 80, 80, 76, 69\)</span>, and we use this as the digits in a certain radix <span class="math">\(R\)</span>, to get <span class="math">\(h(k) = (65R^4 + 80R^3 + 80R^2 + 76R^1 + 69R^0) \mod M\)</span>. Since the value before the modulo is so large, it is hard to compute. We can do the equivalent function <span class="math">\(h(k) = ((((((((65 \mod M)R + 80) \mod M)R + 80) \mod M)R + 76) \mod M)R + 69) \mod M\)</span> instead.</p>
<h1 id="section-13">26/2/15</h1>
<p>In external memory, a B-tree uses <span class="math">\(O(n \log_M n)\)</span> disk transfers. Ideally, we want to use hashing in external memory much like in internal memory to speed up our accesses.</p>
<p>External memory is organized in <strong>pages</strong>, which are chunks that each fit in internal memory. We can easily read pages of external memory at a time.</p>
<p>We could use open addressing, but this would result in a lot of disk trasnfers. We could use hashing with chaining/closed addressing, but potentially if there are long chains one chunk would not fit in internal memory and we would need to store it in external memory again, which wastes space.</p>
<h2 id="directories">Directories</h2>
<p>There is a good way to do hashing with external memory, a data structure known as a <strong>directory</strong>, using a technique called <strong>extendible hashing</strong>.</p>
<p>Let <span class="math">\(h(k) \in \set{0, \ldots, 2^L - 1}\)</span> - all hash values are binary strings of size <span class="math">\(L\)</span>. The directory contains an array, stored in internal memory, of size <span class="math">\(2^d\)</span> where <span class="math">\(d \le L\)</span> (<span class="math">\(d\)</span> is called the <strong>order</strong> of the directory).</p>
<p>Each slot in the array points to the a block in external memory, a chunk of fixed size. Each slot of the array has an index that can be represented as a binary string of length <span class="math">\(d\)</span>. The idea is that every key-value pair with a hash value that begins with binary string <span class="math">\(x_1 \ldots x_d\)</span> is stored at the index represented by <span class="math">\(x_1 \ldots x_d\)</span>. For example, a key-value pair with hash <span class="math">\(110000\)</span> in a directory of order 2 is stored at index <span class="math">\(11\)</span>, or 3 in decimal.</p>
<p>Each block stores an integer <span class="math">\(k_d \le d\)</span>, the <strong>local depth</strong>, which represents how many bits of the hashes of the keys will be the same, starting from the beginning. A block also stores key-value pairs such that the first <span class="math">\(k_d\)</span> bits of the hashes of each key are the same. The local depth also shows how many references there are to the block, as <span class="math">\(2^{d - k_d}\)</span>. For example, a block with local depth 2 stores key-value pairs with hashes that all begin with the same first two bits.</p>
<p>Blocks can store their key-value pairs however they want, like hash tables or BSTs, since they are loaded into internal memory before we work with them. We will assume that searching within the block is <span class="math">\(O(\log S)\)</span> where <span class="math">\(S\)</span> is the maximum number of key-value pairs a block can store.</p>
<p>Now, to lookup a key-value pair, we take the first <span class="math">\(d\)</span> bits of the key hash as an array index, then read the block at that array index into internal memory and search for the key-value pair within that block. For example, a key-value pair with hash <span class="math">\(010001\)</span> stored in a directory of order 3 is in the array at index <span class="math">\(010\)</span>, or 2 in decimal.</p>
<p>To insert a key-value pair, we take the first <span class="math">\(d\)</span> bits of the key hash as an array index, then read the block at that array index into internal memory. If there is room in the block, we insert the key-value pair and are finished. Otherwise, if the block contains the entries for multiple slots (<span class="math">\(k_d &lt; d\)</span>), we split the block into two blocks with local depth <span class="math">\(k_d + 1\)</span>, and make sure the array points to the correct blocks, then try to insert again. Otherwise, we have to rebuild the directory's entire array by doubling the array and populating it again, so <span class="math">\(d\)</span> is incremented.</p>
<p>To delete a key-value pair, we do something similar to insertion by reading the block and deleting the key, but merge two blocks together if possible. Merging involves decrementing <span class="math">\(k_d\)</span> on one block, copying key-value pairs over, then deleting the other block and updating references in the directory's array.</p>
<p>Clearly, lookup is <span class="math">\(O(1)\)</span> and only ever needs 1 disk transfer. Insertion could potentially take <span class="math">\(O(2^d)\)</span> time (to rebuild to array), but this happens very rarely, and only needs 2-3 disk transfers.</p>
<p>The advantage of extendible hashing is that we never have to re-update all the blocks in external memory, or rehash them. This makes it very well suited for working with external memory.</p>
<p>An empty directory starts off with all slots pointing to the same block, and the <span class="math">\(k_d\)</span> of that block is 0. Basically, all of the slots are combined into one. It is possible for multiple slots to point to the same block, when <span class="math">\(k_d &lt; d\)</span>. Insertion and deletion can cause blocks to become combined and split combined blocks.</p>
<p>So far, we have seen various ways of implementing dictionaries: unsorted arrays, sorted arrays, hash tables, AVL-trees, in increasing order of difficulty to implement.</p>
<h1 id="section-14">3/3/15</h1>
<h2 id="heuristics-and-self-rearranging-arrays">Heuristics and Self-rearranging Arrays</h2>
<p>In the real world, sometimes algorithms with poor worst case time complexity will actually perform better.</p>
<p>For dictionaries implemented using unordered arrays or linked lists, search will always need <span class="math">\(O(n)\)</span> worst case. However, a successful search takes <span class="math">\(O(\text{index of } k \text{ within the array/linked list})\)</span>.</p>
<p>The idea is to place keys that are more frequently searched for should be at the beginning of the array/linked list. In the real world, the 80-20 rule of thumb says that about 80% of searches will be for 20% of the keys.</p>
<p>If we know the frequencies of search for the keys beforehand, we can just put them in the array/linked list sorted by decreasing frequency. This allows to to minimize <span class="math">\(\sum_{k \in \text{ keys of the dictionary}} \text{frequency of search} \times \text{index of } k\)</span>. This is called <strong>static ordering</strong>.</p>
<p>We often don't know the frequencies. Instead, we could change the order of the keys with every search. Every time we search for a key and find it, we move it toward the beginning.</p>
<p>One such movement strategy is always moving the found item to the front, and shifting the array to the right by 1, the <strong>move-to-front heuristic</strong>. There is also the <strong>transpose heuristic</strong>, which always moves the found item 1 item toward the front of the array if not already at the front (by swapping it with its left neighbor). These are all <span class="math">\(O(1)\)</span>, except the move-to-front heuristic on arrays, which is <span class="math">\(O(n)\)</span>. The move-to-front heuristic tends to be better in practice.</p>
<p>For ordered arrays, search will always need <span class="math">\(O(\log n)\)</span> worst case. However, in real life we often have a general idea where the key is. For example, if we are looking for a name beginning with Z in a phone book, we would start near the end.</p>
<p>The idea is to guide the search by where we expect the key to be. If the current bounds in our search are <span class="math">\(l\)</span> and <span class="math">\(r\)</span> in the sorted array <span class="math">\(A\)</span> (assumed to contain cardinal values), then we would expect the a key <span class="math">\(k\)</span> to be at index <span class="math">\(l + (r - l)\frac{k - A[l]}{A[r] - A[l]}\)</span>, assuming the values are all uniformly spaced. This is called <strong>interpolation search</strong>:</p>
<pre><code>def interpolation_search(key, A, left = 0, right = len(A)):
    if left &gt;= right: return A[l] if A[l] == k else None
    middle = left + (right - left) * ((key - A[left]) / (A[right] - A[left]))
    if A[middle] &gt;= key: return interpolation_search(key, A, left, middle)
    return interpolation_search(key, A, middle + 1, right)</code></pre>
<p>This is now greater than <span class="math">\(O(\log n)\)</span> in the worst case, but it works pretty well in real life.</p>
<p>We also have <strong>galloping</strong>, which is a search technique for sorted arrays in which we start from the beginning, and skip ahead by increasing increments. For example, we might skip ahead by 1 at first, then 2, then 4, then 8, and so on, until we are at or past the desired key, in which case we would do a binary search or similar on the bounds. This is good for huge arrays (and even works on infinite-size arrays, as long as they are sorted) because we can start searching before the entire array is finished loading, and is actually <span class="math">\(O(\log n)\)</span> if we increase the search index exponentially.</p>
<p>A <strong>skip list</strong> is a data structure used for implementing dictionaries using ordered linked lists. Basically, the idea is that we store additional references to nodes within the list, so we can skip to it directly when we need to, like with an ordered array. We would essentially store lists <span class="math">\(L_1, \ldots, L_k\)</span>, where <span class="math">\(L_k\)</span> is a subset of <span class="math">\(L_{k - 1}\)</span> of proportional size, <span class="math">\(L_1\)</span> contains every node, and <span class="math">\(L_k\)</span> includes only 1. Each node has multiple fields for the next pointers of each level of list, so at each node we can go to the next element of any of the lists that include it. The goal is to have each list be a uniformly spaced sample of the elements of the lower list. This is a lot like a tree with variable arity and fixed height.</p>
<p>To search, we would first search left to right through the top list. Right before we pass the key's value or reach the end, we drop down a level and try searching left to right again, repeating the search and drop down until we have found the key or are at the lowest level and try to drop down (key not found).</p>
<p>To insert, we search for where <span class="math">\(k\)</span> should be, insert <span class="math">\(k\)</span> into the lowest list <span class="math">\(l_1\)</span>, and then randomly choose how many lists we choose to insert it into with decreasing probability (probability of inserting into <span class="math">\(i\)</span> lists above should be <span class="math">\(\frac 1 {2^i}\)</span>).</p>
<p>This takes <span class="math">\(O(n)\)</span> expected space and <span class="math">\(O(\log n)\)</span> expected runtime for search, insert, and delete.</p>
<h1 id="section-15">5/3/15</h1>
<p>Our abstract datatype dictionaries we have insertion, deletion, and search. However, there are often other ueful operations in many situations.</p>
<h2 id="range-queries">Range Queries</h2>
<p>A <strong>range query</strong> is an operation <span class="math">\(\text{Range}(x_1, x_2)\)</span> that returns all the key-value pairs <span class="math">\(k, v\)</span> such that <span class="math">\(x_1 \le k \le x_2\)</span> - all the key-value pairs in a range.</p>
<p>For implementations of dictionaries using unordered arrays, hash tables, heaps, and other data structures that use arrays that are not sorted, we can't do better than just searching through the entire array. For implementations that are sorted, like sorted arrays or BSTs, we can do binary search for the range endpoints, and build the result along the way, in <span class="math">\(O((x_2 - x_1) + \log n)\)</span> time.</p>
<p>We don't really care about the <span class="math">\(x_2 - x_1\)</span> term since we can't do anythin about it - it is the cost of producing the output. We will only consider the terms that depend on <span class="math">\(n\)</span>.</p>
<p>To do range queries on a BST, we can do a modified tree traversal to flatten the desired subtrees:</p>
<pre><code>def range_query(node, lower, upper):
    if node == None: return []
    if node.value &lt; lower or node.value &gt; upper: return []
    return range_query(node.left) + [(node.key, node.value)] + range_query(node.right)</code></pre>
<p>Range queries are extremely useful for building databases and working with multidimensional data. Suppose the items stored in a dictionary all each have <span class="math">\(d\)</span> attributes, and that the attributes are numbers - the values of the items are <span class="math">\(d\)</span>-tuples of numbers.</p>
<p>Now we want to do a <span class="math">\(d\)</span>-dimensional range query to search by multiple attributes.</p>
<p>One way to do 2D range queries is to use a quadtree. This is a data structure that stores points as a tree-like structure, where each node has 4 children that split a 2D rectangle into 4 quadrants. To build one, we find a bounding box of all the points (preferably, a power of 2), and then recursively split boxes into 4 smaller boxes as quandrants as necessary until every non-split box contains 0 or 1 points. To draw a quadtree, we label the edges as one of <span class="math">\(NE, NW, SW, SE\)</span> (and draw them in this order from left to right), label leaves as empty or the item it stores, and label non-leaf nodes with the bounding boxes they represent.</p>
<p>Searching a quadtree is similar to searching a binary tree, just with more cases for which child to recurse into. Insertion is similar to binary tree insertion as well, but when we find the leaf where the new point should go, we place it down if the leaf is empty, replace the leaf if the points are equal, or split the node into quadrants if it already has a point (putting the old point into the correct quadrant) and attempt to insert the new key into the correct child.</p>
<p>As a result, insertion can split a node many times. In fact, the height of a quadtree is unbounded with respect to <span class="math">\(n\)</span>. Potentially, we could have an arbitrary number of quadrant splits depending on how close together points are (closeness of new point to any existing points determines the max number of splits upon insertion). The height is actually <span class="math">\(O\left(\log \frac{\text{max distance between points}}{\text{min distance between points}}\right)\)</span>.</p>
<h1 id="section-16">10/3/15</h1>
<p>Range search in quadtrees is similar to range search in binary trees, but with bounds on both the X and Y coordinates of the points:</p>
<pre><code>def box_query(node, left, top, right, bottom, node_width, node_height):
    if node is None: return []
    if node.is_leaf: return [(node.key, node.value)]
    if node_width &lt; left or right &lt; 0: return [] # outside of the box bounds
    if node_height &lt; top or bottom &lt; 0: return [] # outside of the box bounds
    return range_query(node.top_left, left, top, right, bottom, node_width / 2, node_height / 2) +
           range_query(node.top_right, left + width / 2, top, right, bottom, node_width / 2, node_height / 2) +
           range_query(node.bottom_left, left, top + height / 2, right, bottom, node_width / 2, node_height / 2) +
           range_query(node.bottom_right, left + width / 2, top + height / 2, right, bottom, node_width / 2, node_height / 2s)</code></pre>
<p>We can also use a coloring algorithm that colors each node white (none of the children are in the box), black (all of the children are in the box), or gray (not sure, or neither). We recurse on gray nodes, ignore white nodes, and output all descendants of black nodes.</p>
<p>The runtime is therefore <span class="math">\(O(g + \frac w {O(g)} + \frac b {O(g)})\)</span>, where <span class="math">\(g, w, b\)</span> represents gray, white, and black, repectively. The number of gray nodes is unbounded, and therefore the runtime is as well.</p>
<p>A <strong>k-d tree</strong> is a data structure similar to quadtrees. Where in quadtrees we could split nodes into 4 equal parts when inserting, in k-d trees, we split the region into 2 parts (alternatingly horizontally and vertically, or through each dimension in higher dimensional k-d trees) that each have roughly the same number of points, and the size of both regions can be different.</p>
<p>The partitions/splits are placed at the median of all the points inside the node. Calculating the median can be done in <span class="math">\(O(n)\)</span> time, but is rather difficult to do in practice. Leaf nodes are those that have no split, and store points.</p>
<p>The height of a k-d tree is always <span class="math">\(O(\log n)\)</span> since each split cuts the search space roughly in half, so search is possible in <span class="math">\(O(\log n)\)</span> (compare coordinate of search to coordinate of split, then recurse if necessary). The tree uses <span class="math">\(O(n)\)</span> space.</p>
<p>Insertion is done by finding the leaf in which we'd insert the node, insert it so the node has 2 elements, then split the node and put the 2 elements into the 2 leaves, all <span class="math">\(O(\log n)\)</span> overall. However, this means the tree is no longer balanced, and the medians are no longer at the medians. What we could do instead is to keep track of how unbalanced the tree is, and if it gets too bad, we just rebuild the tree entirely. There are theoretical implementations that can rebalance a k-d tree after insertion, but they are largely impractical.</p>
<p>Deletion is done by finding the leaf containing the node, removing it, and then combining the node with its parent, so it is <span class="math">\(O(\log n)\)</span>. There are no ways to rebalance a k-d trees after a deletion.</p>
<p>Building a k-d tree from a set of elements is possible in <span class="math">\(\Theta(n \log n)\)</span>, by finding the median of all the elements (in <span class="math">\(O(n)\)</span>), partitioning the elements into two sets, then recursively rebuilding those sets into k-d trees.</p>
<p>Range query in a k-d tree is very similar to range queries in quadtrees, but we need to ensure we keep the query bounds and the current node's bounds up to date. Overall, this is a <span class="math">\(O(n) + O(\sqrt n)\)</span> operation, and there is a theorem to prove it.</p>
<p>Proof:</p>
<blockquote>
<p>We color the tree with our range query values - white for nodes that are entirely outside the query bounds, black for nodes entirely inside, and gray for all others, then ignore white nodes, return black nodes, and recurse on gray nodes.<br />Clearly, the time complexity of range queries is <span class="math">\(O(n)\)</span> for returning all the black nodes, plus the number of grey nodes. We want to prove that there are <span class="math">\(O(\sqrt n)\)</span> grey nodes.<br />Gray nodes are possible when parallel edges of the query rectangle intersect one edge of the node, when parallel edges of the query rectangle intersect parallel edges of the node, when the query rectangle is entirely inside the node, and when perpendicular edges of the query rectangle intersect perpendicular edges of the node.<br />Clearly, every gray node must have at least one intersection - there are more intersections than gray nodes.<br />Clearly, the largest number of intersection points between a line and a rectangle is proportional to the largest number of intersection points between a rectangle and a rectangle, so they have the sasme order.<br />Let <span class="math">\(Q(n)\)</span> be the largest number of intersection points between a horizontal/vertical line and the node's rectangle or its children's rectangles. Clearly, the number of gray nodes is <span class="math">\(O(Q(n))\)</span>.<br />Clearly, <span class="math">\(Q(n) \le 2 + Q(\frac n 4) + Q(\frac n 4) = O(\sqrt n)\)</span>, since there must be at least 2 points intersected in the node's rectangle, and the line can intersect at most <span class="math">\(Q(\frac n 4)\)</span> points in the descendents.<br />Since there are <span class="math">\(O(\sqrt n)\)</span> intersections, there are <span class="math">\(O(\sqrt n)\)</span> gray nodes.&lt; So there are at most <span class="math">\(O(\sqrt n)\)</span> recursions, and the time complexity is <span class="math">\(O(n) + O(\log n)\)</span> as required.</p>
</blockquote>
<p>We can use a strategy where we don't always alternate which dimension to split along to improve the rectangle shapes (ideally they would all be squares), which can sometimes reduce tree height.</p>
<p>We can also reduce height by storing points in interior nodes, which means that the points are on the split lines. If not, then we can also split at non-point coordinates - all split coordinates are valid.</p>
<h1 id="section-17">12/3/15</h1>
<p>Range trees are another way to implement dictionaries that use points as keys. This tree has the advantage of not having the bad time complexities of quadtrees and kd-trees. Also, it allows us to do range queries in a much simpler way.</p>
<p>Basically, a range tree stores points in a balanced BST by the X coordinate of each point. Every node in the tree also contains a balanced BST, which contains the node's points and the points of all of the node's descendents, by the Y coordinate of each point. When we compare points, we compare the X coordinates, then the Y coordinates if the X coordinates are equal.</p>
<p>This means that points will be stored multiple times in the same tree. Each point will appear in the tree at most once per level of the tree, and is proportional to the height. Therefore, range trees need O(n n) memory, since each node needs <span class="math">\(O(\log n)\)</span> memory.</p>
<p>Searching a range tree is as easy as using binary search to find a point in the main tree with the same X coordinate, then search in the node's tree for the one with the same Y coordinate, all possible in <span class="math">\(O(\log n)\)</span>.</p>
<p>Inserting is simply inserting the new point into the main tree, then going upwards and inserting the point into the tree for each ancestor node of the new node and rebalancing those trees. However, we can't rebalance the main tree because that would ruin the node trees, so we basically have to rebuild it every so often. Deletion is a similar operation.</p>
<p>To do a range query within <span class="math">\((x_1, x_2)\)</span> to <span class="math">\((x_2, y_2)\)</span>, we do a range query in the main tree for <span class="math">\(x_1\)</span> to <span class="math">\(x_2\)</span>. Now we color the tree using this query: the nodes that are on the paths of the boundary nodes in the range query are gray, the nodes that are entirely inside the range query are black, and the ones that are entirely outside are white. We ignore white nodes, do an explicit test for whether the point is in the range for gray nodes, and know that the point is in the range for black nodes. For points that we know are in the X range, we can now do a range query for <span class="math">\(y_1\)</span> to <span class="math">\(y_2\)</span> for each point, and combined, this is the result of the original range query. Since there are <span class="math">\(O(\log n)\)</span> of each type of node, and the range query in the node tree is <span class="math">\(O(n)\)</span>, range queries are <span class="math">\(O(\log^2 n) + O(n)\)</span> (and can actually be a bit better with certain tricks).</p>
<h2 id="tries">Tries</h2>
<p>Dictionaries for words are relatively common. There are dedicated data structures that make it very simple to associate data with words. Recall that words are sequences of characters, and characters are elements of a finite set, an alphabet (commonly <span class="math">\(\Sigma = \set{0, 0}\)</span> or ASCII values). For this purpose we can use a <strong>trie</strong>.</p>
<p>A trie is an <span class="math">\(n\)</span>-ary tree with edges labelled with characters from <span class="math">\(\Sigma\)</span>, or the end of word symbol (by convention, we usually use $). Basically, words are stored as the sequence of edges in paths from the root node to leaf nodes character by character, where the <span class="math">\(i\)</span>th level corresponds to the <span class="math">\(i\)</span>th character (or the end of word symbol) of a word. Tries do not contain any unnecessary edges. At the leaf nodes we could store the whole word, or any data we want:</p>
<pre><code>     / word: 1
    /$
  / \1
 /   \
/1    word: 11
\0
 \   word: 0
  \ /$</code></pre>
<p>One variation of this is to store words at internal nodes instead of using the end of word symbol. This might be beneficial if using an end of word symbol makes implementation difficult. We could also not store the words at all, and instead keep track of all the edge labels as we recurse downward to reconstruct the word.</p>
<p>Searching for a word is as easy as iterating through each character and taking the edge labelled by that character as the new tree to search in. If a leaf node is reached, we found the word. Otherwise, one of the labelled edges we want don't exist and the word is not in the trie. The runtime of this operation is <span class="math">\(O(\abs{w}\abs{\Sigma})\)</span>, where <span class="math">\(w\)</span> is the word and <span class="math">\(\Sigma\)</span> is the alphabet.</p>
<p>To insert a node into a trie, we follow edges corresponding to characters in the word (creating them as necessary) until we arrive at some node, then create an edge labelled with the end of word symbol to a leaf node and store the data that leaf node. To delete a node from a trie, we find where it should be in the trie, then delete that node and its ancestors up to but not including the first one with 2 or more children, to remove unnecessary edges. Insertion is <span class="math">\(O(\abs{w}\abs{\Sigma})\)</span>, and deletion is <span class="math">\(O(\abs{w})\)</span>.</p>
<p>We can also compress tries by allowing edges to store more than one character. This allows us to write them in a cleaner way, but still uses the same asymptotic memory.</p>
<p>A <strong>Patricia trie</strong> is a special type of trie in which nodes are also labelled with the index of the character to test, in addition to edges labelling characters. In a normal trie, the level of the node determines the index of the haracter we test; a node at level 5 means we take edges corresponding to the 5th character of the word. In a Patricia trie, we can test any character at any node. If the index is greater than the index of the end of word symbol in the word, then we take that to mean that the word is not in the Patricia trie.</p>
<p>Patricia tries can actually compress tries - we can simply make tries that avoid checking characters that are the same in all the descendents (for example, if all the descendents have the second character be 0, then we can just skip checking the second character in that subtree), and since we store the entire word at each leaf node, we can't get to a leaf node by following a word that isn't actually in the trie.</p>
<p>Patricia tries use less space in practice, though don't really improve time complexities.</p>
<h1 id="section-18">17/3/15</h1>
<h2 id="pattern-matching">Pattern Matching</h2>
<p>Pattern matching is the problem of finding strings inside other strings, like what <code>grep</code> does.</p>
<p>Formally, the problem is checking for the existance and finding the position of the string <span class="math">\(P[0 \ldots m - 1] \in \Sigma^*\)</span> within the string <span class="math">\(T[0 \ldots n - 1] \in \Sigma^*\)</span>, where <span class="math">\(\Sigma\)</span> is a set of characters. Here, <span class="math">\(P\)</span> is the <strong>pattern/needle</strong>, <span class="math">\(T\)</span> is the <strong>text/haystack</strong>, and <span class="math">\(\Sigma\)</span> is the <strong>alphabet</strong>.</p>
<p>Generally speaking, <span class="math">\(T\)</span> will be much larger than <span class="math">\(P\)</span>. Although we care about time complexity with respect to the length of <span class="math">\(P\)</span>, we care much more about time complexity with respect to <span class="math">\(T\)</span>.</p>
<p>A <strong>substring</strong> of <span class="math">\(T\)</span> is a string <span class="math">\(S\)</span> such that <span class="math">\(S = T[i \ldots j]\)</span> for some <span class="math">\(i, j\)</span>. A <strong>prefix</strong> is a string <span class="math">\(S\)</span> such that <span class="math">\(S = T[0 \ldots i]\)</span> for some <span class="math">\(i\)</span>. A <strong>suffix</strong> is a string <span class="math">\(S\)</span> such that <span class="math">\(S = T[i \ldots n - 1]\)</span> for some <span class="math">\(i\)</span>.</p>
<p>An <strong>occurrence</strong> of <span class="math">\(P\)</span> in <span class="math">\(T\)</span> is an index <span class="math">\(s\)</span> such that <span class="math">\(T[s \ldots s + (m - 1)] = P[0 \ldots m - 1]\)</span>. If <span class="math">\(s\)</span> is an occurrence of <span class="math">\(P\)</span> in <span class="math">\(T\)</span>, then we say <span class="math">\(P\)</span> occurs in <span class="math">\(T\)</span> at shift <span class="math">\(s\)</span>. Note that the equality operator over strings doesn't take constant time - checking if an index <span class="math">\(s\)</span> is an occurrence takes <span class="math">\(\Theta(m)\)</span> time in the worst case.</p>
<p>The brute force algorithm for pattern matching is just to try every possible shifts and return the first one that works:</p>
<pre><code>def find(needle, haystack):
    for s in range(len(haystack) - len(needle)):
        for i, c in enumerate(needle):
            if c != haystack[s + i]: break
        else: return True
    return False</code></pre>
<p>Clearly, this is <span class="math">\(\Theta(m(n - m + 1))\)</span> in the worst case. For small, constant <span class="math">\(m\)</span> this is actually relatively fast, especially in practice.</p>
<p>However, we are clearly doing a lot of unnecessary comparisons. For example, if we have a pattern &quot;gattaca&quot; and text &quot;gattaccgattact&quot;, we would fail the match at the second &quot;c&quot;, but since we know we didn't match, we could just skip 7 characters ahead and start at the second &quot;g&quot;, avoiding the need to check all the shifts in between. For longer patterns, this gives huge performance gains.</p>
<p>The idea is to improve runtime by using preprocessing - doing a lot of work upfront, but then making later searches very fast. We could preprocess the text (suffix trees do this), or preprocess the pattern (DFAs, KMP, Boyer-Moore, Rabin-Karp all do this).</p>
<p>String matching using DFAs is always <span class="math">\(O(n)\)</span>. An NFA is easy to construct, but the DFA is not - converting an NFA to a DFA results in exponentially increasing numbers of states. Instead, we have to design the DFA directly. For example, if we are matching <span class="math">\(P = abac\)</span> in <span class="math">\(\Sigma = \set{a, b, c}\)</span>, then we have the following DFA:</p>
<pre><code># one transition per each character of the pattern
start &gt; a &gt; a
a &gt; b &gt; ab
ab &gt; a &gt; aba
aba &gt; c &gt; abac
abac &gt; accept

# transitions from each state to the state with the longest value that is a suffix of the current state
a &gt; c &gt; start
a &gt; a &gt; a
ab &gt; b, c &gt; start
aba &gt; a &gt; a
aba &gt; b &gt; ab</code></pre>
<p>The basic algorithm is to add a state for each prefix of <span class="math">\(P\)</span>, and for each combination of prefix <span class="math">\(w\)</span> and next character <span class="math">\(x\)</span>, add a transition from the state <span class="math">\(w\)</span> to the state with the longest prefix <span class="math">\(v\)</span> that is a suffix of <span class="math">\(wx\)</span>.</p>
<p>Finding the DFA is possible in quadratic time, or cubic using a naive solution. However, this is far too slow in practice.</p>
<p>Instead, we can use <span class="math">\(\epsilon\)</span>-NFAs, that we make deterministic. We enforce that we only take the <span class="math">\(\epsilon\)</span> transition if there are no other possible transitions. Basically, we have the standard matcher, but at each state we have an <span class="math">\(\epsilon\)</span> transition that tells us where to go if the <span class="math">\(i\)</span>th character doesn't match. Since there is only one epsilon transition, with our rule the <span class="math">\(\epsilon\)</span>-NFA is deterministic.</p>
<p>For example, the <span class="math">\(\epsilon\)</span>-NFA for <span class="math">\(P = abac\)</span> appears as follows:</p>
<pre><code># one transition per each character of the pattern
start &gt; a &gt; a
a &gt; b &gt; ab
ab &gt; a &gt; aba
aba &gt; c &gt; abac
abac &gt; accept

# epsilon transitions to the failure states
a &gt; &gt; start
ab &gt; &gt; start
aba &gt; &gt; a</code></pre>
<p>This is the <strong>Knuth-Morris-Pratt algorithm</strong> (KMP) - using <span class="math">\(\epsilon\)</span>-NFAs to match strings.</p>
<p>The idea is to describe the <span class="math">\(\epsilon\)</span>-NFA using a <strong>failure function</strong> <span class="math">\(\mathbb{T}(k)\)</span> - the state to go to given that we have seen <span class="math">\(P[0 \ldots k - 1]\)</span> in the text and the next character of <span class="math">\(T\)</span> is not <span class="math">\(P[k]\)</span>. We can represent the failure function using a table where the columns are <span class="math">\(k\)</span> values and the rows are the prefixes with lengths equal to those <span class="math">\(k\)</span> values and the corresponding values of <span class="math">\(\mathbb{T}(k)\)</span>.</p>
<p>In code, we can represent each state by the length of the prefix it represents, where the start state is 0 and the state representing the prefix &quot;aba&quot; represents 3. This looks like the following:</p>
<pre><code>def kmp(T, P):
    compute the failure function $T(k): 1 \ldots m - 1 \to 0 \ldots m - 1$
    state = 0
    i = 0
    while i &lt; len(T):
        if T[i] == P[state]: # match/forward transition
            state += 1 # move forward in pattern
            i += 1 # move forward in text
            if state == len(P): return True
        else: # mismatch/backward transition
            if state &gt; 0: state = T(state) # follow epsilon transition
            else: i += 1 # stay at start state and move forward in text</code></pre>
<p>Clearly, each run of the loop either does a forward transition, which consumes a character and therefore can happen at most <span class="math">\(n\)</span> times, or we do a backward transition, which is always followed by a forward transition (since the state we transition to must be able to consume the next character), and therefore can happen at most <span class="math">\(n\)</span> times. As a result, the loop can run at most <span class="math">\(O(n + n) = O(n)\)</span> times, and is therefore the algorithm's worst case time complexity.</p>
<p>How do we compute the failure function?</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>