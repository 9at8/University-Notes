<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>STAT231 | Anthony Zhang</title>
  <link rel="stylesheet" href="../css/base.css" type="text/css">
  <link rel="stylesheet" href="../css/note.css" type="text/css">
  <link rel="stylesheet" href="../highlight/styles/default.css">
  <link rel="stylesheet" href="../highlight/styles/paraiso.light.css">
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body onload="highlight()">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
  <h1>Lecture Notes by <a href="/">Anthony Zhang</a>.</h1>
  <ul class="site_links">
    <li><a href="/blog/" class="page">blog</a></li>
    <span class="divider"></span>
    <li><a href="http://uberi.github.io/University-Notes" class="page">notes</a></li>
    <span class="divider"></span>
    <li><a href="/resume.pdf" class="page">résumé</a></li>
    <span class="divider"></span>
    <li><a href="https://github.com/Uberi" class="contact">github</a></li>
    <span class="divider"></span>
    <li><a href="http://www.linkedin.com/pub/anthony-zhang/8b/aa5/7aa" class="contact">linkedin</a></li>
    <span class="divider"></span>
    <li><a href="mailto:azhang9@gmail.com" class="contact">email</a></li>
    <span class="divider"></span>
    <li><a href="https://www.facebook.com/anthony.zhang.user" class="contact">facebook</a></li>
    <span class="divider"></span>
    <li><a href="https://twitter.com/anthony926535" class="contact">twitter</a></li>
    <span class="divider"></span>
    <li><a href="/anthony-zhang.asc" class="info">GPG key</a></li>
  </ul>
<p style="display:none"><span class="math">\[
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\tup}[1]{\left\langle #1 \right\rangle}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1 \right\rceil}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\rem}{\operatorname{rem}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\imag}{\boldsymbol{i}}
\newcommand{\dee}{\mathop{}\!\mathrm{d}}
\newcommand{\lH}{\overset{\text{l'H}}{=}}
\newcommand{\evalat}[1]{\left.\left(#1\right)\right|}
\newcommand{\sech}{\operatorname{sech}}
\newcommand{\spn}{\operatorname{Span}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\prp}{\operatorname{perp}}
\newcommand{\refl}{\operatorname{refl}}
\newcommand{\magn}[1]{\left\lVert #1 \right\rVert}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\sys}[2]{\left[ #1 \mid #2\hskip2pt \right]}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\formlp}{\operatorname{Form}(\mathcal{L}^P)}
\]</span></p>
<h1 id="stat231">STAT231</h1>
<p>Statistics.</p>
<pre><code>Cyntha Struthers
Section 001
Email: castruth@gmail.com
Mondays/Wednesdays/Fridays 1:30pm-2:20pm in STP 105</code></pre>
<p>This is the second time I'm taking this course. The first time around, I received an injury about a month in that made going to this class no longer feasible. This set of notes therefore contains the Spring 2016 version of the course, combined with the Winter 2017 version of the course. Over time, these will be merged together.</p>
<h1 id="section">2/5/16</h1>
<p>There are 3 tutorial tests, 2 midterms, and a final exam.</p>
<p>In STAT230, we were given information about the population, and wanted to find the chance of a sample happening. In STAT231, we will be learning about the opposite - given information about a sample of the population, what can we figure out about the population? Where STAT230 asks, &quot;given 100 fair coin tosses, what is the probability of getting 60 heads?&quot;, STAT231 asks, &quot;if we toss a coin 100 times and get 60 heads, what can we say about the fairness of the coin?&quot;.</p>
<p>The Challenger space shuttle disaster occurred due to a failed O-ring. However, the right data analysis could have presented this tragedy - there was already enough data available to find this problem, but it wasn't looked at in the right way. Statistics can help prevent this sort of error.</p>
<p>Kansas weather reports, in a study, were found to be accurate about 85% of the time in predicting rain. However, if they just said &quot;it's not going to rain&quot;, they would be right 90% of the time. Likewise, ESPN pundits were found to correctly predict the results of games about 48% of the time. Statistics can help us evaluate which analysts and predictors are believable.</p>
<p>Statistics also allows us to evaluate relationships between two things. For example, does smoking cause lung cancer, or does lung cancer cause smoking? Are they related at all?</p>
<h2 id="statistical-data">Statistical data</h2>
<p>There are two main types of data:</p>
<ul>
<li><strong>Numeric data</strong>, like a grade or a width. Numerical data can be:
<ul>
<li><strong>Discrete</strong> - an element of a countably large set. For example, the number of pennies in a set of coins.</li>
<li><strong>Continuous</strong> - a measure, like height or weight.</li>
</ul></li>
<li><strong>Categorical data</strong> (area of study, name, etc.). Categorical data can be:
<ul>
<li><strong>Binary</strong> - falling into two categories.</li>
<li><strong>Ordinal</strong> - there are categories, but there is an underlying order to the data. For example, colors are categorical data, but they have an underlying order on the EM spectrum.</li>
</ul></li>
</ul>
<p>A <strong>transformation</strong> is a function over a variable. A <strong>linear transformation</strong> is one of the form <span class="math inline">\(y = mx + b\)</span>. A linear transformation is also known as an <strong>affine transformation</strong>.</p>
<p>A <strong>coding</strong> is a transformation that converts categorical data to numerical data. For example, colors can be assigned numbers, like 0 for red, 1 for orange, and so on.</p>
<h2 id="summaries">Summaries</h2>
<p>We often want to extract important information about a data set to find its properties. When we do so, we extract <strong>data summaries</strong>. We can do this <strong>numerically</strong> (like finding the mean, stddev, median, etc.) or <strong>graphically</strong> (like making a scatter plot). Numerical summaries tell us about certain fundamental properties of data sets, while graphical summaries tell us the shape of the data.</p>
<p>Common numerical summaries we care about are:</p>
<ul>
<li>The centre of the data, or <strong>central tendency</strong>:
<ul>
<li>The <strong>sample mean/arithmetic mean</strong>: for a variable <span class="math inline">\(y\)</span>, the sample mean is denoted <span class="math inline">\(\overline y = \frac 1 n \sum_{i = 0}^n y_i\)</span>.
<ul>
<li>The nice thing about this is that the sum of the deviations from the mean is always 0, so <span class="math inline">\(\sum (y_i - \overline y) = 0\)</span>.</li>
<li>Under an affine transformation <span class="math inline">\(y_i = ax_i + b\)</span>, <span class="math inline">\(\overline y = a \overline x + b\)</span> - the mean can simply be transformed as well.</li>
</ul></li>
<li>The <strong>geometric mean</strong> is better for logarithmically distributed data: for a variable <span class="math inline">\(y\)</span>, the geometric mean is denoted <span class="math inline">\(\overline y = \left(\prod_{i = 0}^n y_i\right)^{\frac 1 n}\)</span>.</li>
<li>The <strong>harmonic mean</strong> is rarely useful, and is the reciprocal of the arithmetic mean of the reciprocals of a variable.</li>
<li>The average is usually the one value that, if applied in the problem's situation, is equivalent to applying all the original values.</li>
<li>The <strong>median</strong> is the middle-most observation, or the sample mean of the middlemost observations if there are multiple. Essentially, we arrange the dataset in ascending order, and then pick the middle one.
<ul>
<li>The advantage of medians is that they are less sensitive to outliers than summaries like the sample mean.</li>
<li>The concept of the median can be extended to <strong>quartiles</strong> and <strong>percentiles</strong>, and generalized into <strong>quartiles</strong>. While the median is the value such that 50% of the dataset is at or below it, the quartiles and percentiles are a different fraction.</li>
<li>The first quartile is a value for which 25% of the data is equal or below that value, while the second quartile is 50%, the third 75%, and the fourth 100%.</li>
<li>The first percentile is a value for which 1% of the data is equal or below that value, while the second percentile is 2%, the third 3%, and so on.</li>
</ul></li>
<li>The <strong>mode</strong> is the observation or observations that occur most often (a dataset can have more than 1 mode). This is often more useful for categorical data, or discrete numerical data with only a few possibilities.</li>
</ul></li>
<li>The volatility, or <strong>dispersion</strong>:
<ul>
<li>The <strong>range of a dataset</strong> is two numbers - the minimum value of the dataset, and the maximum value. This can also be thought of as the zeroth and fourth quartile values.</li>
<li>The <strong>interquartile range</strong> (IQR) is the range of the middle 50% of the dataset - the first quartile value and third quartile value.</li>
<li>The <strong>sample variance</strong> is defined as <span class="math inline">\(s^2 = \frac{1}{n - 1} \sum_{i = 0}^n \left(y_i - \overline y\right)^2\)</span>. Note that the variance is <span class="math inline">\(s^2\)</span>, not <span class="math inline">\(s\)</span>. Also, the variance of a dataset <span class="math inline">\(x = \set{x_1, \ldots, x_n}\)</span>
<ul>
<li>This is almost, but not quite, the average of the squared deviation from the mean.</li>
<li>Basically, this measures how much the data is spread out from the mean.</li>
<li>There's a good reason to divide by <span class="math inline">\(n - 1\)</span> rather than <span class="math inline">\(n\)</span>, but we'll cover that later on.</li>
<li>Under an affine transformation <span class="math inline">\(y_i = ax_i + b\)</span>, <span class="math inline">\(s_y^2 = a^2 s_x^2\)</span> - the squared factor applies to the variance, but not the variance.</li>
<li>Another useful formula for the sample variance is <span class="math inline">\(s^2 = \frac 1 {n - 1} \left(\sum y_i^2 - n(\overline y)^2\right)\)</span>.</li>
</ul></li>
<li>The <strong>standard deviation</strong> is the positive square root of the sample variance, denoted <span class="math inline">\(s\)</span>.
<ul>
<li>Why do we square the deviations rather than just adding them up like <span class="math inline">\(\sum_{i = 0}^n (y_i - \overline y)\)</span>? The negative deviations and positive deviations would cancel each other out; squaring the values ensures that the standard deviation accumulates to a non-negative value. This also ensures that variance is always symmetric - points above and below the mean both contribute the same amount to the variance.</li>
<li>Why do we square the deviations rather than adding their absolute values like <span class="math inline">\(\sum_{i = 0}^n \abs{y_i - \overline y}\)</span>? While the absolute value function would still represent variance (the given formula is called the <strong>mean absolute deviation</strong>), it's harder to work with since it's not differentiable. Also, the squared deviation is affected by outliers quadratically while the absolute deviation is only affected linearly, so taking its square root later would give unintuitive results.</li>
<li>Under an affine transformation <span class="math inline">\(y_i = ax_i + b\)</span>, <span class="math inline">\(s_y = a s_x\)</span> - the factor applies to the standard deviation, but not the intercept (this trivially follows from the formula for the sample variance).</li>
</ul></li>
<li>The <strong>mean absolute deviation</strong> is defined as <span class="math inline">\(\frac{1}{n - 1} \sum_{i = 0}^n \abs{y_i - \overline y}\)</span>. This is what we get if we use the absolute value function rather than squaring in the formula for variance. It's relatively rarely used.</li>
</ul></li>
<li>How fat the tails are, or <strong>kurtosis</strong> - the frequency of extreme obserations.</li>
<li>How symmetric the data is about some point or axis, or <strong>symmetry</strong>.</li>
</ul>
<p>Suppose we have $100 in a bank account, with 4% interest the first year, 8% the second, and 12% the third. What is the average interest rate?</p>
<blockquote>
<p>Although it would seem at first glance to be 8%, the arithmetic mean doesn't represent the actual average here. The average interest rate is the single rate such that, after those 3 years, we would have the same interest gains as we did with this bank account.<br />
Let's denote this rate as <span class="math inline">\(x\)</span>. Then according to the interest formula, <span class="math inline">\(100(1 + x)^3 = 100 \cdot 1.04 \cdot 1.08 \cdot 1.12\)</span>.<br />
So <span class="math inline">\((1 + x)^3 = 1.257984\)</span> and <span class="math inline">\(x = 0.0795059\)</span>, or about 0.795%.<br />
As it turns out, this is basically the same thing as the geometric mean of the interest rates. In this case, we use the geometric mean since the interest is defined in terms of an enponential function of <span class="math inline">\(n\)</span>.</p>
</blockquote>
<p>A car drives from A to B at 40 km/h, and immediately drives back at 60 km/h. What is the average speed of the car?</p>
<blockquote>
<p>Although it would seem at first glance to be 50 km/h, this is not the case, since the car takes less time driving back since it's going faster.<br />
Let's denote the distance between A and B as <span class="math inline">\(x\)</span>, in km. Then the trip from A to B took <span class="math inline">\(\frac{x}{40}\)</span> hours, and the trip back took <span class="math inline">\(\frac{x}{60}\)</span> hours.<br />
Since the total time taken is <span class="math inline">\(\frac{x}{40} + \frac{x}{60}\)</span>, the average speed is <span class="math inline">\(\frac{40\frac{x}{40} + 60\frac{x}{60}}{\frac{x}{40} + \frac{x}{60}} = \frac{2}{\frac{1}{40} + \frac{1}{60}}\)</span>, or 48 km/h.<br />
As it turns out, this is the same thing as the harmonic mean of the speeds.</p>
</blockquote>
<h1 id="section-1">4/5/16</h1>
<p>When we talk about distributions, we oftne picture them as bumps or hills - a high area in the middle, and low areas on the sides. In this picture, the <strong>tails</strong> of a distribution are the low areas on the sides. The concept of tails doesn't really generalize that well, so we'll avoid it for actual formal explanations.</p>
<h3 id="centrality">Centrality</h3>
<p>To find a percentile <span class="math inline">\(\beta\)</span>, we sort the observations from low to high, and then take the value at the 1-indexed position <span class="math inline">\(\beta(n + 1)\)</span> (or the sample mean of the two values closest to the 1-indexed position <span class="math inline">\(\beta(n + 1)\)</span>).</p>
<p>The average/central tendency isn't always the only thing we care about. Suppose you flip a coin <span class="math inline">\(n\)</span> times before a heads appears. If you are paid <span class="math inline">\(2^n\)</span> dollars for doing so, at what price would you pay to play this game?</p>
<blockquote>
<p>Note that a head comes up on the first trial with 1/2 probability, paying out 2 dollars, on the second trial with 1/4 probability, paying out 4 dollars, and so on.<br />
So the expected value is <span class="math inline">\(\sum_{i = 1}^\infty i \frac 1 i = \infty\)</span> - the expected value is infinity!<br />
However, most people wouldn't even pay 40 dollars to play - it seems like risk/variability is also an important factor in deciding whether to play. This is called St. Peter's Paradox.</p>
</blockquote>
<p>Suppose we have a dataset <span class="math inline">\(x = \set{x_1, \ldots, x_n}\)</span>. Suppose we know one observation with value <span class="math inline">\(k\)</span> is unreliable, and we want to discard it from <span class="math inline">\(x\)</span>. Can we find the new mean and variance?</p>
<blockquote>
<p>Let <span class="math inline">\(y = \set{y_1, \ldots, y_{n - 1}}\)</span> be the dataset with the unreliable observation removed.<br />
Clearly, <span class="math inline">\(\sum y_i = \sum x_i - k\)</span>, since we removed only that one element, and since <span class="math inline">\(\sum x_i = n \overline x\)</span>, <span class="math inline">\(\sum y_i = \sum x_i - k\)</span>.<br />
So the sample mean is <span class="math inline">\(\overline y = \frac{\sum x_i - k}{n - 1}\)</span>.<br />
To find the variance, we can use the alternate form of the variance formula, so <span class="math inline">\(s_y^2 = \frac 1 {(n - 1) - 1} \left(\sum y_i^2 - (n - 1)(\overline y)^2\right)\)</span>.<br />
Since <span class="math inline">\(\sum y_i = \sum x_i - k\)</span>, <span class="math inline">\(\sum y_i^2 = \sum x_i^2 - k^2\)</span>.<br />
Since we already know <span class="math inline">\(\overline y\)</span>, <span class="math inline">\(s_y^2 = \frac{1}{n - 2} \left(\sum x_i^2 - k^2\right) - \frac{\left(\sum x_i - k\right)^2}{n - 1}\)</span>.<br />
This same technique can be used to calculate the mean and variance of a dataset after adding or removing any number of items to it, without looking at the existing elements of the dataset.</p>
</blockquote>
<p>When you have a dataset with sample mean <span class="math inline">\(\overline x\)</span> and remove an item with value <span class="math inline">\(\overline x\)</span>, the mean stays the same, while the variance increases or stays the same.</p>
<p>The batting champion of a baseball season is the player with the highest batting average (probability of hitting the ball in each attempt, times 1000). However, although there have been 13 people with a batting average above 400 before 1941, there have been none after that year. Why does this happen?</p>
<p>In every sport, players have gotten better, absolutely speaking, over time - it doesn't seem like batters are actually getting worse. However, better pitchers, fielders, and managers can make a batter's job a lot harder.</p>
<p>We can actually test this by looking at the batting average for individual average batters over their careers. As it turns out, the average batter's batting average stays about the same over their career. However, the variance in the average player's ball hitting rate shrinks over their careers. Since the average is lower than 400, the shrinking variances mean that the tails in the batting average distribution shrink as well (causing fewer 400+ batting averages). When we become more consistent at hitting the ball at the average rate, that means there are fewer low-performers, but also fewer high-performers.</p>
<h1 id="section-2">9/5/16</h1>
<h3 id="symmetry">Symmetry</h3>
<p>We also often care about <strong>skewness</strong> - how non-symmetric the data is. This measures how different the dataset looks when comparing the side of the mean, mirrored over to the right side. Left-skewing/negative-skewing datasets have distributions that seem to &quot;lean to the right&quot; (they have a long left tail), while right-skewing/positive-skewing distributions seem to &quot;lean to the left&quot; (they have a long right tail).</p>
<p>One way to estimate skewness is to compare the mean and median, measuring <span class="math inline">\(\text{mean} - \text{median}\)</span>. If the mean is less than the median, then we'd say the dataset might be left skewed (more of the weight is on the left), if they were equal, we say the dataset is probably symmetric (weight is about the same above and below the mean), and if the mean was greater than the median, then we say the dataset might be right-skewed (more of the weight is on the right side).</p>
<p>The real measure of skewness is <span class="math inline">\(\frac{\frac 1 n \sum \left(y_i - \overline y\right)^3}{\left(\frac 1 n \sum \left(y_i - \overline y\right)\right)^{\frac 3 2}}\)</span>. We won't be expected to memorize this for the course.</p>
<p>For example, if a person is diagnosed with a disease that has a mean time of death about 8 months after diagnoses, and survives for several months after diagnosis, then they should hope that the distribution of deaths is more right-skewed, so that most of the probability of death is already behind them.</p>
<h3 id="spikiness">Spikiness</h3>
<p><strong>Kurtosis</strong> is the measure of how frequent extreme observations are with respect to the normal distribution. Basically, it checks if the tails of the dataset's distribution are fatter (more extreme observations) or thinner (less extreme observations) than a normal distribution. It can also be said to measure how peaky/pointy the distribution's curve is at the top.</p>
<p>Kurtosis is defined as <span class="math inline">\(K = \frac{\frac 1 n \sum \left(y_i - \overline y\right)^4}{\left(\frac 1 n \sum \left(y_i - \overline y\right)\right)^2}\)</span>. A normal distribution has <span class="math inline">\(K = 3\)</span>. If <span class="math inline">\(K &gt; 3\)</span>, then the tails of the dataset's distribution are fatter than a normal distribution (there are more extreme observations), and if <span class="math inline">\(K &lt; 3\)</span>, then the tails of the dataset's distribution are thinner than a normal distribution (there are fewer extreme observations).</p>
<p>Kurtosis is especially useful in finance, where accurate assessment of risks and expected returns are important. It basically tells us how normal-like our data is - if the kurtosis is very close to 3, we can often assume that the dataset is normally distributed, which is extremely useful for doing statistics with.</p>
<h3 id="association">Association</h3>
<p>Suppose we have a dataset <span class="math inline">\(D = \set{\tup{x_1, y_1}, \ldots, \tup{x_n, y_n}}\)</span> with two variables. How do we measure how much these two variables are associated (how is one variable affected by the other)?</p>
<p>For example, suppose we have categorical variables, <span class="math inline">\(x = \set{\text{smoker}, \text{non-smoker}}\)</span> and <span class="math inline">\(y = \set{\text{lung cancer}, \text{no lung cancer}}\)</span>. How do we determine the relationship between these variables, given a dataset of samples?</p>
<p><strong>Relative risk</strong> is a measure of association between two categorical variables. Basically, relative risk is <span class="math inline">\(\frac{P(A \mid B)}{P(A \mid \neg B)}\)</span> - the probability of <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> occurrs over the probability of <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> does not occur. For independent variables, relative risk is 1. The farther the relative risk is from 1, the more strongly it implies that the variables are associated.</p>
<p>For two categorical Boolean variables, relative risk is <span class="math inline">\(\frac{\frac{\abs{x \wedge y}}{\abs{x}}}{\frac{\abs{\neg x \wedge y}}{\abs{\neg x}}}\)</span>. Expanded into a more useful form, it becomes <span class="math inline">\(\frac{\frac{\abs{x \wedge y}}{\abs{x \wedge y} + \abs{x \wedge \neg y}}}{\frac{\abs{\neg x \wedge y}}{\abs{\neg x \wedge y} + \abs{\neg x \wedge \neg y}}}\)</span>.</p>
<p>The <strong>sample correlation coefficient</strong> is a measure of association between two numerical variables, like relative risk is for two categorical variables. It's defined as <span class="math inline">\(r_{xy} = \frac{\sum \left(x_i - \overline x\right)\left(y_i - \overline y\right)}{\sqrt{\sum \left(x_i - \overline x\right)^2} \sqrt{\sum \left(y_i - \overline y\right)^2}}\)</span>. Specifically, it measures <strong>linear association</strong>.</p>
<p>The sign of <span class="math inline">\(r_{xy}\)</span> (specifically, the sign of the numerator) tells us the direction of the association (positive means variables tend to increase each other, while negative means one tends to increase when the other decreases, and vice versa), and the magnitude of <span class="math inline">\(r\)</span> tells us the strength of the association (0 meaning no association). The denominator ensures that <span class="math inline">\(-1 \le r_{x, y} \le 1\)</span> for any dataset.</p>
<p>For non-linear relationships, the sample correlation coefficient doesn't work so well. For example, for two variables <span class="math inline">\(x, y\)</span> associated by <span class="math inline">\(y = x^2\)</span>, the sample correlation coefficient is 0, even though we defined them to be associated by a particular function. A general measure of association for any function (not just linear ones) wouldn't really be meaningful since it's always possible to construct a function to fit a finite dataset. Therefore, all measures of association must be against a particularly chosen class of functions.</p>
<p>For a perfect linear relationship between variables <span class="math inline">\(x, y\)</span>, like <span class="math inline">\(y = ax + b\)</span>, <span class="math inline">\(r_{xy} = 1\)</span> if <span class="math inline">\(b &gt; 0\)</span>, and <span class="math inline">\(r_{xy} = -1\)</span> if <span class="math inline">\(b &lt; 0\)</span>. If <span class="math inline">\(b = 0\)</span>, then <span class="math inline">\(r_{xy} = 0\)</span>.</p>
<p>Since we're assuming our dataset is a sample of the population rather than the population itself, we can only find evidence of associations rather than actual associations themselves. Even if the evidence is very strong, it doesn't say for sure that the association is there. Basically, correlation doesn't imply causation.</p>
<h1 id="section-3">11/5/16</h1>
<p>The reason we have supparies is to figure out the shape of a dataset, and see if we can identify it as following a certain distribution. obtaining the distribution allows us to make predictions about future observations and other useful statistical things.</p>
<p>The <strong>five number summary</strong> of a dataset is a common set of summaries: minimum, first quartile, median, third quartile, and, maximum. Basically, it gives 5 equally spaced points on the histogram.</p>
<p>Since <span class="math inline">\(\sum (x_i - \overline x)(y_i - \overline y) = \sum x_i y_i - n \overline x \overline y\)</span> and <span class="math inline">\(\sum (x_i - \overline x)\)</span>, we can write the formula for the correlation coefficient more simply as <span class="math inline">\(\frac{\sum x_i y_i - n \overline x \overline y}{s_x s_y}\)</span>.</p>
<h3 id="graphical-summaries">Graphical Summaries</h3>
<p>One of the most commonly used graphical summaries is the histogram. This is generally used when we have data that can be grouped/binned (organized into disjoint, ordered sets). By &quot;histogram&quot;, most people actually mean a <strong>frequency histogram</strong> - a bar plot where the X axis is the bins, and the Y axis is the frequency of observations in each bin.</p>
<p>In contrast, statisticians usually mean <strong>density histograms</strong> when talking about histograms. This is the same thing, except the Y axis is the density of the bin.</p>
<p>The bins in a density histogram don't all have to all have the same range - one can encompass 30 elements, while another might encompass 10. On the plot, the bars don't all have to have the same width. The height of each bar is chosen such that the area of the bar (width of the bin times the height) is equal to the relative frequency of observations that fall into its corresponding bin. The relative frequency is simply <span class="math inline">\(\frac{\text{number of observations in the bin}}{\text{total number of observations}}\)</span>.</p>
<p>Note that the total area underneath the density histogram is always 1. Since the total area under a probability density function is also 1, the density histogram is very useful for comparing data with known probability density functions, which we do very often when trying to figure out if a dataset follows a certain distribution. We can't use the usual frequency histogram for this because the total area of the bars doesn't add up to 1.</p>
<p>A <strong>box-and-whiskers plot</strong> is a useful way to show the five number summary that also highlights outliers. It's a 1D plot that looks something like this:</p>
<pre><code>  o   &lt;- each outlier is plotted as a point, while non-outliers are only summarized by the box and whiskers

----- &lt;- top whisker represents maximum value that isn&#39;t an outlier
  |
----- &lt;- top of box represents third quartile
|   |
|---| &lt;- line in the box represents median
|   |
----- &lt;- bottom of box represents first quartile
  |
----- &lt;- bottom whisker represents minimum value that isn&#39;t an outlier

      &lt;- there might not be any outliers in the dataset</code></pre>
<p>An <strong>outlier</strong> is an observation <span class="math inline">\(x_i\)</span> such that <span class="math inline">\(x_i &gt; Q_3 + 1.5 IQR\)</span> or <span class="math inline">\(x_i &lt; Q_1 - 1.5 IQR\)</span>, where <span class="math inline">\(Q_1, Q_3\)</span> are the first and third quartiles, and <span class="math inline">\(IQR\)</span> is the interquartile range <span class="math inline">\(Q_3 - Q_1\)</span>. The reason we use 1.5 is because it is convention, and fits the normal distribution relatively well. In other words, an outlier is any point that doesn't fall into the range <span class="math inline">\([2.5Q_1 - 1.5Q_3, 2.5Q_3 - 1.5Q_1)]\)</span>. When we're drawing a box plot, we figure out the quartiles, figure out the ranges in which observations are outliers, and then find the max/min observations that aren't outliers.</p>
<p>Box plots visually show the shape of the data. A box with whiskers that are different lengths is skewed to one side, while the height of the box shows the dispersion.</p>
<p>The <strong>empirical cumulative distribution function</strong> (empirical CDF) for discrete numerical data is a plot where the X axis is the range of possible observation values, and the Y axis is the number of observations that are less than or equal to that value. So for a dataset <span class="math inline">\(x = \set{x_1, \ldots, x_n}\)</span>, and <span class="math inline">\(F(x_i) = \abs{\set{v \in x \middle| v \le x_i}}\)</span>, the empirical CDF is a plot of <span class="math inline">\((x_i, F(x_i))\)</span>.</p>
<p>The advantage of the empirical CDF is that the percentile values are directly shown on the plot - to find the <span class="math inline">\(n\)</span>th percentile, we find the X axis value such that the Y axis is <span class="math inline">\(n\)</span> percent of the maximum Y axis value. For example, to find the median, we would find the Y axis . Also, the mode is the bar that has the largest increase in height compared to the bar on its left.</p>
<p>The Q-Q plot is used for checking if a dataset resembles a normal distribution. This is a plot of the <span class="math inline">\(\alpha\)</span>th quantile of the dataset and the <span class="math inline">\(\alpha\)</span>th quantile of the Z distribution <span class="math inline">\(N(0, 1)\)</span> (normal distribution with mean 0 and variance 0), for all <span class="math inline">\(\alpha\)</span>.</p>
<p>So the median of the dataset is plotted at the X axis value that is the median of the Z distribution, the 95th percentile of the dataset is plotted at the X axis value that is the 95th percentile of the Z distribution, and so on. Basically, this is a plot of <span class="math inline">\((p \text{th percentile of the Z distribution}, p \text{th percentile of the dataset})\)</span> for all <span class="math inline">\(0 \le p \le 100\)</span>. The X axis goes on infinitely in both directions.</p>
<p>If the Q-Q plot resembles a straight line, the dataset resembles a normal distribution. For example, for a normally distributed dataset, it would always be the case that the <span class="math inline">\(p\)</span>th percentile of the dataset be a linear function of the <span class="math inline">\(p\)</span>th percentile of the Z distribution, so all <span class="math inline">\((p \text{th percentile of the Z distribution}, p \text{th percentile of the dataset})\)</span> would lie along the same line.</p>
<p>A <strong>scatter plot</strong> plots two variables against each other, where the X axis is one variable and the Y axis is the other, and points are plotted for each observation. The scatter plot is great for finding patterns in the data, like correlations, grouping, and so on.</p>
<h1 id="section-4">16/5/16</h1>
<p>Quiz about numerical and graphical summaries, I'm writing in DC1351 on Thursday at 3:30PM, for just under 1 hour.</p>
<h2 id="statistical-analysis">Statistical Analysis</h2>
<p>A common statistical problem is &quot;Given a population of observations, some of the characteristics of which are unknown, and a sample taken from that population, what can we say about the population by looking just at the sample?&quot;.</p>
<p>The PPDAC approach is a sort of template for solving this type of problem: problem, plan, daa, analysis, and conclusion. It's specific to statistics at Waterloo.</p>
<ul>
<li>The <strong>problem</strong> can be descriptive (what properties of the population are we interested in?), causative (does X cause Y? how are they related?), and predictive (what will be the result of doing X?).
<ul>
<li>We should also identify and explicitly state the population of interest. A <strong>unit</strong> is a member of the target population, and a <strong>variate</strong> is a characteristic of a unit. An <strong>attribute</strong> is a function over the variates of a collection of units.</li>
<li>For example, for finding the presidential approval rating, the target population is the voting population, a unit is any particular voter, a variate is whether they approve of the president, and the proportion of approvals is the attribute.</li>
</ul></li>
<li>The <strong>plan</strong> can be experimental (variables are chosen and controlled when making observations) or observational (variables can't be controlled when making observations).
<ul>
<li>We first choose a study population from which we draw the sample.</li>
<li>For the presidental approval rating example, we might choose the subset of voters that have a phone (so we can call them to ask whether they approve).</li>
<li>For a new drug being tested, the study population might be a collection of mice, while the target population is all humans - the study population doesn't have to be a subset of the target population.</li>
</ul></li>
<li>The <strong>analysis</strong> involves setting up a statistical model.
<ul>
<li>That means we assume (after obtaining sufficient evidence to back it up) that the data follows some known distribution, possibly with unknown parameters.</li>
<li>For the presidental approval rating example, since we can probably assume that the voters are independently making decisions, and that approval is a binary result, we can probably use a binomial distribution.</li>
<li>A <strong>bias</strong> is a systematic error in the data. Bias is often caused by measurement error, or selection bias (for example, people only ask for exam remarks if they got less than they feel they deserved, so marking errors might be systematically higher than the true value).</li>
<li>When distribution parameters are unknown, we often use Greek letters like <span class="math inline">\(\mu, \sigma, \pi\)</span>. When we have sample parameters, which we do know, we often use English letters like <span class="math inline">\(\overline y, s^2, p\)</span>, or Greek letters with hats, like <span class="math inline">\(\hat, \mu, \hat \sigma, \hat pi\)</span>.</li>
<li>The <strong>study error</strong> is the difference between the target population mean and the study population mean for the attribute under study: <span class="math inline">\(\mu_1 - \mu_2\)</span>.</li>
<li>The <strong>sampling error</strong> is the difference between the sample mean and the study population mean for the attribute under study: <span class="math inline">\(\overline y - \mu_2\)</span>.</li>
<li>When we're picking a sample, we really want it to represent the population - the properties of the population should be likely to be similar to the properties of the population, to reduce the sampling error as much as possible. We do this by doing <strong>random sampling</strong>. Correctly doing random sampling is critical in avoiding biases, but often depends on characteristics of the problem.</li>
</ul></li>
<li>The <strong>conclusion</strong> is a statement that should be understandable by non-experts.</li>
</ul>
<h3 id="estimation">Estimation</h3>
<p>The <strong>method of maximum likelihood</strong> is finding th most likely value of an unknown parameter of a statistical model, based on a sample. For example:</p>
<blockquote>
<p>We have a coin that either has a 25% chance of getting heads, or a 75% chance of getting heads.<br />
Suppose that we flipped the coin 100 times and got 30 heads. Obviously, we would say that it's most likely that the coin has a 25% chance of getting heads.<br />
Mentally, it seemed like we asked &quot;what is the probability of 30 heads occuring, if it were the case that the coin had a 25% chance of getting heads?&quot; and &quot;what would that probability be if it were the case that the coin had a 75% chance of getting heads instead?&quot;, and then chose the option that maximizes the probability of the event occuring.</p>
</blockquote>
<p>Essentially, the method of maximum likelihood is picking the parameter that makes our observed sample properties most likely.</p>
<p>Formally, given a discrete distribution <span class="math inline">\(Y\)</span> with probability function <span class="math inline">\(f\)</span> and unknown parameter <span class="math inline">\(\theta\)</span>, then the <strong>likelihood function</strong> is defined as <span class="math inline">\(L(\theta; y_1, \ldots, y_n) = P(Y_1 = y_1, \ldots, Y_n = y_n) \text{ for the given value of } \theta\)</span>.</p>
<p>We then find the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function. This value is the <strong>maximum likelihood estimate</strong> (MLE), denoted <span class="math inline">\(\hat \theta\)</span>.</p>
<p>Suppose we have a variable <span class="math inline">\(X \tilde \mathrm{Poisson}(\mu)\)</span>. What is the MLE of <span class="math inline">\(\mu\)</span>?</p>
<blockquote>
<p>Clearly, the probability of each sample observation is <span class="math inline">\(\frac{e^{-\mu} \mu^r}{r!}\)</span> where <span class="math inline">\(r\)</span> is the value of the observation, by the definition of the Poisson distribution.<br />
Therefore, <span class="math inline">\(P(Y_1 = y_1, \ldots, Y_n = y_n) = \frac{e^{-\mu} \mu^{x_1}}{x_1!} \cdots \frac{e^{-\mu} \mu^{x_n}}{x_n!}\)</span> (we can just multiply the probabilities together because all of the observations are independent, which we know is the case since we're using the Poisson distribution).<br />
We can find the value of <span class="math inline">\(\mu\)</span> that maximizes the likelihood function by taking the logarithm of the whole thing, then finding where the derivative of that is 0. This is a pretty common way to find the max.<br />
This value of <span class="math inline">\(\mu\)</span> is the maximum likelihood estimate.</p>
</blockquote>
<h1 id="section-5">18/5/16</h1>
<p>The midterm covers everything up to next week.</p>
<p>Most statistical inference problems start with parameter estimation. We generally start with a guess, and then try to refine it using samples.</p>
<p>A sample <span class="math inline">\(y\)</span> can be thought of as a a set of outcomes of a random variable <span class="math inline">\(Y\)</span>, rather than just a set of numbers. This random variable is the <strong>statistical model</strong>.</p>
<p>To review, lowercase Greek letters like <span class="math inline">\(\mu, \theta\)</span> are unknown parameters, lowercase Latin letters like <span class="math inline">\(x, y\)</span> are known/sample parameters, and uppercase Latin letters like <span class="math inline">\(X, Y\)</span> are random variables. These letters correspond: <span class="math inline">\(y_i\)</span> is a single outcome drawn from the random variable <span class="math inline">\(Y_i\)</span>.</p>
<p>Likewise, <span class="math inline">\(\overline y\)</span>, the sample mean can be thought of as a single outcome of a random variable <span class="math inline">\(\overline Y\)</span>. As a result, with multiple samples, we can get a whole distribution of different <span class="math inline">\(\overline y\)</span> values. The unknown population mean is <span class="math inline">\(\mu\)</span>, while <span class="math inline">\(\overline Y\)</span> is the random variable with the distribution of all the different <span class="math inline">\(\overline y\)</span> values.</p>
<p>These random variables, like <span class="math inline">\(\overline Y, S^2, \overtilde \pi\)</span> are <strong>estimators</strong>. These sample values, like <span class="math inline">\(\overline y, s^2, \hat \pi\)</span> are <strong>estimates</strong>.</p>
<p>Estimate the approval rating of the president:</p>
<blockquote>
<p>The approval rating is <span class="math inline">\(\pi\)</span>, a population parameter that we don't know. Suppose we interview 10 people and get <span class="math inline">\(\set{A, A, D, D, D, D, A, D, D, D}\)</span>, where <span class="math inline">\(A\)</span> means approval and <span class="math inline">\(D\)</span> means disapproval.<br />
We want to find <span class="math inline">\(\hat \pi\)</span>, the MLE of <span class="math inline">\(\pi\)</span>. Clearly, <span class="math inline">\(L(\pi; y_1, \ldots, y_n) = \pi \pi (1 - \pi) (1 - \pi) (1 - \pi) (1 - \pi) \pi (1 - \pi) (1 - \pi) (1 - \pi)\)</span>, because the probability of each person approving is just <span class="math inline">\(\pi\)</span>.<br />
So <span class="math inline">\(L(\pi; y_1, \ldots, y_n) = \pi^3 (1 - \pi)^7\)</span>, and the log likelihood function is <span class="math inline">\(l(\pi) = \ln(\pi^3 (1 - \pi)^7) = 3 \ln \pi + 7 \ln(1 - \pi)\)</span>.<br />
Clearly, the value of <span class="math inline">\(\pi\)</span> that maximizes <span class="math inline">\(l(\pi)\)</span> is <span class="math inline">\(\frac{3}{10}\)</span> (by taking the derivative of the log likelihood function). Therefore, the MLE for the approval rating is <span class="math inline">\(\frac{3}{10}\)</span>.</p>
</blockquote>
<p>Estimate the average website hits per hour, given a sample of visits over <span class="math inline">\(n\)</span> hours <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span>:</p>
<blockquote>
<p>Assume that the data is from a Poisson distribution. Then we want to figure out the distribution parameter <span class="math inline">\(\mu\)</span>.<br />
Construct the likelihood function <span class="math inline">\(L(\mu; y_1, \ldots, y_n) = \frac{e^{-\mu} \mu^{y_1}}{y_1!} \cdot \ldots \cdot \frac{e^{-\mu} \mu^{y_n}}{y_n!}\)</span>.<br />
So <span class="math inline">\(L(\mu; y_1, \ldots, y_n) = \frac{e^{-n\mu} \mu^{\sum y_i}}{y_1! \ldots y_n!}\)</span>.<br />
So the log likelihood function is <span class="math inline">\(l(\mu) = -n \mu + \sum (y_i \ln \mu) - \ln(y_1! \ldots y_n!)\)</span>.<br />
Differentiating the log likelihood function with respect to <span class="math inline">\(\mu\)</span>, we get <span class="math inline">\(-n + \sum \frac{y_i}{\mu}\)</span>.<br />
By setting the derivative to 0 and solving for <span class="math inline">\(\mu\)</span>, we find the maximum value of the log likelihood function at <span class="math inline">\(\hat \mu = \frac 1 n \sum y_i = \overline y\)</span>.<br />
So if we have a Poisson distribution, the best guess for the parameter <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\overline y\)</span>.</p>
</blockquote>
<p>Estimate the probability that a Canadian contestant wins Jeopardy, given a sample of shows Canadian contestants appeared in <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span>:</p>
<blockquote>
<p>Clearly, if a contestant appears in <span class="math inline">\(n\)</span> shows, then they won <span class="math inline">\(n\)</span> in a row and lost in the <span class="math inline">\(n + 1\)</span>th show.<br />
We assume that shows are independent. Therefore, we have a geometric distribution, <span class="math inline">\(Y \tilde \mathrm{Geo}(\pi)\)</span>.<br />
Recall that for a geometric distribution, <span class="math inline">\(P(Y = y) = \pi^{y - 1}(1 - \pi)\)</span>.<br />
So <span class="math inline">\(L(\pi; y_1, \ldots, y_n) = \pi^{y_1 - 1}(1 - \pi) \ldots \pi^{y_n - 1}(1 - \pi) = \pi^{\sum y_i - n} (1 - \pi)^n\)</span>.<br />
So <span class="math inline">\(l(\pi) = (\sum y_i - n) \ln x + n \ln (1 - x)\)</span>. Using the usual methods, we find the value of <span class="math inline">\(\pi\)</span> that maximizes <span class="math inline">\(l(\pi)\)</span> to get the MLE of <span class="math inline">\(\pi\)</span>.</p>
</blockquote>
<p>Estimate the proportion of left handers in this university:</p>
<blockquote>
<p>Strategy 1: Suppose we got our data by asking people from the population until we got a 10 left handers, and we needed to ask 100 before before there were 10.<br />
Strategy 2: Suppose we got our data by asking 100 people from the population, and 10 of them turned out to be left handers.<br />
Let <span class="math inline">\(\pi\)</span> be the proportion of left handers. In strategy 1, we have a negative binomial distribution.<br />
So in strategy 1, we asked 99 people and got either left or right handed, and the 100th person answered left handed. So the likelihood function is <span class="math inline">\(L(\pi) = {99 \choose 9} \pi^9 (1 - \pi)^{90} \times \pi\)</span>.<br />
So in strategy 2, we asked all 100 people, and got either left or right handed. So the likelihood function is <span class="math inline">\(L(\pi) = {100 \choose 10} \pi^{10} (1 - \pi)^{90}\)</span>.<br />
As it turns out, both strategies give the same MLE for <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\frac{1}{10}\)</span>.</p>
</blockquote>
<p>Note that when we're maximising value of a parameter by taking the derivative, setting it to 0, and solving for the parameter, we have to make sure that the solution is actually the global maximum, not a local maximum or even a minimum. When there are multiple solutions, we must check each of them to find the one that results in the largest likelihood value.</p>
<p>Note that in this course, there will only ever be one solution to the derivative being equal to 0, so we won't have to worry about it.</p>
<p>When we have lots of independent observations, the likelihood function will look like <span class="math inline">\(P(Y_1 = y_1) \cdot \ldots \cdot P(Y_n = y_n)\)</span>. So if <span class="math inline">\(n\)</span> is large, we're multiplying a lot of numbers between 0 and 1, which means that the actual likelihood is going to be very small. To deal with this, we sometimes use the <strong>relative likelihood function</strong> <span class="math inline">\(R(\theta) = \frac{L(\theta)}{L(\hat \theta)}\)</span>, which always has a maximum of 1 at <span class="math inline">\(\hat \theta\)</span>.</p>
<p>Note that this only works for discrete distributions. For continuous ones, we have other tools.</p>
<h1 id="section-6">25/5/16</h1>
<p>;wip: missed due to interviews</p>
<h1 id="section-7">30/5/16</h1>
<p>While the likelihood interval results in a likelihood interval, the sampling distribution gives us a confidence interval.</p>
<p>The theory of estimation deals with problems of the form &quot;estimate an unknown population attribute <span class="math inline">\(\theta\)</span> given a sample drawn from the population, <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span> to get the estimate, <span class="math inline">\(\hat \theta(y_1, \ldots, y_n)\)</span>&quot;. So far, we've looked at the maximum likelihood estimation technique for solving these for discrete distributions.</p>
<p>Another one is the <strong>method of least squares</strong>, where we find the value of <span class="math inline">\(\theta\)</span> that minimises some squared error function we define - minimizing <span class="math inline">\(\sum E(y_i, \theta)^2\)</span>. If <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\mu\)</span> in a normal distribution, then this works out to just be the sample mean, since the error is <span class="math inline">\(y_i - \mu\)</span> and we're minizing (y_i - )^2$. This is not the best method, but it's very popular becuase it's so simple.</p>
<p>So far, when we're estimating we've been trying to get a single value as the estimate. However, it's often more useful to get an interval estimate instead - to find an interval <span class="math inline">\([A, B]\)</span> such that <span class="math inline">\(\theta\)</span> has at least a given probability of being within this interval. This type of interval problem often shows up as determining margins of error.</p>
<p>One way to obtain an interval estimate is via a relative likelihood function. Unlike the MLE method, where we find the most likely value, we're finding all values of a relative likelihood that exceed a certain probability threshold.</p>
<p>Basically, a <span class="math inline">\(p\)</span>-percent likelihood interval for an unknown parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\set{\theta \middle| R(\theta) \ge p}\)</span>, where <span class="math inline">\(R(\theta) = \frac{L(\theta)}{L(\hat \theta)}\)</span> is the relative likelihood function and <span class="math inline">\(\hat \theta\)</span> is the MLE. The lowest and highest values in the resulting set is then the likelihood interval. The likelihood interval itself, though, is a set of all values of <span class="math inline">\(\theta\)</span> that satisfy the criteria.</p>
<p>All <span class="math inline">\(p\)</span>-percent likelihood intervals are subsets of <span class="math inline">\(q\)</span>-percent intervals if <span class="math inline">\(q \le p\)</span> - smaller percentage likelihood intervals are subsets of higher percentage likelihood itnervals. The MLE belongs to all likelihood intervals. The higher the value of <span class="math inline">\(p\)</span>, the higher the probability that the true value doesn't fall into the interval. Likelihood intervals are not easy to interpret and don't have an intuitive meaning, so we generally want to use something else.</p>
<p>The <strong>method of sampling distributions</strong> is a better way to estimate intervals. An <span class="math inline">\(n\)</span>-percent confidence interval means that we're <span class="math inline">\(n\)</span> percent sure that the value falls withint this interval, which is a lot more intuitive. Given a sample <span class="math inline">\(\set{y_1, \ldots, y_n}\)</span> and a confidence threshold <span class="math inline">\(p\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Identify the pivotal distribution, from the model.</li>
<li>Find the endpoints of the pivotal distribution that exceed the confidence threshold <span class="math inline">\(p\)</span>.</li>
<li>Rearrange the endpoints to construct a coverage interval.</li>
<li>Estimate the coverage interval using the sample to get a confidence interval.</li>
</ol>
<p>;wip: for a normal dist the coverage interval is <span class="math inline">\((\overline Y - Z^* \frac{\sigma}{\sqrt{n}}, \overline Y + Z^* \frac{\sigma}{\sqrt{n}})\)</span>, and the confidence interval is <span class="math inline">\((\overline y - Z^* \frac{\sigma}{\sqrt{n}}, \overline y + Z^* \frac{\sigma}{\sqrt{n}})\)</span></p>
<p>The confidence interval tells us the probability that the true value falls within a certain range. However, we can also use them to say &quot;the true value must be at most <span class="math inline">\(A\)</span> from our estimate&quot; by choosing our sample size <span class="math inline">\(n\)</span> such that <span class="math inline">\(A = \frac{Z^* \sigma}{\sqrt{n}}\)</span>.</p>
<p>The problem with confidence intervals is if we don't know <span class="math inline">\(\sigma\)</span> or if the population isn't normal.</p>
<p>From the central limit theorem, if <span class="math inline">\(n\)</span> is large, and <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are arbitrary distributions with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then <span class="math inline">\(\overline Y \tilde N(\mu, \frac{\sigma^2}{n})\)</span>.</p>
<h1 id="section-8">1/6/16</h1>
<p>;wip: missed first half due to interviews</p>
<p>Continuing with multiple examples of choosing sample sizes such that we have a certain margin of error - making the confidence interval at least a certain size. Interestingly, to get a margin of error of 3% in any population, all we need to do is ask around 1000 people, regardless of how large the population is.</p>
<h1 id="section-9">5/6/16</h1>
<p>I dropped this course due to my injuries, as it became infeasible to continue going to classes.</p>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Anthony Zhang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2014 Anthony Zhang.
</div>
<script type="text/javascript">
MathJax.Hub.Config({
  jax: ["input/TeX","output/HTML-CSS"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
</body>
</html>