CO487
=====

Applied cryptography.

    Alfred Menezes
    Section 001
    Email: ajmeneze@uwaterloo.ca
    Office hours: Mondays 3:00pm-5:00pm, Fridays 1:00pm-3:00pm in MC 5026
    Mondays/Wednesdays/Fridays 11:30pm-12:20pm

# 4/3/16

All course resources are on LEARN. Course has 5 assignments (summing up to 20% of final grade), midterm (worth 30%), and a final exam (worth 50%). Midterm is March 8, 2017 at 7-9pm.

Cryptography is a tool we can use to secure communicatoins when there are malicious adversaries. Some of the fundaental goals of cyptography are:

* Confidentiality - data is secret to all people except authoeized parties.
* Data Integrity - data is unaltered.
* Origin authentication - data can be confirmed to be from a particular source.
* Non-repudiation - once a statement is published, it is not possible to deny/revoke it later.

Different applications might require different subsets of these goals.

In this course, we use Bob and Alice as placeholders to represent two parties trying to communicate securely. We use Eve or Mallory as a placeholder to represent one or more adversaries trying to attack the communications (by injecting data, eavesdropping, or other approaches, depending on the threat model)

Some famous early practical uses of cryptography was the enigma machine, which used multiple spinning rotors to encrypt/decrypt messages. In that cryptosystem, the German military leadership and the German U-boats had the role of Bob and Alice, trying to communicate confidentially, and Alan Turing and his team had the role of Eve, trying to break the confidentiality guarantees and read those communications.

However, cryptosystems like Enigma and its successor, Lorenz, are a far cry from modern cryptosystems, which have a much more mathematically sound foundation. Modern cryptography is what makes online banking, online shopping, and cellular networks possible.

For example, SSL, the protocol that makes online security possible, ensures origin authentication and confidentiality between a user and a website. SSL uses symmetric-key encryption to implement confidentiality, and a MAC scheme (HMAC) to implement origin authentication. Of course, to make this possible they both need to have the same secret key for the symmetric-key cryptography, and for the MAC. To confidentially and authentically share this secret key, we use public-key encryption. Of course, to make this possible we need a way to obtain authentic copies of the public keys of both parties - this is implemented using digital signatures, where trusted third parties known as certificate authorities keep track of public keys that are considered authentic (generally by vetting the site owner in the real world), and digitally sign them to confirm that they're authentic. The certificate authorities' public keys are pre-installed in the browser, which acts as a root of trust - if you trust that public key is authentic, and that the certificate authorities are correctly keeping track of authentic .

SSL is one of the most successful cryptosystems ever deployed, used for tons of sites on the internet. However, there are a number of possible weaknesses that might result in the guarantees being broken:

* The crpyotgraphy itself might be weak (e.g., previously, SSL supportied using RC4 for the symmetric-key encryption, which is nowadays easy to break).
* Quantum computers can attack public-key encryption systems like RSA.
* Random number generators used in the protocol might have weaknesses (e.g., Netscape's flawed RNG, NSA's backdoored EC-DRBG CSPRNG).
* Certificate authorities (e.g., social engineering resulting in Verisign digitally signing non-authentic certificates).
* Bugs in crpytographic code (e.g., Heartbleed)
* Misuse of cryptography, like encrypting the wrong thing.
* Phishing/social engineering attacks on users themselves.
* The server itself leaking data - SSL only protects data in transit, not when it's on the server.

This demonstrates some useful concepts:

* Symmetric-key encryption is used to ensure confidentiality.
* MAC schemes are used to ensure authentication.
* Public-key encryption is used to ensure confidentiality, integrity, authentication, and non-repudiation.
* Digital signatures is used to ensure integrity, authentication, and non-repudiation.

Cryptography is only one part of a large information security ecosystem, including other things like secure operating systems, auditing mechanisms, trusted computing, and risk analysis. In this context, cryptography provides a lot of useful and essential tools, but it's not all there is to security. Attackers will generally target the weakest part of the system, and if that link fails, it is possible that the entire system will.

This course focuses on breadth over depth. For depth, take CO 485 and try out the readings for this course as well.

# 6/1/17

SSL in more detail:

1. Client makes request to the website  (not the full URL, just the host).
2. Server responds with the website's certificate, which contains the website identification info (like host and etc.).

Symmetric-key encryption
------------------------

A **symmetric key encryption scheme** (SKES) is a definition containing:

* A plaintext space $M$.
* A ciphertext space $C$.
* A key space $K$.
* A family of encryption functions $E_k: M \to C, \forall k \in K$.
* A family of decryption functions $D_k: C \to M, \forall k \in K$.

To use one of these to implement message confidentiality, Alice and Bob agree on a particular $k \in K$ over a secure channel (so that nobody else knows about the value of $k$). Then, Alice can compute $c = E_k(m)$, where $m$ is the message, and then sends $c$ to Bob over an unsecure channel. Bob can read the message by then computing $m = D_k(c)$, while anyone without $k$ would have to figure out the correct $D_k$ function to use. An SKES can be designed such that finding the correct $D_k$ would be very difficult.

A substitution cipher is an SKES, under this definition: $M$ is the set of all English messages, $C$ is the set of all encrypted messages, $K$ is the set of all permutations of the English alphabet, $E_k$ maps letters onto $k$ by index, and $D_k$ does the inverse mapping.

A **security model** defines what the adversay is capable of, like what they can do to the communicating parties. For example, there are passive attacks like **ciphertext attacks** (attacker can obtain ciphertext, like if they're listening on the same network), **known-plaintext attacks** (attacker knows some of the plaintext as well the resulting ciphertext, like knowing that someone always signs off their message with their name). There are also active attacks, like **chosen-plaintext attacks** (attacker can choose some part of the plaintext), **clandestine attacks** (attacker is willing to use bribery/blackmail/etc.), and **side-channel attacks** (power analysis, RF emissions analysis, timing attacks).

The security model also includes the attacker's computational abilities. For example, information-theoretic security models assume the attacker has infinite computational resources, complexity-theoretic security models assume the attacker has a turing machine capable of computing any polynomial-time algorithm efficiently, and computational-theoretic security models assume the attacker simply has a lot of computers available.

The attacker's goals, in decreasing order of priority: recovering the secret key, recovering plaintext from ciphertext (without the key), or learn some characteristics of the plaintext given ciphertext (besides message length).

An SKES is **secure** if and only if it is resistant to a chosen-plaintext attack by a computationally bounded adversary (computational-theoretic security). A secure SKES is necessary but not sufficient to guarantee confidentiality.

Additionally, we also assume that the adversaries know everything about the cryptosystem except the plaintext and the key, including the all the algorithms and communicatoin mechanisms.

An SKES should ideally be efficient (for encryption/decryption), have small keys (but not so small that it makes brute force attacks possible), and be secure (even against the designer of the SKES).

A **work factor** measures how hard a task is, in terms of how many computations are needed to complete it: $2^{56}$ operations is easy, $2^{64}$ is feasible, $2^{80}$ operations is barely feasible, and $2^{128}$ operations is infeasible. This will change as our computers get faster. For example, the entire bitcoin network is doing $2^{61}$ hashes per second. However, we use $2^{128}$ operations as infeasible because the Landauer limit tells us doing that many operations on a classical computer would take a significant fraction of the world's power output for a year.

For example, a substitution cipher is not secure, because it's trivially broken by a chosen-plaintext attack. By choosing the plaintext as the alphabet, the ciphertext is just the secret key. In fact, it's not even secure against ciphertext-only messages. Although it's infeasible to exhaustively try every permutation of the alphabet for valid-looking decrypted messages, we can simply use letter frequency analysis to try likely candidates first.

A polyalphabetic cipher uses multiple alphabet permutations for substitution, choosing between them with a particular algorithm. For example, a Vigenere cipher has a word with non-repeated letters added to the message's letters mod 26. This is nice because as the message grows longer, the letter frequency graph is a bit flatter. We can break that by using a chosen-plaintext attack with plaintext "AAAAAAAAAAAAAAAAAA", so if "A" corresponds to the numerical value 0, the ciphertext is just the key. Therefore, the Vigenere cipher is not secure.

From now on, we assume plaintext, keys, and ciphertext are all binary strings. What happens when we apply the concept of a Vignere cipher, but with a uniformly random key that's as long as the entire message?

A **one time pad** XORs a random key with the plaintext (the key is as long as the message). However, it's important to not reuse keys, because $c_1 = m_1 \xor k_1$ and $c_2 = m_2 \xor k_2$ implies that $c_1 \xor c_2 = m_1 \xor m_2$, which leaks a lot of information about the plaintext. If the key is uniformly randomly selected, every key is equally likely, so every ciphertext is also equally likely. This is information-theoretically secure - a one-time pad provably cannot be broken by a ciphertext-only attack even by attackers with infinite computational resources. While one-time pads have these nice properties, in practice they are hard to use because the key has to be as long as the ciphertext, making sharing keys a pain.

A **stream cipher** is like a one-time pad, but instead of a truly random key, we use a pseudorandom bit generator, where the PRBG's seed is the secret key. This is no longer perfectly secure because it depends on the quality of the randomness of the PRBG, but it's often a practical compromise since the key can be a lot shorter. Like with a one-time pad, we also shouldn't re-use keys, since that would give us some of the values of the PRBG's output by XORing the ciphertexts, which would make it easier to learn the PRBG's state. One example of a stream cipher is RC4.

# 9/1/17

The PRBG should satisfy two requirements:

* Indistinguishability requirement - the output should be indistinguishable from a random sequence.
* Unpredictability requirement - the output should not be predictable even if some of the previous outputs are known.

Most random number generators built into programming languages are not cryptographically secure - they're only intended to satisfy the indistinguishability requirement. For example, `rand` in UNIX uses a linear congruential generator with known constants, which is straightforward to predict.

RC4 was designed by Rivest, the R in RSA. It was used in everything from SSL/TLS to Adobe Acrobat, as one of the most popular stream ciphers around. It's nice because it's very fast and has variable key sizes, but was proprietary for a long time has a lot of weaknesses, even though none of them are catastrophic. It consists of a key scheduling algorithm, and a keystream generator.

The key scheduling algorithm generates a random-looking permutation of $0, \ldots, 255$. It starts by initializing $S$ with $0, \ldots, 255$, and $\overline{K}$ to the key repeated over and over until it fills 256 entries in the array. Then, the array entries in $S$ are swapped based on $j_{i + 1} = (S[i] + \overline{K} + j_i) \mod 256$. The keystream generator just applies that permutation to generate the keystream, which is then XORed with the plaintext to get the ciphertext.

Wireless networks lose the physical security of wired networks, and security for those networks is a lot harder because attackers can do it from a distance with no physical evidence. The original WiFi standard, IEEE 802.11, includes the Wireless Equivalent Privacy (WEP) protocol for protecting link-level data in transit between clients and access points. WEP was intended to provide confidentiality using RC4, data integrity using a checksum, and access control by rejecting improperly encrypted packets.

In WEP, the client first shares a 40-bit or 104-bit key with the access point (this was because the US classified cryptography as munitions, disallowing most cryptography with keys greater than 40 bits for export; and keys greater than 104 bits for domestic use). Messages are then divided into fixed-size packets, which are then each encrypted with a per-packet initialization vector (IV). The issue is that WEP didn't specify certain things, like how the key should be distributed, and how IVs should be managed.

Implementations ended up using one shared key per LAN, infrequently changed, and just generating random IVs or using consecutive integers as IVs. To send a packet, a party would select a 24-bit IV, compute the CRC-32 checksum of the message, the checksum is then appended to the plaintext, and then XORed with the RC4 keystream to get the ciphertext, where the key is the IV the shared WEP key appended to the IV. The sender then transmits the IV and the ciphertext. The receiver then, having the IV and the WEP shared key, gets gets the plaintext concatenated with the checksum, and then verifies that the checksum is correct, rejecting the packet if the checksum doesn't validate.

Turns out, WEP implements none of its goals. One problem is IV collision - since the IV is only 24-bits, the birthday paradox means that only about $2^{12}$ packets are needed for a collision. If two packets have the same IV, then $c_1 \lxor c_2 = m_1 \lxor m_2$ - the ciphertexts can be XORed to get the plaintexts, which can then be analyzed using known plaintexts or statistical analysis, so confidentiality is broken. Another problem is that the checksum is linear - we can make certain changes to the ciphertext while still ensuring that the checksum will verify correctly, so data integrity is broken. Finally, the checksum is unkeyed, so knowing the plaintext for one encrypted packet is enough to get the RC4 keystream and encrypt messages properly themselves.

Shamir and co. (the S in RSA) came up with an even better attack in 2001, based on the fact that 104-bit keys were infrequently changed, the IV is very predictable/randomly selected, and we know the first byte of the plaintext (the protocols add known headers to the plaintext before encrypting). With 5 million packets, the secret key itself can be recovered. Modern techniques can recover the key in just 40000 packets.

IEEE published updated wireless security standards. For example, WPA was a temporary replacement, and WPA2 came out afterwards, using AES instead of RC4, and designed to be much stronger.

# 11/1/17

Assignment 1 should be started now.

Attacks only get better over time, never worse. In WEPs case, it wasn't the security of the algorithms like RC4 that were broken, it was the use of those algorithms that was incorrect. In the future, thanks to upcoming advances related to Moore's law and quantum computing, currently secure cryptosystems could easily become broken.

A **block cipher** is the other common type of SKES. While a stream cipher encrypts one bit at a time, a block cipher breaks up the plaintext into fixed-length blocks, encrypting the message one block at a time. Some commonly known block ciphers are DES and AES.

DES has a 56-bit key length (which is now relatively easy to break), and a block size of 64 bits. AES (Rijndael, "rined'all") is its successor, and has a 128/192/256 bit key length. There are currently no significant known attacks on AES.

Clause Shannon gave a couple of principles for designing good block ciphers:

* Diffusion - each ciphertext bit should depend as many plaintext bits as possible.
* Confusion - there should be a complicated relatoinship between key and ciphertext bit.
* Key size - keys should be large enough to prevent exhaustive search.

Also, block ciphers should be fast, so we can use them in more applications.

A Feistel cipher is a class of ciphers, including DES. Feistel ciphers are parameterized based on $n$ (half of block size), $h$ (number of rounds), and $l$ (key size). It generates $h$ subkeys from the cipher key, one for each round. Each key is used to generate a component function $f_1, \ldots, f_h$ that "scrambles" its input.

Each block $m$ is then broken into two halves, $\tup{m_0, m_1}$. Then, we perform $h$ encryption rounds: at each round $i$, the right half gets moved into the left half, and the original left half is XORed with $f_i$ of the original right half, and that becomes the right half. After those $h$ rounds, the two halves are the ciphertext.

# 13/1/17

;wip: feistel ciphers

The New Data Seal cipher was invented by IBM as the predecessor to DES. It's a Feistel cipher, and is a relatively complicated one, but it also happens to be completely broken today to chosen-plaintext attacks. It has a block size of 64 bits, and uses 16 rounds, so $n = 64, h = 16$. Here's the basic idea:

1. Let $S_k: \set{0, 1}^8 \to \set{0, 1}^8$ be the secret key - an arbitrary function of a byte.
2. Let $f: \set{0, 1}^{64} \to \set{0, 1}^{64}$, the component function, be defined as follows:
    1. Since the input is 64-bits, let $z = \tup{z^1, \ldots, z^8}$ be the 8 bytes of the input.
    2. For each byte $z^j$, let $n_1^j = z^j[7:4]$ and $n_2^j = z^j[3:0]$ - the two nibbles of the byte.
    3. Let $t = S_k(z^1[7] \ldots z^8[7])$ - the key function applied to the byte obtained by taking the first bit of each byte in $z$.
    4. For each byte $z_j$, if bit $j$ of $t$ is 1, let $p_1^j = S_1(n_2^j)$ and $p_2^j = S_0(n_1^j)$, otherwise let $p_1^j = S_0(n_1^j)$ and $p_2^j = S_1(n_2^j)$. $S_0, S_1$ are functions defined by the NDS specification (that are too long to list here), and we're swapping the nibbles if the corresponding bit of the key function $S_k$ is true.
    5. Output $P(p_1^1 p_2^1 \ldots p_1^8 p_2^8)$ - a scrambling function $P$ applied to all of the scrambled and permuted nibbles concatenated together. $P$ is also a function defined by the NDS specification (that is too long to list here).
3. For each 16-byte block of the input $z = \tup{z^1, \ldots, z^8}$, run the Feistel cipher ladder with $f$ as the component function:
    1. Let $m_0, m_1$ be the two 8-byte halves of the 16-byte block.
    2. For 16 rounds $1 \le j \le 16$, set $m_j$ to $m_{j - 1}$ and $m_{j + 1}$ to $m_{j - 1} \lxor f$.
    3. Output $m_{16} m_{17}$.

In fact, we can recover the entire secret key from the ciphertext in a few hundred chosen plaintext attacks. The main issue is that there's no subkeys - each round used the same component function and secret key, rather than deriving unique component functions for each round - each $f_1, \ldots, f_h$ is the same function! Let $T$ denote the function that does one round of encryption, with regard to the secret key $S_k$. Basically, $T(\tup{m_{i - 1}, m_i}) = \tup{m_i, m_{i - 1} \xor f(m_i)}$ for some fixed function $f$ and the two halves of the message are $m_0, m_1$.

Clearly, applying $T(m)$ to the message 16 times is the full encryption function. Let's represent this as $T^{16}(m)$. Let $F = T^{16}$ - this is the full encryption function, since the cipher is just applying $T$ 16 times. Clearly, $T(F(m)) = T(T^{16}(m)) = T^{17}(m) = T^{16}(T(m)) = F(T(m))$ for any byte $m$.

Here's the attack. For every possible byte $r$, we want to determine $S_k(r)$. Select $u = (m_0, m_1)$ such that the byte formed by taking the first bit of each byte in $m_1$ is $r$, and $p_1^j \ne p_2^j$ for $1 \le j \le 8$ the scrambled nibbles aren't the same within each of the 8 bytes in $m_1$.

As an aside, $x^*$ seems to mean "the first bit of each byte in $x$".

By the rules of the chosen plaintext attack, we can get Bob to give us $\tup{a, b} = F(u)$, but not what $F(m)$ or $T(m)$ are (we know the values of the function at this point, but that's it).

However, we know that $T(F(u)) = F(T(u)) = \tup{b, \cdot}$. Since the value of $S_k(r)$ is only a byte, we can guess every possible value of $S_k(r)$, and check each guess $t$ by computing $T_t(u)$ and then get Bob to give us $F(T_t(u)) = \tup{c, d}$. Clearly, $b \ne c$ implies that $S_k(r) \ne t$, and $b = c$ implies that $S_k(r) = t$ is very very likely (there is a tiny chance of accidentally getting it right). This is because we're assuming that $F$ works roughly like a random permutation, so since $b$ and $c$ are 64 bits, $F(T_t(u))$ has only a $\frac 1 {2^{64}}$ probability of $b = c$ without $S_k(r) = t$, because we selected $u$ such that $p_1^j \ne p_2^j$. Eventually, we get every possible value of $S_k$.

Also, since we check every possible byte $r$ (256 values), and for each value of $r$, we guess-and-check possible bytes $t$ (on average, 128 checks, worst case 256), we should expect $128 \times 256 = 2^{15}$ chosen plaintexts on average before recovering the entirety of $S_k$.

The entire attack can be summarized as:

1. For each possible byte $r$:
    1. Find a byte $u = \tup{m_0, m_1}$ such that $m_1^* = r$ and $p_1^j \ne p_2^j, 1 \le j \le 8$.
    2. Get Bob to compute $F(u) = \tup{a, b}$.
    3. For each possible byte $t$:
        1. Compute $T_t(u)$.
        2. Get Bob to compute $F(T_t(u)) = \tup{c, d}$.
        3. If $b = c$, we know that $S_k(r) = t$ with overwhelming probability, and then go to the next $r$.
2. Now we have $S_k(r)$, and can implement the .

Basically, we need about $2^{15}$ chosen plaintexts to recover the entire secret key, which is quite feasible on today's computers.

# 16/1/17

Chosen plaintext attacks are not always feasible, but when they are they're very powerful. For example, consider a mail forwarder that encrypts incoming emails and forwards them to another destination.

DES is one of NDS' successors. Originally with 64-bit keys, the NSA weakened the final standard to 56 bits. DES is pretty weak compared to modern ciphers, but 3-DES is still commonly in use. The design principles are classified, but it's been heavily analyzed.

DES is a Feistel cipher with 16 rounds, a 64-bit block and a 56-bit key. For each round of the cipher, a 48-bit subkey is derived from the 56-bit secret key, by selecting 48 bits from the secret key. The plaintext halves are also expanded from 32-bits to 48-bits. After each Feistel round completes, we use 8 S-box functions (which are publicly known) to scramble the XORed result and reduce 48-bit results to 32-bits. This is the source of nonlinearity in DES, and is very important for security.

With a key space of size $2^{56}$, it's perfectly practical to just brute force today, even if we can't do it on our laptops just yet. A massively parallel effort broke a DES-encrypted message in just over 22 hours in 1999, in response to a challenge set by RSA security. Also, it's got a block size of 64-bits, which means that you'd expect a collision at around $2^{32}$ uniformly distributed blocks, thanks to the birthday paradox. So, on a busy network, if Alice and Bob are sending many blocks back and forth, Eve might observe a duplicated block, which tells us a little bit of information about the plaintext - those two corresponding blocks in the plaintext are identical (this isn't much of a problem in practice, but is still an issue).

Differential cryptoanalysis attacks can be used to recover the key in $2^{47}$ chosen plaintexts, though this is usually an infeasibly large number of chosen plaintexts in practice (turns out, DES was specifically designed to guard against this - the NSA was aware of the attack before it was public knowledge). Linear cryptoanalysis attacks can do the same thing in just $2^{43}$ chosen plaintexts, which is slightly more practical.

After these weakenings, 3-DES was invented as a more secure SKES in order to take advantage of existing DES hardware (DES was designed for hardware acceleration). Basically, it's just DES applied to DES applies to DES applied to the plaintext. Note that applying ciphers repeatedly doesn't always provide more security (consider a substitution cipher, for example), but in this case it provides reasonably good. Another variant is Double-DES, which is DES applied to DES applied to the plaintext (each DES function has its own key, so we have a 112-bit key overall).

Turns out Double-DES is vulnerable to a meet-in-the-middle attack. Basically, if $E_{k_1}, E_{k_2}$ are the two DES encryption functions in Double-DES and $c = E_{k_2}(E_{k_1}(m))$, then $E^{-1}(c) = E_{k_1}(m)$.

Suppose we have 3 known plaintext/ciphertext pairs $\tup{m_1, c_1}, \ldots, \tup{m_3, c_3}$. For each possible 56-bit $k_2$, decrypting $c_1$ with our guess for $k_2$, and store that plaintext and the key used in a table, indexed by the plaintext. Then, for each possible 56-bit $k_1$, try encrypting the $m_1$ with our guess for $k_1$, and see if it matches any entry in the table. If there's a match, and those guesses for $k_1, k_2$ also satisfy $c_2 = E_{k_2}(E_{k_1}(m_2))$ and $c_3 = E_{k_2}(E_{k_1}(m_3))$, then our guesses for $k_1, k_2$ are the actual secret keys.

Due to this attack, Double-DES is only slightly better than plain DES, with about $2^{57}$ bits of security and requiring about $2^{56}(64 + 56)$ bits of memory to store the table. The main hard part is getting the ~1 exabyte of storage needed to mount this attack, but it's quite within the realm of possibility. We can even do a time-memory tradeoff and do $2^{56 + s}$ DES operations in return for $2^{56 - s}$ bits of memory, which can easily bring the memory requirements down to a manageable size while keeping the number of operations feasible.

# 18/1/17

Why do we need 3 pairs of known plaintext/ciphertext pairs rather than just 1 or 2? Well, for each key in the key space, we can think of the encryption function is just a random function (it's not, but this is a good enough assumption for our purposes).

Let $k$ be the actual $l$-bit secret key (so $E_k(m) = c$) and $k'$ (also $l$ bits) be our guess for this key. If $E_{k'}$ is a uniformly random function (from our assumption), then it clearly has a $\frac 1 {2^L}$ probability of satisfying $E_{k'}(m) = c$, where $L$ is the number of bits in the plaintext - it might just randomly happen to output the right ciphertext given our particular plaintext. What's the probability that our guess is correct if it satisfies $E_{k'}(m) = c$?

Suppose $E_{k'}(m) = c$ and $k' \ne k$ (our guess for the key is incorrect, but gives the right ciphertext for our particular plaintext). Then the number of $E_{k'}$ such that $E_{k'}(m) = c$ is $\frac{2^l - 1}{2^{L}}$ ($2^l - 1$ is the number of keys in the keyspace that are not the actual key, and $2^{L}$ is the number of possible plaintexts). This is essentially the number of incorrect keys that work for a single plaintext/ciphertext pair without actually being the correct key.

We want to use enough plaintext/ciphertext pairs such that the expected number of these incorrect keys is close to 0. Clearly, with $t$ of these pairs, we have $\frac{2^l - 1}{2^{Lt}}$ expected false keys. For Double-DES, one pair gives us $\frac{2^{112} - 1}{2^{64}}$ expected incorrect keys, which is far too many. With 3 pairs, we get around $\frac 1 {2^{16}}$ expected false keys, much more reasonable.

There's also triple DES, which is just applying DES three times now - $c = E_{k_1}(E_{k_2}(E_{k_3}(m)))$, where $c$ is the resulting ciphertext, $m$ is the plaintext, and $k_1, k_2, k_3$ are the three 56-bit DES keys. We don't have any proof that it's more secure than DES alone, but a meet-in-the-middle attack takes around $2^{112}$ steps, around $2^{64}$ message/plaintext pairs, and a huge table.

Given our plaintext $m = m_1 \ldots m_t$, how do we encrypt it if the Block cipher modes of operation:

* Electronic Codebook - input is split into blocks, and then each block is encrypted with the key using the block cipher. In other words, for each plaintext block $i$, $c_i = E_k(m_i)$.
    * This is very bad, never use it - identical plaintext blocks result in identical ciphertext blocks under the same key, so chosen-plaintext attacks can tell you whether a given plaintext block is equal to an attacker-chosen plaintext block.
* Cipher Block Chaining - select a random $L$-bit initialization vector (where $L$ is the block size) as $c_0$. Then, for each plaintext block $i$, let $c_i = E_k(m_i \lxor c_{i - 1})$.
    * This is actually proven to be semantically secure against chosen plaintext attacks, assuming that the block cipher itself is semantically secure.

AES was developed as part of a public, open competition in 1997 to build a successor to DES. It needed to have 3 key sizes (128, 192, and 256 bits), have a 128-bit block size, and be efficiently implementable on hardware/software. While 128 bits is infeasible for the foreseeable future, the larger key sizes are intended to protect against potential future attacks, as well as quantum computers - Grover's algorithm can perform exhaustive key search in only $2^{\frac 1 2 l}$ operations, where $l$ is the number of bits in the key.

With 15 submissions initially, Rijndael won in the end. Rijndael became AES in late 2001.

Rijndael is an iterated block cipher, not a Fiestel cipher.